{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## anaconda3 (Python 3.12.0) Kernel\n",
    "import numpy as np\n",
    "\n",
    "# pair trade packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pairs Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary loaded from pairsOutcome.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/r7qn2m9n1zb6y_0q191gdqth0000gn/T/ipykernel_34700/3199335484.py:12: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  tpxData = pd.read_csv('TPX_prices.csv', index_col=0, parse_dates=True, date_parser=custom_date_parser)\n"
     ]
    }
   ],
   "source": [
    "def custom_date_parser(date_str):\n",
    "    return datetime.strptime(date_str, '%d/%m/%Y')\n",
    "\n",
    "# Load the dictionary from the pickle file\n",
    "with open('pairsOutcome.pkl', 'rb') as file:\n",
    "    pairsOutcome = pickle.load(file)\n",
    "\n",
    "print(\"Dictionary loaded from pairsOutcome.pkl\")\n",
    "\n",
    "\n",
    "# Load stock data and get return \n",
    "tpxData = pd.read_csv('TPX_prices.csv', index_col=0, parse_dates=True, date_parser=custom_date_parser)\n",
    "tpxData = tpxData.dropna(axis='columns')\n",
    "return_df = (tpxData / tpxData.shift(1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Pair Trade Portfolio\n",
    "`pairsOutcome` already have TOPIX stocks with highest liquidity and are tested for stationarity over a 1 year window\n",
    "\n",
    "Choose top 10 known pair trades by returns in the total dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_pairs = [('5401 JP Equity', '8604 JP Equity'), ('6273 JP Equity', '9984 JP Equity'), ('8053 JP Equity', '8058 JP Equity'), ('7733 JP Equity', '9613 JP Equity'), ('4684 JP Equity', '7832 JP Equity'), ('6762 JP Equity', '6857 JP Equity'), ('9020 JP Equity', '9022 JP Equity'), ('7267 JP Equity', '8306 JP Equity'), ('8308 JP Equity', '8802 JP Equity'), ('4901 JP Equity', '6702 JP Equity'), ('6503 JP Equity', '7269 JP Equity'), ('7267 JP Equity', '8801 JP Equity'), ('4519 JP Equity', '7532 JP Equity'), ('6988 JP Equity', '7267 JP Equity'), ('6326 JP Equity', '6954 JP Equity'), ('6752 JP Equity', '8604 JP Equity'), ('4901 JP Equity', '9613 JP Equity')]\n",
    "top_keys = [f\"{pair[0]} {pair[1]}\" for pair in working_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 performing trades:\n",
      "1. Key: 5401 JP Equity 8604 JP Equity, Value: 0.008934753682416807\n",
      "2. Key: 6273 JP Equity 9984 JP Equity, Value: 0.9917734886387003\n",
      "3. Key: 8053 JP Equity 8058 JP Equity, Value: 0.5183664464524095\n",
      "4. Key: 7733 JP Equity 9613 JP Equity, Value: 0.33979225570551896\n",
      "5. Key: 4684 JP Equity 7832 JP Equity, Value: 0.8866951703468606\n",
      "6. Key: 6762 JP Equity 6857 JP Equity, Value: -0.6701447016767165\n",
      "7. Key: 9020 JP Equity 9022 JP Equity, Value: 0.30716236746749637\n",
      "8. Key: 7267 JP Equity 8306 JP Equity, Value: 1.1628735722945633\n",
      "9. Key: 8308 JP Equity 8802 JP Equity, Value: 0.4326318755024665\n",
      "10. Key: 4901 JP Equity 6702 JP Equity, Value: -0.3387706704830611\n",
      "11. Key: 6503 JP Equity 7269 JP Equity, Value: 1.332416127365959\n",
      "12. Key: 7267 JP Equity 8801 JP Equity, Value: 0.6442746073863653\n",
      "13. Key: 4519 JP Equity 7532 JP Equity, Value: 1.138329913797488\n",
      "14. Key: 6988 JP Equity 7267 JP Equity, Value: 0.6453419709959551\n",
      "15. Key: 6326 JP Equity 6954 JP Equity, Value: 1.193819719845174\n",
      "16. Key: 6752 JP Equity 8604 JP Equity, Value: -0.47897415581246794\n",
      "17. Key: 4901 JP Equity 9613 JP Equity, Value: 1.1033889062314903\n"
     ]
    }
   ],
   "source": [
    "# Print the top 10 performing trades\n",
    "print(\"Top 10 performing trades:\")\n",
    "for i, key in enumerate(top_keys, 1):\n",
    "    print(f\"{i}. Key: {key}, Value: {pairsOutcome[key].cumpnl.iloc[-2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/r7qn2m9n1zb6y_0q191gdqth0000gn/T/ipykernel_34700/2263820838.py:4: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  valid = pd.read_csv('validPairs5.csv',\n"
     ]
    }
   ],
   "source": [
    "## Get pair stock data\n",
    "def custom_date_parser(date_str):\n",
    "    return datetime.strptime(date_str, '%d/%m/%Y')\n",
    "valid = pd.read_csv('validPairs5.csv', \n",
    "                    index_col=0, \n",
    "                    parse_dates=True, \n",
    "                    date_parser=custom_date_parser)\n",
    "## get list of pair stocks\n",
    "validPairsList = [\n",
    "    [item.strip() + ' Equity' for item in pair.split('Equity') if item.strip()]\n",
    "    for pair in top_keys\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollingWindow = 262\n",
    "cutLossSd = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in validPairsList:\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    #Calculate Standard Deviations\n",
    "    df['spread'] = valid[f'spread_{pair[0]}_{pair[1]}']\n",
    "    df['mid'] =  df['spread'].rolling(rollingWindow).mean()\n",
    "    df['1sd high'] = df['spread'].rolling(rollingWindow).mean() + df['spread'].rolling(rollingWindow).std()\n",
    "    df['1sd low'] = df['spread'].rolling(rollingWindow).mean() - df['spread'].rolling(rollingWindow).std()\n",
    "    df['2sd high'] = df['spread'].rolling(rollingWindow).mean() + df['spread'].rolling(rollingWindow).std() * cutLossSd\n",
    "    df['2sd low'] = df['spread'].rolling(rollingWindow).mean() - df['spread'].rolling(rollingWindow).std() * cutLossSd\n",
    "    df['position'] = 0\n",
    "\n",
    "    df.loc[(df['spread'] > df['1sd high']) & (df['spread'] < df['2sd high']), 'position'] = -1\n",
    "    df.loc[(df['spread']< df['1sd low']) & (df['spread'] > df['2sd low']), 'position'] = 1\n",
    "\n",
    "    #Calculate PnL\n",
    "    df[f'{pair[0]} position'] = df['position']\n",
    "    df[f'{pair[1]} position'] = df['position'] * -1\n",
    "    df['dailypnl'] = df[f'{pair[1]} position']*return_df[f'{pair[1]}'].shift(-1) + df[f'{pair[0]} position']*return_df[f'{pair[0]}'].shift(-1)\n",
    "    df['cumpnl'] = df['dailypnl'].cumsum()\n",
    "\n",
    "    pairsOutcome[f'{pair[0]} {pair[1]}'] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make indicators and spread stationary around 0\n",
    "Deduct the mean from all values to translate to 0 axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "workingPairOutcome = {}\n",
    "\n",
    "for pair in top_keys:\n",
    "    dummy_df = pairsOutcome[top_keys[0]].iloc[::,:6]\n",
    "    dummy_df = dummy_df.subtract(dummy_df['mid'], axis=0).drop(columns=['mid']) # centre spread and SD\n",
    "    dummy_df = dummy_df.div(dummy_df['2sd high']-dummy_df['1sd high'],axis=0)   # express SD as integers, give spread as propotionate\n",
    "    dummy_df['2sd_high_boolean'] = (dummy_df['spread']>dummy_df['2sd high']).astype(int)\n",
    "    dummy_df['1sd_high_boolean'] = (dummy_df['spread']>dummy_df['1sd high']).astype(int)\n",
    "    dummy_df['0sd_high_boolean'] = (dummy_df['spread']>0).astype(int)\n",
    "    dummy_df['0sd_low_boolean']  = (dummy_df['spread']<0).astype(int)\n",
    "    dummy_df['1sd_low_boolean']  = (dummy_df['spread']<dummy_df['1sd low'] ).astype(int)\n",
    "    dummy_df['2sd_low_boolean']  = (dummy_df['spread']<dummy_df['2sd low'] ).astype(int)\n",
    "    dummy_df = dummy_df.drop(columns=['spread','1sd high', '1sd low', '2sd high', '2sd low'])\n",
    "    workingPairOutcome[pair] = dummy_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workingPairOutcome[top_keys[5]][-5:]     # spread is not a proportion and direction of SD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Challenge\n",
    "\n",
    "## Background\n",
    "Initial evaluation of the baseline portfolio shows that draw downs are small. Originally team had the idea of using Machine Learning to optimise for sizing of these pair trades. However since there was no significant drawdowns the returns are linearly increasing with investment sizing i.e. greater nominal investment in the the pair trade the proportionate increase in returns without realising significant drawdown risk.\n",
    "\n",
    "Instead of optimising for sizing, we can explore Machine Learning in terms of strategy on this stationary dataset. Whereas our prescribed strategy is to enter at +/- 1 std dev, exit at 0 with +/- 2 std dev stop loss. These are only suggestions and arbitrary levels.\n",
    "\n",
    "With Machine Learning, we can discover if it will uncover the mean reverting nature and recommend another threshhold. We use Q Learner to understand state space with the same spread, mid, std dev parameters as the baseline.\n",
    "\n",
    "### Q Value table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairTradeEnv1:\n",
    "    def __init__(self):\n",
    "        # Initialize environment variables and parameters\n",
    "        self.num_actions = 3\n",
    "        self.num_states = 2**6\n",
    "        self.earliest_step = 261  # hot start\n",
    "        self.last_step = 2868\n",
    "\n",
    "        self.state = np.zeros(4)\n",
    "        self.current_step = self.earliest_step\n",
    "\n",
    "    def reset(self,pair_idx):\n",
    "        # Reset the environment to its initial state\n",
    "        self.current_step = self.earliest_step\n",
    "        self.state = workingPairOutcome[top_keys[pair_idx]][self.current_step]\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action,pair_idx):\n",
    "        # Advance the time step\n",
    "        self.current_step += 1\n",
    "        next_state = workingPairOutcome[top_keys[pair_idx]][self.current_step]\n",
    "        reward = self.calculate_reward(action,pair_idx)\n",
    "        done = self.current_step >= self.last_step\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def calculate_reward(self, position,pair_idx):\n",
    "        \"\"\"\n",
    "        Give one _previous_ day's return\n",
    "        Input:\n",
    "            position: position for idx (current step)\n",
    "            idx: usually current timestp \n",
    "            pair: tuple of tpx stock\n",
    "        Output:\n",
    "            dailypnl\n",
    "        \"\"\"\n",
    "        pair = validPairsList[pair_idx]\n",
    "        # position = position_vector @ np.array([-1,0,1])\n",
    "        position_0 = position\n",
    "        position_1 = position * -1\n",
    "        ## return_df gives the return for the previous day for the given idx\n",
    "        dailypnl = position_0*return_df[f'{pair[0]}'].iloc[self.current_step] + position_1*return_df[f'{pair[1]}'].iloc[self.current_step] \n",
    "\n",
    "        return dailypnl\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions, alpha=0.3, gamma=0.9, epsilon=0.3):\n",
    "        self.num_actions = num_actions\n",
    "        self.num_states = num_states\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.action_to_index = {-1: 0, 0: 1, 1: 2}\n",
    "        self.index_to_action = {0: -1, 1: 0, 2: 1}\n",
    "        self.Q = np.zeros((num_states, num_actions))\n",
    "        \n",
    "        # State visitation frequency\n",
    "        self.state_counts = np.zeros(num_states)\n",
    "\n",
    "    def b_to_d_state(self, binary_state):\n",
    "        decimal_index = sum(binary_state[i] * 2**(5 - i) for i in range(6))\n",
    "        return decimal_index\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action_index = np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            state_index = self.b_to_d_state(state)\n",
    "            action_index = np.argmax(self.Q[state_index])\n",
    "        return self.index_to_action[action_index]\n",
    "\n",
    "    def update_Q(self, state, action, reward, next_state):\n",
    "        state_index = self.b_to_d_state(state)\n",
    "        next_state_index = self.b_to_d_state(next_state)\n",
    "        self.Q[state_index, self.action_to_index[action]] += self.alpha * (\n",
    "            reward + self.gamma * np.max(self.Q[next_state_index]) - self.Q[state_index, self.action_to_index[action]]\n",
    "        )\n",
    "        self.state_counts[state_index] += 1\n",
    "\n",
    "    def learn(self, num_episodes, env):\n",
    "        for episode in range(num_episodes):\n",
    "            for pair_idx in range(10):\n",
    "                # Sample initial state with bias toward underrepresented states\n",
    "                state_probabilities = 1 / (self.state_counts + 1)\n",
    "                state_probabilities /= state_probabilities.sum()\n",
    "                sampled_state_index = np.random.choice(self.num_states, p=state_probabilities)\n",
    "\n",
    "                state = env.reset(pair_idx)\n",
    "                done = False\n",
    "\n",
    "                while not done:\n",
    "                    action = self.choose_action(state)\n",
    "                    next_state, reward, done = env.step(action, pair_idx)\n",
    "                    reward *= 10\n",
    "                    self.update_Q(state, action, reward, next_state)\n",
    "                    state = next_state\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "env = PairTradeEnv1()\n",
    "agent = Agent(num_states=env.num_states, num_actions=env.num_actions)\n",
    "agent.learn(num_episodes, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_labels = [f\"{i:06b}\" for i in range(64)]\n",
    "df = pd.DataFrame(agent.Q, index=binary_labels, columns=[-1, 0, 1])\n",
    "df = df[(df != 0).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>mean_reversion</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000100</th>\n",
       "      <td>0.084522</td>\n",
       "      <td>0.056134</td>\n",
       "      <td>-0.004717</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000110</th>\n",
       "      <td>0.118823</td>\n",
       "      <td>0.093761</td>\n",
       "      <td>0.131606</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000111</th>\n",
       "      <td>0.157484</td>\n",
       "      <td>0.103588</td>\n",
       "      <td>0.063113</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001000</th>\n",
       "      <td>0.073366</td>\n",
       "      <td>0.076011</td>\n",
       "      <td>0.064393</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>011000</th>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.055637</td>\n",
       "      <td>0.019247</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111000</th>\n",
       "      <td>0.009486</td>\n",
       "      <td>0.069012</td>\n",
       "      <td>-0.007259</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              -1         0         1  mean_reversion position\n",
       "000100  0.084522  0.056134 -0.004717               0       -1\n",
       "000110  0.118823  0.093761  0.131606               1        1\n",
       "000111  0.157484  0.103588  0.063113               0       -1\n",
       "001000  0.073366  0.076011  0.064393               0        0\n",
       "011000  0.005333  0.055637  0.019247              -1        0\n",
       "111000  0.009486  0.069012 -0.007259               0        0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['mean_reversion'] = [0,1,0,0,-1,0]\n",
    "df['position'] = df[[-1, 0, 1]].idxmax(axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "table is taking a long time to generalise. using one pair is not good enough to get any poistioning as everything is flat. only after adding all 10 pairs then performance churns out shorting on 1 SD high cross."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2sd_high_boolean', '1sd_high_boolean', '0sd_high_boolean',\n",
       "       '0sd_low_boolean', '1sd_low_boolean', '2sd_low_boolean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline ['5401 JP Equity', '8604 JP Equity'], Total Reward: 0.365425120007449, step 2606\n"
     ]
    }
   ],
   "source": [
    "# Get baseline results\n",
    "t_pair = validPairsList[0]\n",
    "max_steps_per_episode = 3000\n",
    "agent.epsilon = 0\n",
    "\n",
    "def get_baseline(env, max_steps_per_episode, pair_idx):\n",
    "    agent.epsilon = 0\n",
    "    env.reset(pair_idx)\n",
    "    total_reward = 0\n",
    "    current_step = 261\n",
    "    env.current_step = current_step\n",
    "    env.last_step = 2868\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        action = pairsOutcome[top_keys[pair_idx]].iloc[env.current_step]['position']\n",
    "        _, reward, done = env.step(action,pair_idx)\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Baseline {t_pair}, Total Reward: {total_reward}, step {step}\")\n",
    "\n",
    "get_baseline(env, 3000, pair_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: (0, 0, 0, 0, 0, 0), Count: 261\n",
      "Row: (0, 0, 1, 0, 0, 0), Count: 712\n",
      "Row: (0, 1, 1, 0, 0, 0), Count: 703\n",
      "Row: (1, 1, 1, 0, 0, 0), Count: 162\n",
      "Row: (0, 0, 0, 1, 0, 0), Count: 446\n",
      "Row: (0, 0, 0, 1, 1, 0), Count: 470\n",
      "Row: (0, 0, 0, 1, 1, 1), Count: 225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(0, 0, 0, 1, 0, 0): -1,\n",
       " (0, 0, 0, 1, 1, 0): 1,\n",
       " (0, 0, 0, 1, 1, 1): -1,\n",
       " (0, 0, 1, 0, 0, 0): 0,\n",
       " (0, 1, 1, 0, 0, 0): 0,\n",
       " (1, 1, 1, 0, 0, 0): 0,\n",
       " (0, 0, 0, 0, 0, 0): 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "array = workingPairOutcome['5401 JP Equity 8604 JP Equity']\n",
    "# Convert rows to tuples (hashable type) for counting\n",
    "row_tuples = map(tuple, array)\n",
    "\n",
    "# Count unique rows\n",
    "row_counts = Counter(row_tuples)\n",
    "q_policy_actions = df.position.values\n",
    "ls_state = []\n",
    "# Print results\n",
    "for row, count in row_counts.items():\n",
    "    ls_state.append(row)\n",
    "    print(f\"Row: {row}, Count: {count}\")\n",
    "\n",
    "ls_state = ls_state[1:]\n",
    "ls_state.sort()\n",
    "dict_policy_actions = dict(zip(ls_state, q_policy_actions))\n",
    "dict_policy_actions[(0, 0, 0, 0, 0, 0)] = 0\n",
    "dict_policy_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workingPairOutcome['5401 JP Equity 8604 JP Equity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "workingPairQresults = {}\n",
    "\n",
    "for pair_idx in range(len(top_keys)):\n",
    "    df = pd.DataFrame(workingPairOutcome[top_keys[pair_idx]], columns=dummy_df.columns)\n",
    "\n",
    "    # Assign policy values using the dictionary\n",
    "    df['position'] = df.apply(lambda row: dict_policy_actions.get(tuple(row), np.nan), axis=1)\n",
    "    df[df.isna().any(axis=1)]\n",
    "    pair = validPairsList[pair_idx]\n",
    "    #Calculate PnL\n",
    "    df[f'{pair[0]} position'] = df['position']\n",
    "    df[f'{pair[1]} position'] = df['position'] * -1\n",
    "    df['dailypnl'] = df[f'{pair[1]} position'].values*return_df[f'{pair[1]}'].shift(-1).values \\\n",
    "                    + df[f'{pair[0]} position'].values*return_df[f'{pair[0]}'].shift(-1).values\n",
    "    df['cumpnl'] = df['dailypnl'].cumsum()\n",
    "    workingPairQresults[top_keys[pair_idx]] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total return \t\t0.20227\n",
      "total train return \t0.18891\n",
      "total test return \t0.01336\n"
     ]
    }
   ],
   "source": [
    "total_q_return = 0\n",
    "total_train_q_return = 0\n",
    "total_test_q_return = 0\n",
    "\n",
    "for pair in top_keys:\n",
    "    total_q_return += workingPairQresults[pair]['cumpnl'].iloc[-2]\n",
    "    total_train_q_return += workingPairQresults[pair]['cumpnl'].iloc[2868-2]\n",
    "\n",
    "\n",
    "print(f\"total return \\t\\t{total_q_return/len(top_keys):.5f}\")\n",
    "print(f\"total train return \\t{total_train_q_return/len(top_keys):.5f}\")\n",
    "print(f\"total test return \\t{(total_q_return - total_train_q_return)/len(top_keys):.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

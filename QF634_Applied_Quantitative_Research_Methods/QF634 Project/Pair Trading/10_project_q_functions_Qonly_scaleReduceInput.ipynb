{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## anaconda3 (Python 3.12.0) Kernel\n",
    "import numpy as np\n",
    "import csv\n",
    "# pair trade packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pairs Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/r7qn2m9n1zb6y_0q191gdqth0000gn/T/ipykernel_43477/734353408.py:7: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  tpxData = pd.read_csv('TPX_prices.csv', index_col=0, parse_dates=True, date_parser=custom_date_parser)\n"
     ]
    }
   ],
   "source": [
    "def custom_date_parser(date_str):\n",
    "    return datetime.strptime(date_str, '%d/%m/%Y')\n",
    "\n",
    "with open('pairsOutcome.pkl', 'rb') as file:\n",
    "    pairsOutcome = pickle.load(file)\n",
    "\n",
    "tpxData = pd.read_csv('TPX_prices.csv', index_col=0, parse_dates=True, date_parser=custom_date_parser)\n",
    "tpxData = tpxData.dropna(axis='columns')\n",
    "return_df = (tpxData / tpxData.shift(1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Pair Trade Portfolio\n",
    "`pairsOutcome` already have TOPIX stocks with highest liquidity and are tested for stationarity over a 1 year window\n",
    "\n",
    "Choose top 10 known pair trades by returns in the total dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/r7qn2m9n1zb6y_0q191gdqth0000gn/T/ipykernel_43477/3045906102.py:7: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  valid = pd.read_csv('validPairs5.csv',\n"
     ]
    }
   ],
   "source": [
    "with open(\"output_clustering.csv\", 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)\n",
    "    working_pairs = [tuple(row) for row in reader]\n",
    "\n",
    "top_keys = [f\"{pair[0]} {pair[1]}\" for pair in working_pairs]\n",
    "valid = pd.read_csv('validPairs5.csv', \n",
    "                    index_col=0, \n",
    "                    parse_dates=True, \n",
    "                    date_parser=custom_date_parser)\n",
    "validPairsList = [\n",
    "    [item.strip() + ' Equity' for item in pair.split('Equity') if item.strip()]\n",
    "    for pair in top_keys\n",
    "]\n",
    "rollingWindow = 262\n",
    "cutLossSd = 2\n",
    "\n",
    "for pair in validPairsList:\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    #Calculate Standard Deviations\n",
    "    df['spread'] = valid[f'spread_{pair[0]}_{pair[1]}']\n",
    "    df['mid'] =  df['spread'].rolling(rollingWindow).mean()\n",
    "    df['1sd high'] = df['spread'].rolling(rollingWindow).mean() + df['spread'].rolling(rollingWindow).std()\n",
    "    df['1sd low'] = df['spread'].rolling(rollingWindow).mean() - df['spread'].rolling(rollingWindow).std()\n",
    "    df['2sd high'] = df['spread'].rolling(rollingWindow).mean() + df['spread'].rolling(rollingWindow).std() * cutLossSd\n",
    "    df['2sd low'] = df['spread'].rolling(rollingWindow).mean() - df['spread'].rolling(rollingWindow).std() * cutLossSd\n",
    "    df['position'] = 0\n",
    "\n",
    "    df.loc[(df['spread'] > df['1sd high']) & (df['spread'] < df['2sd high']), 'position'] = -1\n",
    "    df.loc[(df['spread']< df['1sd low']) & (df['spread'] > df['2sd low']), 'position'] = 1\n",
    "\n",
    "    #Calculate PnL\n",
    "    df[f'{pair[0]} position'] = df['position']\n",
    "    df[f'{pair[1]} position'] = df['position'] * -1\n",
    "    df['dailypnl'] = df[f'{pair[1]} position']*return_df[f'{pair[1]}'].shift(-1) + df[f'{pair[0]} position']*return_df[f'{pair[0]}'].shift(-1)\n",
    "    df['cumpnl'] = df['dailypnl'].cumsum()\n",
    "\n",
    "    pairsOutcome[f'{pair[0]} {pair[1]}'] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make indicators and spread stationary around 0\n",
    "Deduct the mean from all values to translate to 0 axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workingPairOutcome = {}\n",
    "\n",
    "for pair in top_keys:\n",
    "    dummy_df = pairsOutcome[top_keys[0]].iloc[::,:6]\n",
    "    dummy_df = dummy_df.subtract(dummy_df['mid'], axis=0).drop(columns=['mid']) # centre spread and SD\n",
    "    dummy_df = dummy_df.div(dummy_df['2sd high']-dummy_df['1sd high'],axis=0)   # express SD as integers, give spread as propotionate\n",
    "    dummy_df['2sd_high_boolean'] = (dummy_df['spread']>dummy_df['2sd high']).astype(int)\n",
    "    dummy_df['1sd_high_boolean'] = (dummy_df['spread']>dummy_df['1sd high']).astype(int)\n",
    "    dummy_df['0sd_high_boolean'] = (dummy_df['spread']>0).astype(int)\n",
    "    dummy_df['0sd_low_boolean']  = (dummy_df['spread']<0).astype(int)\n",
    "    dummy_df['1sd_low_boolean']  = (dummy_df['spread']<dummy_df['1sd low'] ).astype(int)\n",
    "    dummy_df['2sd_low_boolean']  = (dummy_df['spread']<dummy_df['2sd low'] ).astype(int)\n",
    "    dummy_df = dummy_df.drop(columns=['spread','1sd high', '1sd low', '2sd high', '2sd low'])\n",
    "    workingPairOutcome[pair] = dummy_df.to_numpy()\n",
    "\n",
    "workingPairOutcome[top_keys[5]][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to spikey-ness of reward around 0, scale from -1 to 1 and give more rewards above 1, 2, 3\n",
    "dailypnl_sd = 0.018390515013803736 # see /Users/ju/Projects/00_SMU/mqf_practice/QF634_Applied_Quantitative_Research_Methods/QF634 Project/Pair Trading/03_project_state_space_analysis.ipynb for SD derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Challenge\n",
    "\n",
    "## Background\n",
    "Initial evaluation of the baseline portfolio shows that draw downs are small. Originally team had the idea of using Machine Learning to optimise for sizing of these pair trades. However since there was no significant drawdowns the returns are linearly increasing with investment sizing i.e. greater nominal investment in the the pair trade the proportionate increase in returns without realising significant drawdown risk.\n",
    "\n",
    "Instead of optimising for sizing, we can explore Machine Learning in terms of strategy on this stationary dataset. Whereas our prescribed strategy is to enter at +/- 1 std dev, exit at 0 with +/- 2 std dev stop loss. These are only suggestions and arbitrary levels.\n",
    "\n",
    "With Machine Learning, we can discover if it will uncover the mean reverting nature and recommend another threshhold. We use Q Learner to understand state space with the same spread, mid, std dev parameters as the baseline.\n",
    "\n",
    "### Q Value table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline(env, trained_agent, max_steps_per_episode, pair_idx):\n",
    "    env.reset(pair_idx)\n",
    "    total_reward = 0\n",
    "    current_step = 261\n",
    "    env.current_step = current_step\n",
    "    env.last_step = 2868\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        state = workingPairOutcome[top_keys[pair_idx]][current_step+step]\n",
    "        state_index = env.valid_states.index(tuple(state))\n",
    "        action_index = np.argmax(trained_agent.Q[state_index])\n",
    "        action = trained_agent.index_to_action[action_index]\n",
    "        _, reward, done = env.step(action,pair_idx)\n",
    "        total_reward += reward\n",
    "            \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"{pair_idx+1}. {top_keys[pair_idx]}: Reward: {total_reward:.5f}, step {step}\")\n",
    "    return total_reward\n",
    "\n",
    "class PairTradeEnv1:\n",
    "    def __init__(self):\n",
    "        # Initialize environment variables and parameters\n",
    "        self.num_actions = 3\n",
    "        self.valid_states = [\n",
    "            (0, 0, 0, 1, 0, 0),\n",
    "            (0, 0, 0, 1, 1, 0),\n",
    "            (0, 0, 0, 1, 1, 1),\n",
    "            (0, 0, 1, 0, 0, 0),\n",
    "            (0, 1, 1, 0, 0, 0),\n",
    "            (1, 1, 1, 0, 0, 0),\n",
    "            (0, 0, 0, 0, 0, 0),\n",
    "        ]  # Define valid states\n",
    "        self.state_mapping = {\n",
    "            i: state for i, state in enumerate(self.valid_states)\n",
    "        }\n",
    "        self.earliest_step = 261  # Hot start\n",
    "        self.last_step = 2868\n",
    "\n",
    "        self.state = np.zeros(6)\n",
    "        self.current_step = self.earliest_step\n",
    "\n",
    "    def reset(self, pair_idx):\n",
    "        # Reset the environment to its initial state\n",
    "        self.current_step = self.earliest_step\n",
    "        self.state = workingPairOutcome[top_keys[pair_idx]][self.current_step]\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action, pair_idx):\n",
    "        # Advance the time step\n",
    "        self.current_step += 1\n",
    "        next_state = workingPairOutcome[top_keys[pair_idx]][self.current_step]\n",
    "        reward = self.calculate_reward(action, pair_idx)\n",
    "        done = self.current_step >= self.last_step\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def calculate_reward(self, position, pair_idx):\n",
    "        \"\"\"\n",
    "        Calculate reward based on the previous day's return.\n",
    "        \"\"\"\n",
    "        pair = validPairsList[pair_idx]\n",
    "        position_0 = position\n",
    "        position_1 = position * -1\n",
    "        dailypnl = position_0 * return_df[f'{pair[0]}'].iloc[self.current_step] \\\n",
    "            + position_1 * return_df[f'{pair[1]}'].iloc[self.current_step]\n",
    "\n",
    "        return dailypnl\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, num_states, num_actions, batch_size, alpha=0.1, gamma=0.9, epsilon=0.5, epsilon_decay=0.95, buffer_size = 10000):\n",
    "        self.env = env\n",
    "        self.num_actions = num_actions\n",
    "        self.num_states = num_states\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = 0.95\n",
    "        # set indices for q table\n",
    "        self.action_to_index = {-1: 0, 0: 1, 1: 2}\n",
    "        self.index_to_action = {0: -1, 1: 0, 2: 1}\n",
    "        self.Q = np.zeros((num_states, num_actions))\n",
    "        # replay memory\n",
    "        self.replay_buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.state_counts = np.zeros(len(self.env.valid_states))\n",
    "        self.batch = []\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action_index = np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            state_index = self.env.valid_states.index(tuple(state))\n",
    "            action_index = np.argmax(self.Q[state_index])\n",
    "        return self.index_to_action[action_index]\n",
    "\n",
    "    def update_Q_from_buffer(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        # get counts of each state\n",
    "        state_counts = np.bincount([state_index for state_index, _, _, _ in self.replay_buffer], \n",
    "                                   minlength=len(self.env.valid_states))\n",
    "        state_probabilities = state_counts / state_counts.sum()\n",
    "        # equally represent all states in a batch\n",
    "        batch = []\n",
    "        for _ in range(self.batch_size):\n",
    "            sampled_state_index = np.random.choice(len(self.env.valid_states), p=state_probabilities)\n",
    "            filtered_transitions = [transition for transition in self.replay_buffer if transition[0] == sampled_state_index]\n",
    "            if not filtered_transitions:\n",
    "                return  \n",
    "            batch.append(filtered_transitions[np.random.choice(len(filtered_transitions))])\n",
    "        # train based on resampled batch\n",
    "        for state_index, action, reward, next_state_index in batch:\n",
    "            reward /= dailypnl_sd\n",
    "            # if reward > 2:\n",
    "            #     reward +=2\n",
    "            # elif reward > 0.5:\n",
    "            #     reward +=1\n",
    "            # elif reward < -2:\n",
    "            #     reward += -2\n",
    "            # elif reward < -0.5:\n",
    "            #     reward += -1\n",
    "            self.Q[state_index, self.action_to_index[action]] += self.alpha * (\n",
    "                reward + self.gamma * np.max(self.Q[next_state_index])\n",
    "                - self.Q[state_index, self.action_to_index[action]]\n",
    "            )\n",
    "            self.state_counts[state_index] += 1\n",
    "\n",
    "    def learn(self, num_episodes):\n",
    "        step_count = 0\n",
    "        for episode in range(num_episodes):\n",
    "            for pair_idx in range(len(top_keys)):\n",
    "                state = self.env.reset(pair_idx)\n",
    "                done = False\n",
    "\n",
    "                while not done:\n",
    "                    action = self.choose_action(state)\n",
    "\n",
    "                    next_state, reward, done = self.env.step(action, pair_idx)\n",
    "                    state_index = self.env.valid_states.index(tuple(state))\n",
    "                    next_state_index = self.env.valid_states.index(tuple(next_state))\n",
    "\n",
    "                    self.replay_buffer.append((state_index, action, reward, next_state_index))\n",
    "                    if len(self.replay_buffer) > self.buffer_size:\n",
    "                        self.replay_buffer.pop(0)\n",
    "\n",
    "                    if step_count%self.batch_size==0:\n",
    "                        self.update_Q_from_buffer()\n",
    "                    \n",
    "                    step_count += 1\n",
    "                    state = next_state\n",
    "            self.epsilon = max(0.3, self.epsilon*self.epsilon_decay)\n",
    "            if episode%20==0:\n",
    "                print(self.epsilon)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gamma = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n",
      "1. 6503 JP Equity 7269 JP Equity: Reward: 2.11578, step 2606\n",
      "2. 6326 JP Equity 6954 JP Equity: Reward: -0.01672, step 2606\n",
      "3. 8053 JP Equity 8058 JP Equity: Reward: -0.05075, step 2606\n",
      "4. 4901 JP Equity 9613 JP Equity: Reward: -1.14694, step 2606\n",
      "5. 6988 JP Equity 7267 JP Equity: Reward: -0.07786, step 2606\n",
      "6. 4901 JP Equity 6702 JP Equity: Reward: -0.37365, step 2606\n",
      "7. 4684 JP Equity 7832 JP Equity: Reward: -0.39928, step 2606\n",
      "8. 7267 JP Equity 8306 JP Equity: Reward: -0.56660, step 2606\n",
      "9. 7267 JP Equity 8801 JP Equity: Reward: -0.70480, step 2606\n",
      "10. 4519 JP Equity 7532 JP Equity: Reward: -0.27370, step 2606\n",
      "0.95\n",
      "1. 6503 JP Equity 7269 JP Equity: Reward: 1.01645, step 2606\n",
      "2. 6326 JP Equity 6954 JP Equity: Reward: 0.78693, step 2606\n",
      "3. 8053 JP Equity 8058 JP Equity: Reward: -1.09874, step 2606\n",
      "4. 4901 JP Equity 9613 JP Equity: Reward: 0.40968, step 2606\n",
      "5. 6988 JP Equity 7267 JP Equity: Reward: 0.58956, step 2606\n",
      "6. 4901 JP Equity 6702 JP Equity: Reward: -0.17914, step 2606\n",
      "7. 4684 JP Equity 7832 JP Equity: Reward: 0.19853, step 2606\n",
      "8. 7267 JP Equity 8306 JP Equity: Reward: -0.88868, step 2606\n",
      "9. 7267 JP Equity 8801 JP Equity: Reward: -0.35496, step 2606\n",
      "10. 4519 JP Equity 7532 JP Equity: Reward: -0.89944, step 2606\n",
      "0.95\n",
      "1. 6503 JP Equity 7269 JP Equity: Reward: 1.30404, step 2606\n",
      "2. 6326 JP Equity 6954 JP Equity: Reward: -0.05835, step 2606\n",
      "3. 8053 JP Equity 8058 JP Equity: Reward: 0.15788, step 2606\n",
      "4. 4901 JP Equity 9613 JP Equity: Reward: -0.59543, step 2606\n",
      "5. 6988 JP Equity 7267 JP Equity: Reward: -0.50251, step 2606\n",
      "6. 4901 JP Equity 6702 JP Equity: Reward: 0.04834, step 2606\n",
      "7. 4684 JP Equity 7832 JP Equity: Reward: -0.56005, step 2606\n",
      "8. 7267 JP Equity 8306 JP Equity: Reward: -0.02155, step 2606\n",
      "9. 7267 JP Equity 8801 JP Equity: Reward: -0.38661, step 2606\n",
      "10. 4519 JP Equity 7532 JP Equity: Reward: -0.34509, step 2606\n",
      "Epoch Total Ave Reward: [-0.14945136647535748, -0.04198132710258564, -0.09593289795481606]\n",
      "Epoch Ave Ave Reward: \t-0.09579\n",
      "Epoch SD  Ave Reward: \t0.04387 \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "num_episodes = 10\n",
    "ls_epoch_reward = []\n",
    "gamma = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # instantiate\n",
    "    env = PairTradeEnv1()\n",
    "    agent = Agent(env=env,\n",
    "                num_states=len(env.valid_states), \n",
    "                num_actions=env.num_actions, \n",
    "                batch_size= len(top_keys)*100, \n",
    "                alpha=0.5, \n",
    "                gamma=gamma, \n",
    "                epsilon_decay=0.995,\n",
    "                epsilon=1, \n",
    "                buffer_size = 10000)\n",
    "    # train\n",
    "    agent.learn(num_episodes)\n",
    "    # evaluate\n",
    "    total_train_return = 0\n",
    "    for idx, _ in enumerate(top_keys):\n",
    "        total_train_return += get_baseline(env, agent, 3000, pair_idx=idx)\n",
    "\n",
    "    ls_epoch_reward.append(total_train_return/len(top_keys))\n",
    "\n",
    "print(f\"Epoch Total Ave Reward: {ls_epoch_reward}\")\n",
    "print(f\"Epoch Ave Ave Reward: \\t{np.array(ls_epoch_reward).mean():.5f}\")\n",
    "print(f\"Epoch SD  Ave Reward: \\t{np.array(ls_epoch_reward).std():.5f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>mean_reversion</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(0, 0, 0, 1, 0, 0)</th>\n",
       "      <td>-0.315604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.062683</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0, 0, 0, 1, 1, 0)</th>\n",
       "      <td>-0.336948</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.260248</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0, 0, 0, 1, 1, 1)</th>\n",
       "      <td>0.443240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.024074</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0, 0, 1, 0, 0, 0)</th>\n",
       "      <td>0.873375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.486601</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0, 1, 1, 0, 0, 0)</th>\n",
       "      <td>0.387037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.438765</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 1, 1, 0, 0, 0)</th>\n",
       "      <td>0.155044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041110</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0, 0, 0, 0, 0, 0)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          -1    0         1  mean_reversion position\n",
       "(0, 0, 0, 1, 0, 0) -0.315604  0.0 -0.062683               0        0\n",
       "(0, 0, 0, 1, 1, 0) -0.336948  0.0 -0.260248               1        0\n",
       "(0, 0, 0, 1, 1, 1)  0.443240  0.0 -0.024074               0       -1\n",
       "(0, 0, 1, 0, 0, 0)  0.873375  0.0 -0.486601               0       -1\n",
       "(0, 1, 1, 0, 0, 0)  0.387037  0.0 -0.438765              -1       -1\n",
       "(1, 1, 1, 0, 0, 0)  0.155044  0.0  0.041110               0       -1\n",
       "(0, 0, 0, 0, 0, 0)  0.000000  0.0  0.000000               0       -1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(agent.Q, index=env.valid_states, columns=[-1, 0, 1])\n",
    "df['mean_reversion'] = [0,1,0,0,-1,0,0]\n",
    "df['position'] = df[[-1, 0, 1]].idxmax(axis=1)\n",
    "\n",
    "df.to_csv(f\"q_table_gamma{agent.gamma}_{agent.gamma}.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0, 0, 1, 0, 0): 0,\n",
       " (0, 0, 0, 1, 1, 0): 0,\n",
       " (0, 0, 0, 1, 1, 1): -1,\n",
       " (0, 0, 1, 0, 0, 0): -1,\n",
       " (0, 1, 1, 0, 0, 0): -1,\n",
       " (1, 1, 1, 0, 0, 0): -1,\n",
       " (0, 0, 0, 0, 0, 0): -1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_policy_actions = dict(df.position)\n",
    "dict_policy_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "table is taking a long time to generalise. using one pair is not good enough to get any poistioning as everything is flat. only after adding all 10 pairs then performance churns out shorting on 1 SD high cross.\n",
    "\n",
    "Experiment: try making gamma 0.1, Q table should closely mimic the state space analysis table with strong mean reversion tendency. This is not what we see. and it even has Q values opposite to state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Absolute ==========\n",
      "total return \t\t-0.04015\n",
      "total train return \t-0.08285\n",
      "total test return \t0.04270\n",
      "========== Normalised ==========\n",
      "total return \t\t-0.00001\n",
      "total train return \t-0.00003\n",
      "total test return \t0.00038\n"
     ]
    }
   ],
   "source": [
    "workingPairQresults = {}\n",
    "for pair_idx in range(len(top_keys)):\n",
    "    df = pd.DataFrame(workingPairOutcome[top_keys[pair_idx]], columns=dummy_df.columns)\n",
    "\n",
    "    # Assign policy values using the dictionary\n",
    "    df['position'] = df.apply(lambda row: dict_policy_actions.get(tuple(row), np.nan), axis=1)\n",
    "    df[df.isna().any(axis=1)]\n",
    "    pair = validPairsList[pair_idx]\n",
    "    #Calculate PnL\n",
    "    df[f'{pair[0]} position'] = df['position']\n",
    "    df[f'{pair[1]} position'] = df['position'] * -1\n",
    "    df['dailypnl'] = df[f'{pair[1]} position'].values*return_df[f'{pair[1]}'].shift(-1).values \\\n",
    "                    + df[f'{pair[0]} position'].values*return_df[f'{pair[0]}'].shift(-1).values\n",
    "    df['cumpnl'] = df['dailypnl'].cumsum()\n",
    "    workingPairQresults[top_keys[pair_idx]] = df\n",
    "\n",
    "total_q_return = 0\n",
    "total_train_q_return = 0\n",
    "total_test_q_return = 0\n",
    "\n",
    "for pair in top_keys:\n",
    "    total_q_return += workingPairQresults[pair]['cumpnl'].iloc[-2]\n",
    "    total_train_q_return += workingPairQresults[pair]['cumpnl'].iloc[2868-2]\n",
    "\n",
    "print(\"=\"*10, \"Absolute\", \"=\"*10)\n",
    "print(f\"total return \\t\\t{total_q_return/len(top_keys):.5f}\")\n",
    "print(f\"total train return \\t{total_train_q_return/len(top_keys):.5f}\")\n",
    "print(f\"total test return \\t{(total_q_return - total_train_q_return)/len(top_keys):.5f}\")\n",
    "print(\"=\"*10, \"Normalised\", \"=\"*10)\n",
    "print(f\"total return \\t\\t{total_q_return/len(top_keys)/(2979-261):.5f}\")\n",
    "print(f\"total train return \\t{total_train_q_return/len(top_keys)/(2868-261):.5f}\")\n",
    "print(f\"total test return \\t{(total_q_return - total_train_q_return)/len(top_keys)/(2979-2868):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gamma > 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 261\n",
      "0.95\n",
      "1. 6503 JP Equity 7269 JP Equity: Reward: -0.12023, step 2606\n",
      "2. 6326 JP Equity 6954 JP Equity: Reward: 0.39556, step 2606\n",
      "3. 8053 JP Equity 8058 JP Equity: Reward: -0.82107, step 2606\n",
      "4. 4901 JP Equity 9613 JP Equity: Reward: 1.11480, step 2606\n",
      "5. 6988 JP Equity 7267 JP Equity: Reward: -0.17254, step 2606\n",
      "6. 4901 JP Equity 6702 JP Equity: Reward: 1.00307, step 2606\n",
      "7. 4684 JP Equity 7832 JP Equity: Reward: -0.11127, step 2606\n",
      "8. 7267 JP Equity 8306 JP Equity: Reward: 0.44704, step 2606\n",
      "9. 7267 JP Equity 8801 JP Equity: Reward: 0.56159, step 2606\n",
      "10. 4519 JP Equity 7532 JP Equity: Reward: -0.37529, step 2606\n",
      "0.5 261\n",
      "0.95\n",
      "1. 6503 JP Equity 7269 JP Equity: Reward: 1.01525, step 2606\n",
      "2. 6326 JP Equity 6954 JP Equity: Reward: 0.69300, step 2606\n",
      "3. 8053 JP Equity 8058 JP Equity: Reward: -0.43573, step 2606\n",
      "4. 4901 JP Equity 9613 JP Equity: Reward: -0.22818, step 2606\n",
      "5. 6988 JP Equity 7267 JP Equity: Reward: 0.44917, step 2606\n",
      "6. 4901 JP Equity 6702 JP Equity: Reward: -1.12489, step 2606\n",
      "7. 4684 JP Equity 7832 JP Equity: Reward: 0.82895, step 2606\n",
      "8. 7267 JP Equity 8306 JP Equity: Reward: 0.20026, step 2606\n",
      "9. 7267 JP Equity 8801 JP Equity: Reward: 0.30389, step 2606\n",
      "10. 4519 JP Equity 7532 JP Equity: Reward: 0.21853, step 2606\n",
      "0.5 261\n",
      "0.95\n",
      "1. 6503 JP Equity 7269 JP Equity: Reward: -0.71315, step 2606\n",
      "2. 6326 JP Equity 6954 JP Equity: Reward: -0.27552, step 2606\n",
      "3. 8053 JP Equity 8058 JP Equity: Reward: 0.24031, step 2606\n",
      "4. 4901 JP Equity 9613 JP Equity: Reward: -0.19417, step 2606\n",
      "5. 6988 JP Equity 7267 JP Equity: Reward: 0.12485, step 2606\n",
      "6. 4901 JP Equity 6702 JP Equity: Reward: -0.50049, step 2606\n",
      "7. 4684 JP Equity 7832 JP Equity: Reward: 0.37105, step 2606\n",
      "8. 7267 JP Equity 8306 JP Equity: Reward: 0.19544, step 2606\n",
      "9. 7267 JP Equity 8801 JP Equity: Reward: 0.31258, step 2606\n",
      "10. 4519 JP Equity 7532 JP Equity: Reward: 0.65805, step 2606\n",
      "Epoch Total Ave Reward: [0.1921647228758977, 0.1920243147234945, 0.02189542255279333]\n",
      "Epoch Ave Ave Reward: \t0.13536\n",
      "Epoch SD  Ave Reward: \t0.08023 \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "num_episodes = 10\n",
    "\n",
    "ls_epoch_reward = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # instantiate\n",
    "    env = PairTradeEnv1()\n",
    "    agent = Agent(env=env,\n",
    "                num_states=len(env.valid_states), \n",
    "                num_actions=env.num_actions, \n",
    "                batch_size= len(top_keys)*100, \n",
    "                alpha=0.5, \n",
    "                gamma=0.5, \n",
    "                epsilon=1,\n",
    "                epsilon_decay=0.995,\n",
    "                buffer_size = 10000)\n",
    "    print(agent.gamma, env.current_step)\n",
    "    # train\n",
    "    agent.learn(num_episodes)\n",
    "    # evaluate\n",
    "    total_train_return = 0\n",
    "    for idx, _ in enumerate(top_keys):\n",
    "        total_train_return += get_baseline(env, agent, 3000, pair_idx=idx)\n",
    "\n",
    "    ls_epoch_reward.append(total_train_return/len(top_keys))\n",
    "\n",
    "print(f\"Epoch Total Ave Reward: {ls_epoch_reward}\")\n",
    "print(f\"Epoch Ave Ave Reward: \\t{np.array(ls_epoch_reward).mean():.5f}\")\n",
    "print(f\"Epoch SD  Ave Reward: \\t{np.array(ls_epoch_reward).std():.5f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14 Dec\n",
    "- experiment with running multiple gamma. \n",
    "- ls_gamma = [.5, .1, .2, .9, 0]\n",
    "    - exp1: Epoch Total Ave Reward: [0.27656047941275835, -0.0678991421108197, -0.08469468350312535, -0.1809340042907028, -0.2585470411630025]\n",
    "    - exp2: Epoch Total Ave Reward: [0.21300592294405787, 0.31927383581329816, -0.19477593352845785, -0.176282995815822, -0.  042197878268430175]\n",
    "- ls_gamma = [.9, 0, .5, .1, .2]\n",
    "    - Epoch Total Ave Reward: [0.31927383581329816, -0.12529959493439907, 0.31927383581329816, -0.2642522721225163, 0.20685181929893712]\n",
    "- settle on gamma 0.5 . try reward clipping\n",
    "- increasing reward ||| if reward > 2: reward += 2, elif reward > 1: reward += 1\n",
    "    - Epoch Total Ave Reward: [-0.01642047141259907, -0.13440731073407086, 0.07929587537637263, 0.2268478521818123, 0.1095582760267271]\n",
    "    - Epoch Ave Ave Reward: \t0.05297\n",
    "    - Epoch SD  Ave Reward: \t0.12170 \n",
    "- clipping reward ||| reward /= dailypnl_sd, reward = np.clip(reward,-1,1) \n",
    "    - Epoch Total Ave Reward: [-0.11145766569664421, -0.3913613850599167, 0.14032909285942924, -0.1606315485750662, 0.042197878268430175]\n",
    "    - Epoch Ave Ave Reward: \t-0.09618\n",
    "    - Epoch SD  Ave Reward: \t0.18254 \n",
    "- increase reward and penalty ||| , reward /= dailypnl_sd\n",
    "            if reward > 2:\n",
    "                reward += 2\n",
    "            elif reward > 1:\n",
    "                reward += 1\n",
    "            elif reward < -2:\n",
    "                reward -= 2\n",
    "            elif reward < -1:\n",
    "                reward -= 1\n",
    "    - Epoch Total Ave Reward: [0.21300592294405787, 0.1651926853830304, 0.33396093223633744, -0.1651926853830304, 0.062178140797388294]\n",
    "    - Epoch Ave Ave Reward: \t0.12183\n",
    "    - Epoch SD  Ave Reward: \t0.16801 \n",
    "- combine increase reward and clipping ||| \n",
    "            reward /= dailypnl_sd\n",
    "            if reward > 2:\n",
    "                reward = 2\n",
    "            elif reward > .5:\n",
    "                reward += 1\n",
    "            elif reward < -2:\n",
    "                reward = -2\n",
    "            elif reward < -.5:\n",
    "                reward += -1\n",
    "    - exp1 | Epoch Total Ave Reward: [-0.07351450140341671, 0.1760664446499776, -0.20093003717357863, 0.22364736069124266, -0.12529959493439907]\n",
    "    - Epoch Ave Ave Reward: \t-0.00001\n",
    "    - Epoch SD  Ave Reward: \t0.16882 \n",
    "    - exp2 | Epoch Total Ave Reward: [0.23802803428152103, 0.08925582031108967, -0.04759668639518292, 0.03709799710794239, 0.04790310922794429]\n",
    "    - Epoch Ave Ave Reward: \t0.07294\n",
    "    - Epoch SD  Ave Reward: \t0.09377 \n",
    "- combine clipping, also reward shape if further from 0, remove noise\n",
    "            reward /= dailypnl_sd\n",
    "            if reward > .1:\n",
    "                reward = max(2,reward+1)\n",
    "            elif reward < -.1:\n",
    "                reward = min(-2, reward-1)\n",
    "    - bad results\n",
    "- revert to earlier increase reward experiment and also set alpha to 0.5. later run alpha experiment\n",
    "    - reward /= dailypnl_sd\n",
    "            if reward > 2:\n",
    "                reward +=2\n",
    "            elif reward > 0.5:\n",
    "                reward +=1\n",
    "            elif reward < -2:\n",
    "                reward += -2\n",
    "            elif reward < -0.5:\n",
    "                reward += -1\n",
    "    - Epoch Total Ave Reward: [0.062178140797388294, 0.18198822677533613, -0.1245877739517566]\n",
    "    - Epoch Ave Ave Reward: \t0.03986\n",
    "    - Epoch SD  Ave Reward: \t0.12615 \n",
    "---\n",
    "- rerun all with newest reward function\n",
    "    - reward /= dailypnl_sd\n",
    "            if reward > 2:\n",
    "                reward +=2\n",
    "            elif reward > 0.5:\n",
    "                reward +=1\n",
    "            elif reward < -2:\n",
    "                reward += -2\n",
    "            elif reward < -0.5:\n",
    "                reward += -1\n",
    "    - Epoch Total Ave Reward: [0.05321204568778015, -0.06916991576129747, -0.06925978742821415]\n",
    "    - Epoch Ave Ave Reward: \t-0.02841\n",
    "    - Epoch SD  Ave Reward: \t0.05771\n",
    "    - really bad returns on both gamma =0 and >0. alpha was lower in =0.\n",
    "    - might be under fitting the reward. maybe clip the punishment and leave the rewards\n",
    "- rerun with removing the negative returns\n",
    "    - not good result\n",
    "- rerun with no reward scaling, only normalisation\n",
    "    - small bactch run\n",
    "    - gamma =0 || Epoch Total Ave Reward: [-0.14945136647535748, -0.04198132710258564, -0.09593289795481606]\n",
    "        - Epoch Ave Ave Reward: \t-0.09579\n",
    "        - Epoch SD  Ave Reward: \t0.04387 \n",
    "    - gamma <0 || Epoch Total Ave Reward: [0.1921647228758977, 0.1920243147234945, 0.02189542255279333]\n",
    "        - Epoch Ave Ave Reward: \t0.13536\n",
    "        - Epoch SD  Ave Reward: \t0.08023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>mean_reversion</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(0, 0, 0, 1, 0, 0)</th>\n",
       "      <td>0.150154</td>\n",
       "      <td>0.297776</td>\n",
       "      <td>-0.337173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0, 0, 0, 1, 1, 0)</th>\n",
       "      <td>0.100002</td>\n",
       "      <td>0.281074</td>\n",
       "      <td>0.753829</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0, 0, 0, 1, 1, 1)</th>\n",
       "      <td>-0.598842</td>\n",
       "      <td>0.217815</td>\n",
       "      <td>-0.329860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0, 0, 1, 0, 0, 0)</th>\n",
       "      <td>0.346429</td>\n",
       "      <td>0.566151</td>\n",
       "      <td>0.516133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0, 1, 1, 0, 0, 0)</th>\n",
       "      <td>-0.048844</td>\n",
       "      <td>0.622264</td>\n",
       "      <td>1.765002</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 1, 1, 0, 0, 0)</th>\n",
       "      <td>-0.047397</td>\n",
       "      <td>0.142649</td>\n",
       "      <td>-0.169307</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0, 0, 0, 0, 0, 0)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          -1         0         1  mean_reversion position\n",
       "(0, 0, 0, 1, 0, 0)  0.150154  0.297776 -0.337173               0        0\n",
       "(0, 0, 0, 1, 1, 0)  0.100002  0.281074  0.753829               1        1\n",
       "(0, 0, 0, 1, 1, 1) -0.598842  0.217815 -0.329860               0        0\n",
       "(0, 0, 1, 0, 0, 0)  0.346429  0.566151  0.516133               0        0\n",
       "(0, 1, 1, 0, 0, 0) -0.048844  0.622264  1.765002              -1        1\n",
       "(1, 1, 1, 0, 0, 0) -0.047397  0.142649 -0.169307               0        0\n",
       "(0, 0, 0, 0, 0, 0)  0.000000  0.000000  0.000000               0       -1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(agent.Q, index=env.valid_states, columns=[-1, 0, 1])\n",
    "df['mean_reversion'] = [0,1,0,0,-1,0,0]\n",
    "df['position'] = df[[-1, 0, 1]].idxmax(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv(f\"q_table_gamma{agent.gamma}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0, 0, 1, 0, 0): 0,\n",
       " (0, 0, 0, 1, 1, 0): 1,\n",
       " (0, 0, 0, 1, 1, 1): 0,\n",
       " (0, 0, 1, 0, 0, 0): 0,\n",
       " (0, 1, 1, 0, 0, 0): 1,\n",
       " (1, 1, 1, 0, 0, 0): 0,\n",
       " (0, 0, 0, 0, 0, 0): -1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_policy_actions = dict(df.position)\n",
    "dict_policy_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "table is taking a long time to generalise. using one pair is not good enough to get any poistioning as everything is flat. only after adding all 10 pairs then performance churns out shorting on 1 SD high cross.\n",
    "\n",
    "Experiment: try making gamma 0.1, Q table should closely mimic the state space analysis table with strong mean reversion tendency. This is not what we see. and it even has Q values opposite to state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Absolute ==========\n",
      "total return \t\t0.00195\n",
      "total train return \t0.03629\n",
      "total test return \t-0.03433\n",
      "========== Normalised ==========\n",
      "total return \t\t0.00000\n",
      "total train return \t0.00001\n",
      "total test return \t-0.00031\n"
     ]
    }
   ],
   "source": [
    "workingPairQresults = {}\n",
    "for pair_idx in range(len(top_keys)):\n",
    "    df = pd.DataFrame(workingPairOutcome[top_keys[pair_idx]], columns=dummy_df.columns)\n",
    "\n",
    "    # Assign policy values using the dictionary\n",
    "    df['position'] = df.apply(lambda row: dict_policy_actions.get(tuple(row), np.nan), axis=1)\n",
    "    df[df.isna().any(axis=1)]\n",
    "    pair = validPairsList[pair_idx]\n",
    "    #Calculate PnL\n",
    "    df[f'{pair[0]} position'] = df['position']\n",
    "    df[f'{pair[1]} position'] = df['position'] * -1\n",
    "    df['dailypnl'] = df[f'{pair[1]} position'].values*return_df[f'{pair[1]}'].shift(-1).values \\\n",
    "                    + df[f'{pair[0]} position'].values*return_df[f'{pair[0]}'].shift(-1).values\n",
    "    df['cumpnl'] = df['dailypnl'].cumsum()\n",
    "    workingPairQresults[top_keys[pair_idx]] = df\n",
    "\n",
    "total_q_return = 0\n",
    "total_train_q_return = 0\n",
    "total_test_q_return = 0\n",
    "\n",
    "for pair in top_keys:\n",
    "    total_q_return += workingPairQresults[pair]['cumpnl'].iloc[-2]\n",
    "    total_train_q_return += workingPairQresults[pair]['cumpnl'].iloc[2868-2]\n",
    "\n",
    "print(\"=\"*10, \"Absolute\", \"=\"*10)\n",
    "print(f\"total return \\t\\t{total_q_return/len(top_keys):.5f}\")\n",
    "print(f\"total train return \\t{total_train_q_return/len(top_keys):.5f}\")\n",
    "print(f\"total test return \\t{(total_q_return - total_train_q_return)/len(top_keys):.5f}\")\n",
    "print(\"=\"*10, \"Normalised\", \"=\"*10)\n",
    "print(f\"total return \\t\\t{total_q_return/len(top_keys)/(2979-261):.5f}\")\n",
    "print(f\"total train return \\t{total_train_q_return/len(top_keys)/(2868-261):.5f}\")\n",
    "print(f\"total test return \\t{(total_q_return - total_train_q_return)/len(top_keys)/(2979-2868):.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## anaconda3 (Python 3.12.0) Kernel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "#f for q\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# pair trade packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pairs Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`validPairs4.csv` already have TOPIX stocks with highest liquidity and are tested for stationarity over a 1 year window\n",
    "\n",
    "Choose top 10 known pair trades by returns in the total dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import load_data\n",
    "\n",
    "workingPairOutcome, top_keys, validPairsList, return_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 performing trades:\n",
      "1. Pair: 1801 JP Equity 2670 JP Equity\n",
      "2. Pair: 3778 JP Equity 6701 JP Equity\n",
      "3. Pair: 2760 JP Equity 6254 JP Equity\n",
      "4. Pair: 5706 JP Equity 6954 JP Equity\n",
      "5. Pair: 7951 JP Equity 9684 JP Equity\n",
      "6. Pair: 1808 JP Equity 6481 JP Equity\n",
      "7. Pair: 3099 JP Equity 5831 JP Equity\n",
      "8. Pair: 1808 JP Equity 6971 JP Equity\n",
      "9. Pair: 4021 JP Equity 9843 JP Equity\n",
      "10. Pair: 5929 JP Equity 6504 JP Equity\n"
     ]
    }
   ],
   "source": [
    "# Print the top 10 performing trades\n",
    "print(\"Top 10 performing trades:\")\n",
    "for i, key in enumerate(top_keys,1):\n",
    "    print(f\"{i}. Pair: {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(             spread  1sd high  1sd low  2sd high  2sd low\n",
       " Date                                                     \n",
       " 27/5/2024  0.019074       1.0     -1.0       2.0     -2.0\n",
       " 28/5/2024  0.680074       1.0     -1.0       2.0     -2.0\n",
       " 29/5/2024  1.055247       1.0     -1.0       2.0     -2.0\n",
       " 30/5/2024  0.595548       1.0     -1.0       2.0     -2.0\n",
       " 31/5/2024  0.143814       1.0     -1.0       2.0     -2.0,\n",
       " (2979, 5))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sample pair data \n",
    "workingPairOutcome[top_keys[0]].tail(), workingPairOutcome[top_keys[0]].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make indicators and spread stationary around 0\n",
    "Deduct the mean from all values to translate to 0 axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test one timestep at a time (even though we can test all at the same time)\n",
    "- give state\n",
    "- Trading should be path dependent due to stop loss. in this case I can only give last position as one of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Challenge\n",
    "\n",
    "## Background\n",
    "Initial evaluation of the baseline portfolio shows that draw downs are small. Originally team had the idea of using Machine Learning to optimise for sizing of these pair trades. However since there was no significant drawdowns the returns are linearly increasing with investment sizing i.e. greater nominal investment in the the pair trade the proportionate increase in returns without realising significant drawdown risk.\n",
    "\n",
    "Instead of optimising for sizing, we can explore Machine Learning in terms of strategy on this stationary dataset. Whereas our prescribed strategy is to enter at +/- 1 std dev, exit at 0 with +/- 2 std dev stop loss. These are only suggestions and arbitrary levels.\n",
    "\n",
    "With Machine Learning, we can discover if it will uncover the mean reverting nature and recommend another threshhold. We use Q Learner to understand state space with the same spread, mid, std dev parameters as the baseline.\n",
    "\n",
    "### Steps\n",
    "#### Environment:\n",
    "- State Space: A set of all possible states the agent can be in.  \n",
    "  - [spread, mid, 2 sd low, 1 sd low, 1 sd high, 2 sd high]\n",
    "- Action Space: A set of all possible actions the agent can take in each state.   \n",
    "  - [-1, # short\\\n",
    "      0, # uninvested\\\n",
    "      1  # long]   \n",
    "- Reward Function: A function that assigns a numerical reward to each state-action pair, indicating the immediate consequence of taking a particular action in a specific state.\n",
    "  - dailypnl\n",
    "- Transition Function: A function that determines the probability of transitioning from one state to another when a particular action is taken.\n",
    "  - deterministic based on historical performance\n",
    "#### Agent:\n",
    "\n",
    "- Q-Table: A matrix that stores the estimated Q-values for each state-action pair. Q-values represent the expected future reward for taking a specific action in a given state.   \n",
    "  - continuous Q table?\n",
    "- Learning Rate (α): A parameter that controls how much the Q-values are updated with each new experience.   \n",
    "- Discount Factor (γ): A parameter that determines the importance of future rewards. A higher discount factor gives more weight to future rewards.   \n",
    "- Exploration Rate (ε): A parameter that controls the balance between exploration (trying new actions) and exploitation (choosing the action with the highest Q-value).   \n",
    "- Q-Learning Algorithm:\n",
    "\n",
    "  - Initialization: Initialize the Q-table with random values or zeros.   \n",
    "  - Exploration and Exploitation: Use an exploration strategy (e.g., ε-greedy) to choose an action:\n",
    "    - With probability ε, choose a random action.   \n",
    "    - With probability 1-ε, choose the action with the highest Q-value for the current state.   \n",
    "  \n",
    "  - Take Action: Execute the chosen action in the environment.   \n",
    "  - Observe Reward and Next State: Observe the immediate reward and the next state resulting from the action.\n",
    "- Update Q-Value: Update the Q-value of the current state-action pair using the following formula:\n",
    "\n",
    "#### Training and Test set\n",
    "\n",
    "2013 is used for warm start\\\n",
    "2014 - 2023 train data since NN need a lot of training data {end 2023 idx == 2868}\\\n",
    "2024 onwards (5 months) test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q_agent import MultiStockEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MultiStockEnv(workingPairOutcome, top_keys, validPairsList, return_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2868"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.last_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.017012574193340124"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# TODO NEED TO CHANGE GET BASELINE TO GET REWARD FOR EACH OF THE STOCKS FOR THAT TIME STEP\n",
    "\n",
    "\"\"\"\n",
    "env.current_step = 265\n",
    "env.calculate_reward(np.ones(10)*-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary loaded from pairsOutcome.pkl\n"
     ]
    }
   ],
   "source": [
    "with open('pairsOutcome.pkl', 'rb') as file:\n",
    "    pairsOutcome = pickle.load(file)\n",
    "\n",
    "print(\"Dictionary loaded from pairsOutcome.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spread</th>\n",
       "      <th>mid</th>\n",
       "      <th>1sd high</th>\n",
       "      <th>1sd low</th>\n",
       "      <th>2sd high</th>\n",
       "      <th>2sd low</th>\n",
       "      <th>position</th>\n",
       "      <th>1801 JP Equity position</th>\n",
       "      <th>2670 JP Equity position</th>\n",
       "      <th>dailypnl</th>\n",
       "      <th>cumpnl</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1/1/2013</th>\n",
       "      <td>-829.459706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2/1/2013</th>\n",
       "      <td>-829.459706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3/1/2013</th>\n",
       "      <td>-829.459706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4/1/2013</th>\n",
       "      <td>-788.012196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7/1/2013</th>\n",
       "      <td>-751.666698</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27/5/2024</th>\n",
       "      <td>-39.304773</td>\n",
       "      <td>-46.289714</td>\n",
       "      <td>319.903354</td>\n",
       "      <td>-412.482783</td>\n",
       "      <td>686.096423</td>\n",
       "      <td>-778.675852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.552379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28/5/2024</th>\n",
       "      <td>204.731719</td>\n",
       "      <td>-44.231112</td>\n",
       "      <td>321.850855</td>\n",
       "      <td>-410.313080</td>\n",
       "      <td>687.932823</td>\n",
       "      <td>-776.395048</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>2.552379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29/5/2024</th>\n",
       "      <td>345.042254</td>\n",
       "      <td>-41.541554</td>\n",
       "      <td>324.802844</td>\n",
       "      <td>-407.885953</td>\n",
       "      <td>691.147243</td>\n",
       "      <td>-774.230351</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02741</td>\n",
       "      <td>2.579789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30/5/2024</th>\n",
       "      <td>178.588811</td>\n",
       "      <td>-39.334254</td>\n",
       "      <td>326.585734</td>\n",
       "      <td>-405.254242</td>\n",
       "      <td>692.505722</td>\n",
       "      <td>-771.174230</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.579789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31/5/2024</th>\n",
       "      <td>15.040724</td>\n",
       "      <td>-37.437415</td>\n",
       "      <td>327.466148</td>\n",
       "      <td>-402.340979</td>\n",
       "      <td>692.369711</td>\n",
       "      <td>-767.244542</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2979 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               spread        mid    1sd high     1sd low    2sd high  \\\n",
       "Date                                                                   \n",
       "1/1/2013  -829.459706        NaN         NaN         NaN         NaN   \n",
       "2/1/2013  -829.459706        NaN         NaN         NaN         NaN   \n",
       "3/1/2013  -829.459706        NaN         NaN         NaN         NaN   \n",
       "4/1/2013  -788.012196        NaN         NaN         NaN         NaN   \n",
       "7/1/2013  -751.666698        NaN         NaN         NaN         NaN   \n",
       "...               ...        ...         ...         ...         ...   \n",
       "27/5/2024  -39.304773 -46.289714  319.903354 -412.482783  686.096423   \n",
       "28/5/2024  204.731719 -44.231112  321.850855 -410.313080  687.932823   \n",
       "29/5/2024  345.042254 -41.541554  324.802844 -407.885953  691.147243   \n",
       "30/5/2024  178.588811 -39.334254  326.585734 -405.254242  692.505722   \n",
       "31/5/2024   15.040724 -37.437415  327.466148 -402.340979  692.369711   \n",
       "\n",
       "              2sd low  position  1801 JP Equity position  \\\n",
       "Date                                                       \n",
       "1/1/2013          NaN         0                        0   \n",
       "2/1/2013          NaN         0                        0   \n",
       "3/1/2013          NaN         0                        0   \n",
       "4/1/2013          NaN         0                        0   \n",
       "7/1/2013          NaN         0                        0   \n",
       "...               ...       ...                      ...   \n",
       "27/5/2024 -778.675852         0                        0   \n",
       "28/5/2024 -776.395048         0                        0   \n",
       "29/5/2024 -774.230351        -1                       -1   \n",
       "30/5/2024 -771.174230         0                        0   \n",
       "31/5/2024 -767.244542         0                        0   \n",
       "\n",
       "           2670 JP Equity position  dailypnl    cumpnl  \n",
       "Date                                                    \n",
       "1/1/2013                         0   0.00000  0.000000  \n",
       "2/1/2013                         0   0.00000  0.000000  \n",
       "3/1/2013                         0   0.00000  0.000000  \n",
       "4/1/2013                         0   0.00000  0.000000  \n",
       "7/1/2013                         0  -0.00000  0.000000  \n",
       "...                            ...       ...       ...  \n",
       "27/5/2024                        0   0.00000  2.552379  \n",
       "28/5/2024                        0  -0.00000  2.552379  \n",
       "29/5/2024                        1   0.02741  2.579789  \n",
       "30/5/2024                        0   0.00000  2.579789  \n",
       "31/5/2024                        0       NaN       NaN  \n",
       "\n",
       "[2979 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## need to go through each df and get the position then join all of them together\n",
    "pairsOutcome[top_keys[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseline \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt_pair\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mget_baseline\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps_per_episode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_pair\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m, in \u001b[0;36mget_baseline\u001b[0;34m(env, max_steps_per_episode, t_pair)\u001b[0m\n\u001b[1;32m     10\u001b[0m env\u001b[38;5;241m.\u001b[39mlast_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2868\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps_per_episode):\n\u001b[0;32m---> 13\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mworkingPairOutcome\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt_pair\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39miloc[env\u001b[38;5;241m.\u001b[39mcurrent_step][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m     _, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     15\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "## Get baseline results\n",
    "t_pair = validPairsList[0]\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "def get_baseline(env, max_steps_per_episode, t_pair):\n",
    "    env.reset()\n",
    "    total_reward = 0\n",
    "    current_step = 261\n",
    "    env.current_step = current_step\n",
    "    env.last_step = 2868\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        action = workingPairOutcome[t_pair].iloc[env.current_step]['position']\n",
    "        _, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Baseline {t_pair}, Total Reward: {total_reward}, step {step}\")\n",
    "\n",
    "get_baseline(env, max_steps_per_episode, t_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class MultiStockEnv(gym.Env):\n",
    "    def __init__(self, workingPairOutcome, top_keys, validPairsList, return_df):\n",
    "        self.workingPairOutcome = workingPairOutcome\n",
    "        self.top_keys = top_keys\n",
    "        self.validPairsList = validPairsList\n",
    "        self.return_df = return_df\n",
    "        \n",
    "        self.earliest_step = 261  # Starting step\n",
    "        self.last_step = 2868  # Ending step\n",
    "        self.current_step = self.earliest_step\n",
    "        \n",
    "        # Number of stocks and possible outcomes per stock (3 outcomes per stock)\n",
    "        self.num_stocks = 10\n",
    "        self.num_actions = 3  # Buy, hold, sell\n",
    "        \n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            actions: List of actions (length of 10, each corresponding to a stock)\n",
    "        Output:\n",
    "            next_state: next state 5 features\n",
    "            reward: total reward for this timestep\n",
    "            done: boolean for if end of dataset\n",
    "            info: optional\n",
    "        \"\"\"\n",
    "        # Advance the time step\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.last_step\n",
    "\n",
    "        # Get the state for each stock\n",
    "        state = np.zeros((self.num_stocks, 5))  # 10 stocks with 3 possible outcomes\n",
    "        for i in range(self.num_stocks):\n",
    "            # Get the current state (outcomes) for the stock\n",
    "            state[i] = self.workingPairOutcome[self.top_keys[i]].iloc[self.current_step].values\n",
    "        \n",
    "        # Calculate the reward (based on action for each stock)\n",
    "        reward = self.calculate_reward(actions)\n",
    "        \n",
    "        # Provide next state\n",
    "        next_state = state.flatten()  # Flatten to 1D array for the agent\n",
    "        info = {}  # Optional information\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Reset to the starting point of the dataset \"\"\"\n",
    "        self.current_step = self.earliest_step\n",
    "        state = np.zeros((self.num_stocks, 5))  # Initialize state for all stocks\n",
    "        for i in range(self.num_stocks):\n",
    "            # Set the state for each stock (first row from each stock's data)\n",
    "            state[i] = self.workingPairOutcome[self.top_keys[i]].iloc[self.current_step].values\n",
    "        \n",
    "        return state.flatten()  # Return flattened state\n",
    "    \n",
    "    def calculate_reward(self, actions):\n",
    "        \"\"\" Calculate reward for the actions taken for each stock \"\"\"\n",
    "        reward = 0\n",
    "        for i in range(self.num_stocks):\n",
    "            position = actions[i]  # Action for the current stock (buy, hold, sell)\n",
    "            reward += self.stock_reward(position, self.current_step, self.validPairsList[i])\n",
    "        return reward\n",
    "    \n",
    "    def stock_reward(self, position, idx, pair):\n",
    "        \"\"\" Compute reward for each stock based on position and return data \"\"\"\n",
    "        position_0 = position\n",
    "        position_1 = position * -1\n",
    "        dailypnl = position_0 * self.return_df[f'{pair[0]}'].iloc[idx] + position_1 * self.return_df[f'{pair[1]}'].iloc[idx]\n",
    "        return dailypnl\n",
    "\n",
    "# Instantiate the environment\n",
    "env = MultiStockEnv(workingPairOutcome, top_keys, validPairsList, return_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h2/r7qn2m9n1zb6y_0q191gdqth0000gn/T/ipykernel_2980/3115168106.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_experience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/h2/r7qn2m9n1zb6y_0q191gdqth0000gn/T/ipykernel_2980/4101604773.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Calculate the reward (based on action for each stock)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Provide next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/h2/r7qn2m9n1zb6y_0q191gdqth0000gn/T/ipykernel_2980/4101604773.py\u001b[0m in \u001b[0;36mcalculate_reward\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_stocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Action for the current stock (buy, hold, sell)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstock_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidPairsList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, input_size, output_size, learning_rate, discount_factor, epsilon, epsilon_decay, batch_size=30, replay_buffer_size=10000):\n",
    "        self.q_network = QNetwork(input_size, output_size)\n",
    "        self.target_network = QNetwork(input_size, output_size)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "        \n",
    "        # Action to index mapping\n",
    "        self.action_to_index = {-1: 0, 0: 1, 1: 2}\n",
    "        self.index_to_action = {0: -1, 1: 0, 2: 1}\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Randomly select actions for each stock\n",
    "            actions = np.random.choice([-1, 0, 1], size=10).tolist()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "                q_values = self.q_network(state_tensor)\n",
    "                \n",
    "                # Get the best action index for each stock\n",
    "                action_indices = torch.argmax(q_values, dim=1).unsqueeze(0).tolist()\n",
    "                actions = []\n",
    "                for idx in action_indices:\n",
    "                    action = self.index_to_action[idx[0]]  # Use idx directly as the key\n",
    "                    actions.append(action)\n",
    "            \n",
    "        return actions\n",
    "\n",
    "    def store_experience(self, state, actions, reward, next_state, done):\n",
    "        for action in actions:  # Store each stock's action separately\n",
    "            self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "\n",
    "        # Convert actions to indices (ensure actions are single values)\n",
    "        actions = torch.tensor([self.action_to_index[action] for action in actions], dtype=torch.long).view(-1, 1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).view(-1, 1)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        # Q-value updates\n",
    "        q_values = self.q_network(states).gather(1, actions)\n",
    "        next_q_values = self.target_network(next_states).max(1, keepdim=True)[0].detach()\n",
    "        target_q_values = rewards + self.discount_factor * next_q_values * (1 - dones)\n",
    "\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network and decay epsilon\n",
    "        self.update_target_network()\n",
    "        self.epsilon = max(0.01, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "# Example usage:\n",
    "input_size = 50  # 10 stocks * 5 indicators per stock\n",
    "output_size = 30  # Action space per stock\n",
    "learning_rate = 0.25\n",
    "discount_factor = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.99999\n",
    "ls_total_reward = []\n",
    "total_episodes = 1_000_000\n",
    "\n",
    "agent = QLearningAgent(input_size, output_size, learning_rate, discount_factor, epsilon, epsilon_decay)\n",
    "\n",
    "# Simulating agent learning (in practice, use a loop with environment interaction)\n",
    "for episode in range(total_episodes):\n",
    "    state = env.reset() \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        agent.store_experience(state, action, reward, next_state, done)\n",
    "        agent.learn()\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    ls_total_reward.append(total_reward)\n",
    "        \n",
    "    print(f\"Episode {episode+1}: Total Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/r7qn2m9n1zb6y_0q191gdqth0000gn/T/ipykernel_2765/76276489.py:1: RuntimeWarning: Mean of empty slice.\n",
      "  np.array(ls_total_reward[-100:]).mean()\n",
      "/Users/ju/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(ls_total_reward[-100:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1458872436.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/h2/r7qn2m9n1zb6y_0q191gdqth0000gn/T/ipykernel_43194/1458872436.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    max_steps_per_episode =\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Get baseline results\n",
    "t_pair = validPairsList[0]\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "def get_baseline(env, max_steps_per_episode, t_pair):\n",
    "    env.reset()\n",
    "    total_reward = 0\n",
    "    current_step = 261\n",
    "    env.current_step = current_step\n",
    "    env.last_step = 2868\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        action = pairsOutcome[t_pair].iloc[env.current_step]['position']\n",
    "        _, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Baseline {t_pair}, Total Reward: {total_reward}, step {step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline 1801 JP Equity 2670 JP Equity, Total Reward: 2.3267375595549673, step 2606\n"
     ]
    }
   ],
   "source": [
    "get_baseline(env, 3000, top_keys[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- first few tries, network is very large\n",
    "- added epsilon search in \"choose_action\" functionso that there will be some chance to explore\n",
    "- changed reward function to multiply losses and give exponential returns to incentivise risk taking\n",
    "\n",
    "### 1 dec 2105: \n",
    "- might have performance is always oscillating negative and positive. This might be because of too large a learning rate. also start from start of training periods max steps to be 3000 so that total results are comparable\n",
    "    - this helped quite abit. \n",
    "`\n",
    "input_size = 7  # Adjust to your specific input size\n",
    "output_size = 3  # Adjust to your desired number of discrete actions\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.8\n",
    "epsilon = 1 # down to .3\n",
    "epsilon_decay = 0.9999\n",
    "num_episodes = 500\n",
    "max_steps_per_episode = 3000\n",
    "`\n",
    "- want to try changing epsilon to only update after the entire episode instead of after each step. its decaying too quickly\n",
    "    - \n",
    "- I want to try with changing reward by changing \"learn\" to use total_reward instead of \"reward\"\n",
    "- Scale the states. need to explore scaling the state since it is still in terms of absolute differences. NN is not able to do proportions\n",
    "- training epochs should be smaller at up to 30 days because mean reversion pattern is 1 to 33 days\n",
    "    - very bad performance with 40 day epochs\n",
    "\n",
    "### 1 dec 2217:\n",
    "- changed target q value fxn to remove exponential reward and scaled negative reward. now both positive and negative are the same. added portion of total reward in episode to incentivise more long term rewards.\n",
    "    - `        if reward > 0:\n",
    "            target_q_value = reward + self.discount_factor * next_q_value * (1 - done) + total_reward * .1\n",
    "        else:\n",
    "            target_q_value = reward + self.discount_factor * next_q_value * (1 - done) + total_reward * .1`\n",
    "    -       `  if episode%1==0:\n",
    "            agent.epsilon *= agent.epsilon_decay`\n",
    "\n",
    "### 2 Dec 2101:\n",
    "- managed to scale but results are not any better\n",
    "- thinking of reducing learning rate to reduce the oscillations\n",
    "    - will try to run with learning rate at 0.01\n",
    "- right now total reward is taking all of the target q function. maybe can make it a 50/50 split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

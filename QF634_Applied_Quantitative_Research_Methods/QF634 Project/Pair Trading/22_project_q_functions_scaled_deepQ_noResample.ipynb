{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## anaconda3 (Python 3.12.0) Kernel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# pair trade packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pairs Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary loaded from pairsOutcome.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/r7qn2m9n1zb6y_0q191gdqth0000gn/T/ipykernel_3474/3199335484.py:12: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  tpxData = pd.read_csv('TPX_prices.csv', index_col=0, parse_dates=True, date_parser=custom_date_parser)\n"
     ]
    }
   ],
   "source": [
    "def custom_date_parser(date_str):\n",
    "    return datetime.strptime(date_str, '%d/%m/%Y')\n",
    "\n",
    "# Load the dictionary from the pickle file\n",
    "with open('pairsOutcome.pkl', 'rb') as file:\n",
    "    pairsOutcome = pickle.load(file)\n",
    "\n",
    "print(\"Dictionary loaded from pairsOutcome.pkl\")\n",
    "\n",
    "\n",
    "# Load stock data and get return \n",
    "tpxData = pd.read_csv('TPX_prices.csv', index_col=0, parse_dates=True, date_parser=custom_date_parser)\n",
    "tpxData = tpxData.dropna(axis='columns')\n",
    "return_df = (tpxData / tpxData.shift(1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Pair Trade Portfolio\n",
    "`pairsOutcome` already have TOPIX stocks with highest liquidity and are tested for stationarity over a 1 year window\n",
    "\n",
    "Choose top 10 known pair trades by returns in the total dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_pairs = [('5401 JP Equity', '8604 JP Equity'), ('6273 JP Equity', '9984 JP Equity'), ('8053 JP Equity', '8058 JP Equity'), ('7733 JP Equity', '9613 JP Equity'), ('4684 JP Equity', '7832 JP Equity'), ('6762 JP Equity', '6857 JP Equity'), ('9020 JP Equity', '9022 JP Equity'), ('7267 JP Equity', '8306 JP Equity'), ('8308 JP Equity', '8802 JP Equity'), ('4901 JP Equity', '6702 JP Equity'), ('6503 JP Equity', '7269 JP Equity'), ('7267 JP Equity', '8801 JP Equity'), ('4519 JP Equity', '7532 JP Equity'), ('6988 JP Equity', '7267 JP Equity'), ('6326 JP Equity', '6954 JP Equity'), ('6752 JP Equity', '8604 JP Equity'), ('4901 JP Equity', '9613 JP Equity')]\n",
    "# TODO update working pairs with new 10\n",
    "top_keys = [f\"{pair[0]} {pair[1]}\" for pair in working_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5401 JP Equity 8604 JP Equity',\n",
       " '6273 JP Equity 9984 JP Equity',\n",
       " '8053 JP Equity 8058 JP Equity',\n",
       " '7733 JP Equity 9613 JP Equity',\n",
       " '4684 JP Equity 7832 JP Equity',\n",
       " '6762 JP Equity 6857 JP Equity',\n",
       " '9020 JP Equity 9022 JP Equity',\n",
       " '7267 JP Equity 8306 JP Equity',\n",
       " '8308 JP Equity 8802 JP Equity',\n",
       " '4901 JP Equity 6702 JP Equity',\n",
       " '6503 JP Equity 7269 JP Equity',\n",
       " '7267 JP Equity 8801 JP Equity',\n",
       " '4519 JP Equity 7532 JP Equity',\n",
       " '6988 JP Equity 7267 JP Equity',\n",
       " '6326 JP Equity 6954 JP Equity',\n",
       " '6752 JP Equity 8604 JP Equity',\n",
       " '4901 JP Equity 9613 JP Equity']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 performing trades:\n",
      "1. Key: 5401 JP Equity 8604 JP Equity, Return: 0.01\n",
      "2. Key: 6273 JP Equity 9984 JP Equity, Return: 0.99\n",
      "3. Key: 8053 JP Equity 8058 JP Equity, Return: 0.52\n",
      "4. Key: 7733 JP Equity 9613 JP Equity, Return: 0.34\n",
      "5. Key: 4684 JP Equity 7832 JP Equity, Return: 0.89\n",
      "6. Key: 6762 JP Equity 6857 JP Equity, Return: -0.67\n",
      "7. Key: 9020 JP Equity 9022 JP Equity, Return: 0.31\n",
      "8. Key: 7267 JP Equity 8306 JP Equity, Return: 1.16\n",
      "9. Key: 8308 JP Equity 8802 JP Equity, Return: 0.43\n",
      "10. Key: 4901 JP Equity 6702 JP Equity, Return: -0.34\n",
      "11. Key: 6503 JP Equity 7269 JP Equity, Return: 1.33\n",
      "12. Key: 7267 JP Equity 8801 JP Equity, Return: 0.64\n",
      "13. Key: 4519 JP Equity 7532 JP Equity, Return: 1.14\n",
      "14. Key: 6988 JP Equity 7267 JP Equity, Return: 0.65\n",
      "15. Key: 6326 JP Equity 6954 JP Equity, Return: 1.19\n",
      "16. Key: 6752 JP Equity 8604 JP Equity, Return: -0.48\n",
      "17. Key: 4901 JP Equity 9613 JP Equity, Return: 1.10\n"
     ]
    }
   ],
   "source": [
    "# # Sort the keys by their cumpnl[-2] values in descending order\n",
    "# top_keys = sorted(\n",
    "#     pairsOutcome,\n",
    "#     key=lambda k: pairsOutcome[k].cumpnl.iloc[-2],  # Access cumpnl[-2] safely\n",
    "#     reverse=True\n",
    "# )[:10]  # Get the top 10 keys\n",
    "\n",
    "# Print the top 10 performing trades\n",
    "print(\"Top 10 performing trades:\")\n",
    "for i, key in enumerate(top_keys, 1):\n",
    "    print(f\"{i}. Key: {key}, Return: {pairsOutcome[key].cumpnl.iloc[-2]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Challenge\n",
    "\n",
    "## Background\n",
    "Initial evaluation of the baseline portfolio shows that draw downs are small. Originally team had the idea of using Machine Learning to optimise for sizing of these pair trades. However since there was no significant drawdowns the returns are linearly increasing with investment sizing i.e. greater nominal investment in the the pair trade the proportionate increase in returns without realising significant drawdown risk.\n",
    "\n",
    "Instead of optimising for sizing, we can explore Machine Learning in terms of strategy on this stationary dataset. Whereas our prescribed strategy is to enter at +/- 1 std dev, exit at 0 with +/- 2 std dev stop loss. These are only suggestions and arbitrary levels.\n",
    "\n",
    "With Machine Learning, we can discover if it will uncover the mean reverting nature and recommend another threshhold. We use Q Learner to understand state space with the same spread, mid, std dev parameters as the baseline.\n",
    "\n",
    "### Steps\n",
    "#### Environment:\n",
    "- State Space: A set of all possible states the agent can be in.  \n",
    "  - [spread, mid, 2 sd low, 1 sd low, 1 sd high, 2 sd high]\n",
    "- Action Space: A set of all possible actions the agent can take in each state.   \n",
    "  - [-1, # short\\\n",
    "      0, # uninvested\\\n",
    "      1  # long]   \n",
    "- Reward Function: A function that assigns a numerical reward to each state-action pair, indicating the immediate consequence of taking a particular action in a specific state.\n",
    "  - dailypnl\n",
    "- Transition Function: A function that determines the probability of transitioning from one state to another when a particular action is taken.\n",
    "  - deterministic based on historical performance\n",
    "#### Agent:\n",
    "\n",
    "- Q-Table: A matrix that stores the estimated Q-values for each state-action pair. Q-values represent the expected future reward for taking a specific action in a given state.   \n",
    "  - continuous Q table?\n",
    "- Learning Rate (α): A parameter that controls how much the Q-values are updated with each new experience.   \n",
    "- Discount Factor (γ): A parameter that determines the importance of future rewards. A higher discount factor gives more weight to future rewards.   \n",
    "- Exploration Rate (ε): A parameter that controls the balance between exploration (trying new actions) and exploitation (choosing the action with the highest Q-value).   \n",
    "- Q-Learning Algorithm:\n",
    "\n",
    "  - Initialization: Initialize the Q-table with random values or zeros.   \n",
    "  - Exploration and Exploitation: Use an exploration strategy (e.g., ε-greedy) to choose an action:\n",
    "    - With probability ε, choose a random action.   \n",
    "    - With probability 1-ε, choose the action with the highest Q-value for the current state.   \n",
    "  \n",
    "  - Take Action: Execute the chosen action in the environment.   \n",
    "  - Observe Reward and Next State: Observe the immediate reward and the next state resulting from the action.\n",
    "- Update Q-Value: Update the Q-value of the current state-action pair using the following formula:\n",
    "\n",
    "#### Training and Test set\n",
    "\n",
    "2013 is used for warm start\\\n",
    "2014 - 2023 train data since NN need a lot of training data {end 2023 idx == 2868}\\\n",
    "2024 onwards (5 months) test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/r7qn2m9n1zb6y_0q191gdqth0000gn/T/ipykernel_3474/2263820838.py:4: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  valid = pd.read_csv('validPairs5.csv',\n"
     ]
    }
   ],
   "source": [
    "## Get pair stock data\n",
    "def custom_date_parser(date_str):\n",
    "    return datetime.strptime(date_str, '%d/%m/%Y')\n",
    "valid = pd.read_csv('validPairs5.csv', \n",
    "                    index_col=0, \n",
    "                    parse_dates=True, \n",
    "                    date_parser=custom_date_parser)\n",
    "## get list of pair stocks\n",
    "validPairsList = [\n",
    "    [item.strip() + ' Equity' for item in pair.split('Equity') if item.strip()]\n",
    "    for pair in top_keys\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollingWindow = 262\n",
    "cutLossSd = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in validPairsList:\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    #Calculate Standard Deviations\n",
    "    df['spread'] = valid[f'spread_{pair[0]}_{pair[1]}']\n",
    "    df['mid'] =  df['spread'].rolling(rollingWindow).mean()\n",
    "    df['1sd high'] = df['spread'].rolling(rollingWindow).mean() + df['spread'].rolling(rollingWindow).std()\n",
    "    df['1sd low'] = df['spread'].rolling(rollingWindow).mean() - df['spread'].rolling(rollingWindow).std()\n",
    "    df['2sd high'] = df['spread'].rolling(rollingWindow).mean() + df['spread'].rolling(rollingWindow).std() * cutLossSd\n",
    "    df['2sd low'] = df['spread'].rolling(rollingWindow).mean() - df['spread'].rolling(rollingWindow).std() * cutLossSd\n",
    "    df['position'] = 0\n",
    "\n",
    "    df.loc[(df['spread'] > df['1sd high']) & (df['spread'] < df['2sd high']), 'position'] = -1\n",
    "    df.loc[(df['spread']< df['1sd low']) & (df['spread'] > df['2sd low']), 'position'] = 1\n",
    "\n",
    "    #Calculate PnL\n",
    "    df[f'{pair[0]} position'] = df['position']\n",
    "    df[f'{pair[1]} position'] = df['position'] * -1\n",
    "    df['dailypnl'] = df[f'{pair[1]} position']*return_df[f'{pair[1]}'].shift(-1) + df[f'{pair[0]} position']*return_df[f'{pair[0]}'].shift(-1)\n",
    "    df['cumpnl'] = df['dailypnl'].cumsum()\n",
    "\n",
    "    pairsOutcome[f'{pair[0]} {pair[1]}'] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make indicators and spread stationary around 0\n",
    "Deduct the mean from all values to translate to 0 axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "workingPairOutcome = {}\n",
    "\n",
    "for pair in top_keys:\n",
    "    dummy_df = pairsOutcome[top_keys[0]].iloc[::,:6]\n",
    "    dummy_df = dummy_df.subtract(dummy_df['mid'], axis=0).drop(columns=['mid']) # centre spread and SD\n",
    "    dummy_df = dummy_df.div(dummy_df['2sd high']-dummy_df['1sd high'],axis=0)   # express SD as integers, give spread as propotionate\n",
    "    dummy_df['2sd_high_boolean'] = (dummy_df['spread']>dummy_df['2sd high']).astype(int)\n",
    "    dummy_df['1sd_high_boolean'] = (dummy_df['spread']>dummy_df['1sd high']).astype(int)\n",
    "    dummy_df['0sd_high_boolean'] = (dummy_df['spread']>0).astype(int)\n",
    "    dummy_df['0sd_low_boolean']  = (dummy_df['spread']<0).astype(int)\n",
    "    dummy_df['1sd_low_boolean']  = (dummy_df['spread']<dummy_df['1sd low'] ).astype(int)\n",
    "    dummy_df['2sd_low_boolean']  = (dummy_df['spread']<dummy_df['2sd low'] ).astype(int)\n",
    "    dummy_df = dummy_df.drop(columns=['spread','1sd high', '1sd low', '2sd high', '2sd low'])\n",
    "    workingPairOutcome[pair] = dummy_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workingPairOutcome[top_keys[5]][-5:]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workingPairOutcome = {top_keys[0]:workingPairOutcome[top_keys[0]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test one timestep at a time (even though we can test all at the same time)\n",
    "- give state\n",
    "- Trading should be path dependent due to stop loss. in this case I can only give last position as one of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "class PairTradeEnv(gym.Env):\n",
    "    # ... (define your environment's state space, action space, etc.)\n",
    "    def __init__(self, workingPairOutcome, top_keys, validPairsList, return_df):\n",
    "        # ... (initialize other parameters)\n",
    "        self.earliest_step = 261  # hot start\n",
    "        self.last_step = 2868\n",
    "        # self.current_step = random.randint(self.earliest_step, self.last_step - 1)\n",
    "        self.current_step = self.earliest_step\n",
    "\n",
    "\n",
    "    def step(self, action, pair_idx):\n",
    "        \"\"\"\n",
    "        Input\n",
    "            action: single value e.g. -1 (short)\n",
    "            pair_idx: index of pair trade\n",
    "        Output:\n",
    "            next_state: next state \n",
    "            reward: reward for last timestep\n",
    "            done: boolean for if end of dataset\n",
    "            info: optional\n",
    "        \"\"\"\n",
    "        # Advance the time step\n",
    "        self.current_step += 1\n",
    "        # Get the next state\n",
    "        next_state = workingPairOutcome[top_keys[pair_idx]][self.current_step]\n",
    "        # Calculate reward (implement your reward function here)\n",
    "        reward = self.calculate_reward(action, self.current_step, validPairsList[pair_idx])\n",
    "        # Check for termination (implement your termination condition here)\n",
    "        done = self.current_step >= self.last_step\n",
    "\n",
    "        # Provide additional information (optional)\n",
    "        info = {}\n",
    "\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def reset(self, pair_idx):\n",
    "        # ... (implement the reset function to initialize the environment)\n",
    "        # reset to start of 2014 every time\n",
    "        # self.current_step = random.randint(self.earliest_step, self.last_step - 1)\n",
    "        self.current_step = self.earliest_step\n",
    "        initial_state = workingPairOutcome[top_keys[pair_idx]][self.current_step]\n",
    "        return initial_state\n",
    "    \n",
    "    def calculate_reward(self, position, idx, pair):\n",
    "        \"\"\"\n",
    "        Give one _previous_ day's return\n",
    "        Input:\n",
    "            position: position for idx (current step)\n",
    "            idx: usually current timestp \n",
    "            pair: tuple of tpx stock\n",
    "        Output:\n",
    "            dailypnl\n",
    "        \"\"\"\n",
    "        # position = position_vector @ np.array([-1,0,1])\n",
    "        position_0 = position\n",
    "        position_1 = position * -1\n",
    "        ## return_df gives the return for the previous day for the given idx\n",
    "        dailypnl = position_0*return_df[f'{pair[0]}'].iloc[idx] + position_1*return_df[f'{pair[1]}'].iloc[idx] \n",
    "\n",
    "        return dailypnl\n",
    "\n",
    "# Instantiate the custom environment\n",
    "env = PairTradeEnv(workingPairOutcome, top_keys, validPairsList, return_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc3.weight grad: tensor([[ 1.3517e-02, -5.1361e-05,  1.1380e-03,  6.2982e-03],\n",
      "        [-4.0735e-01,  1.6797e-03, -4.1802e-02, -2.2497e-01],\n",
      "        [-3.1061e-01,  1.2695e-03, -3.2151e-02, -1.6973e-01]])\n",
      "fc3.bias grad: tensor([ 0.0252, -0.8464, -0.6415])\n",
      "fc3.weight grad: tensor([[ 2.7683e-04,  4.8774e-04,  8.8312e-05, -5.8722e-03],\n",
      "        [-3.4709e-05, -4.4043e-05, -5.5058e-06,  1.9249e-03],\n",
      "        [-2.5288e-04, -3.9088e-04, -6.2896e-05,  1.0862e-02]])\n",
      "fc3.bias grad: tensor([-0.0081,  0.0014,  0.0100])\n",
      "fc3.weight grad: tensor([[-4.6018e-04, -3.6048e-04, -4.3966e-05,  2.0499e-02],\n",
      "        [ 1.2999e-04,  1.0014e-04,  1.1902e-05, -4.8356e-03],\n",
      "        [ 6.1860e-04,  5.4626e-04,  7.5515e-05, -2.2627e-02]])\n",
      "fc3.bias grad: tensor([ 0.0107, -0.0022, -0.0120])\n",
      "fc3.weight grad: tensor([[-2.8053e-02, -5.5768e-03,  3.9809e+00,  3.2393e+00],\n",
      "        [-3.2881e-03, -1.0648e-03,  4.7276e-01,  3.9659e-01],\n",
      "        [ 5.3540e-02,  9.4849e-03, -7.6180e+00, -6.1535e+00]])\n",
      "fc3.bias grad: tensor([ 0.0578,  0.0074, -0.1072])\n",
      "fc3.weight grad: tensor([[ 6.2182e-02,  1.2060e-02, -8.3931e+00, -7.2205e+00],\n",
      "        [-8.2145e-03, -1.5379e-03,  1.1202e+00,  9.5934e-01],\n",
      "        [-6.4244e-02, -1.2320e-02,  8.6599e+00,  7.4480e+00]])\n",
      "fc3.bias grad: tensor([-0.1306,  0.0161,  0.1341])\n",
      "fc3.weight grad: tensor([[-3.9590e-02, -7.2102e-03,  5.3083e+00,  4.5887e+00],\n",
      "        [ 1.5998e-02,  2.5609e-03, -2.1164e+00, -1.8283e+00],\n",
      "        [ 1.6700e-02,  2.6283e-03, -2.2095e+00, -1.9067e+00]])\n",
      "fc3.bias grad: tensor([ 0.0856, -0.0348, -0.0353])\n",
      "fc3.weight grad: tensor([[-1.6920e-02, -2.7739e-03,  2.2319e+00,  1.9630e+00],\n",
      "        [-2.1013e-02, -3.7356e-03,  2.7881e+00,  2.4548e+00],\n",
      "        [-3.0012e-02, -5.2862e-03,  3.9689e+00,  3.4973e+00]])\n",
      "fc3.bias grad: tensor([0.0380, 0.0478, 0.0697])\n",
      "fc3.weight grad: tensor([[ 7.9740e-02,  1.3036e-02, -1.0290e+01, -9.3142e+00],\n",
      "        [-9.3057e-04, -8.0976e-05,  1.2287e-01,  1.0927e-01],\n",
      "        [-3.9038e-02, -6.3179e-03,  5.0367e+00,  4.5580e+00]])\n",
      "fc3.bias grad: tensor([-0.1982,  0.0015,  0.0966])\n",
      "fc3.weight grad: tensor([[-8.9373e-04,  1.9195e-04,  1.0248e-01,  8.9622e-02],\n",
      "        [-4.4251e-03, -4.8278e-04,  5.8135e-01,  5.1252e-01],\n",
      "        [-1.3230e-02, -1.7926e-03,  1.7609e+00,  1.5500e+00]])\n",
      "fc3.bias grad: tensor([0.0023, 0.0134, 0.0382])\n",
      "fc3.weight grad: tensor([[-2.7764e-02, -3.4176e-03,  3.6197e+00,  3.2953e+00],\n",
      "        [-4.9917e-03, -8.3300e-04,  6.5438e-01,  5.9623e-01],\n",
      "        [ 2.5981e-02,  3.0233e-03, -3.3734e+00, -3.0738e+00]])\n",
      "fc3.bias grad: tensor([ 0.0840,  0.0136, -0.0821])\n",
      "fc3.weight grad: tensor([[ 7.2561e-03,  8.8097e-04, -9.9328e-01, -8.6835e-01],\n",
      "        [-1.1549e-03, -6.9754e-05,  1.6090e-01,  1.4270e-01],\n",
      "        [-1.3621e-02, -1.3017e-03,  1.8848e+00,  1.6480e+00]])\n",
      "fc3.bias grad: tensor([-0.0256,  0.0058,  0.0516])\n",
      "fc3.weight grad: tensor([[-8.1538e-03, -5.4608e-04,  1.1887e+00,  1.0162e+00],\n",
      "        [-4.1013e-03, -3.0730e-04,  5.7953e-01,  4.9659e-01],\n",
      "        [-2.5254e-03, -1.9109e-04,  3.5160e-01,  3.0164e-01]])\n",
      "fc3.bias grad: tensor([0.0364, 0.0172, 0.0104])\n",
      "fc3.weight grad: tensor([[ 5.0551e-03,  2.7963e-04, -7.6337e-01, -6.3873e-01],\n",
      "        [ 4.1115e-03,  1.8249e-04, -6.0839e-01, -5.0710e-01],\n",
      "        [-6.3050e-03, -5.1694e-04,  1.0210e+00,  8.5858e-01]])\n",
      "fc3.bias grad: tensor([-0.0274, -0.0203,  0.0397])\n",
      "fc3.weight grad: tensor([[ 1.3940e-03,  6.0245e-04, -2.0664e-01, -1.7108e-01],\n",
      "        [-6.8429e-04, -2.9838e-04,  1.0194e-01,  8.4770e-02],\n",
      "        [-3.1834e-03, -1.4475e-03,  5.2528e-01,  4.3042e-01]])\n",
      "fc3.bias grad: tensor([-0.0086,  0.0047,  0.0222])\n",
      "fc3.weight grad: tensor([[-0.0012, -0.0006,  0.2072,  0.1729],\n",
      "        [-0.0027, -0.0014,  0.4632,  0.3860],\n",
      "        [-0.0024, -0.0011,  0.3925,  0.3269]])\n",
      "fc3.bias grad: tensor([0.0115, 0.0252, 0.0208])\n",
      "fc3.weight grad: tensor([[ 4.3249e-04,  3.1742e-04, -1.1789e-01, -8.8065e-02],\n",
      "        [-9.5444e-05, -1.1801e-04,  3.1453e-02,  2.4319e-02],\n",
      "        [ 2.7289e-04,  1.4872e-04, -6.0209e-02, -4.5184e-02]])\n",
      "fc3.bias grad: tensor([-0.0086,  0.0031, -0.0041])\n",
      "fc3.weight grad: tensor([[-3.0519e-05, -1.0893e-04,  2.2897e-02,  1.7088e-02],\n",
      "        [ 1.6192e-04,  1.2035e-04, -3.3563e-02, -2.4925e-02],\n",
      "        [ 2.3892e-04,  1.6085e-04, -4.6541e-02, -3.4890e-02]])\n",
      "fc3.bias grad: tensor([ 0.0028, -0.0026, -0.0040])\n",
      "fc3.weight grad: tensor([[ 8.1255e-05,  2.1286e-04, -5.5027e-02, -3.9397e-02],\n",
      "        [-1.0361e-04, -1.8727e-04,  5.1592e-02,  3.7134e-02],\n",
      "        [-1.1980e-04, -1.8503e-04,  4.8186e-02,  3.4884e-02]])\n",
      "fc3.bias grad: tensor([-0.0075,  0.0066,  0.0058])\n",
      "fc3.weight grad: tensor([[-9.1520e-05, -4.2387e-05,  1.5808e-02,  1.0461e-02],\n",
      "        [ 2.2065e-04,  1.6490e-04, -4.0204e-02, -2.4792e-02],\n",
      "        [-2.3736e-04, -1.7082e-04,  4.1257e-02,  2.5929e-02]])\n",
      "fc3.bias grad: tensor([ 0.0023, -0.0089,  0.0085])\n",
      "fc3.weight grad: tensor([[-9.9363e-05, -1.3755e-04,  1.9497e-02,  1.1821e-02],\n",
      "        [ 5.8025e-05,  1.0393e-04, -1.1651e-02, -6.7241e-03],\n",
      "        [ 3.1676e-05,  3.2761e-05, -2.5842e-03, -2.2325e-03]])\n",
      "fc3.bias grad: tensor([ 0.0059, -0.0043, -0.0001])\n",
      "fc3.weight grad: tensor([[ 1.9975e-05, -1.8675e-05, -4.9504e-03, -3.1867e-03],\n",
      "        [-3.8423e-05, -1.0913e-04,  7.9461e-03,  3.2811e-03],\n",
      "        [ 1.0748e-04,  2.5843e-04, -2.2132e-02, -1.0092e-02]])\n",
      "fc3.bias grad: tensor([-0.0005,  0.0036, -0.0085])\n",
      "fc3.weight grad: tensor([[-0.0281, -0.0188, -0.0063,  1.5208],\n",
      "        [-0.0292, -0.0211, -0.0094,  1.8303],\n",
      "        [-0.0158, -0.0106, -0.0037,  0.8498]])\n",
      "fc3.bias grad: tensor([0.1597, 0.1829, 0.0915])\n",
      "fc3.weight grad: tensor([[-0.0012, -0.0011, -0.0007,  0.0482],\n",
      "        [ 0.0013, -0.0018, -0.0036,  0.1616],\n",
      "        [ 0.0004, -0.0004, -0.0009,  0.0340]])\n",
      "fc3.bias grad: tensor([0.0124, 0.0237, 0.0048])\n",
      "fc3.weight grad: tensor([[-1.3997e-03, -5.4616e-04,  9.3158e-05,  1.4656e-02],\n",
      "        [ 9.8595e-04, -4.5369e-04, -1.2229e-03,  2.7560e-02],\n",
      "        [-5.6350e-05, -1.2017e-04, -1.4457e-04,  5.3088e-03]])\n",
      "fc3.bias grad: tensor([0.0074, 0.0064, 0.0018])\n",
      "fc3.weight grad: tensor([[-1.1343e-03, -1.7717e-04,  2.8304e-04,  6.9632e-03],\n",
      "        [ 9.0818e-04, -1.7209e-04, -6.4572e-04,  5.2067e-03],\n",
      "        [ 4.5325e-04,  1.0399e-04, -5.9284e-05, -4.6145e-03]])\n",
      "fc3.bias grad: tensor([ 0.0051,  0.0004, -0.0031])\n",
      "fc3.weight grad: tensor([[-2.1871e-04,  2.1762e-04,  3.3781e-04, -5.2508e-03],\n",
      "        [ 4.9216e-04, -1.7667e-04, -3.6127e-04,  1.9710e-03],\n",
      "        [ 2.2534e-04,  4.2127e-06, -3.4409e-05, -2.8164e-03]])\n",
      "fc3.bias grad: tensor([-0.0022, -0.0004, -0.0022])\n",
      "fc3.weight grad: tensor([[ 1.4028e-04,  2.6887e-04,  4.1877e-04, -6.8698e-03],\n",
      "        [ 1.7781e-06, -1.6361e-04, -5.7274e-05,  4.3416e-03],\n",
      "        [ 2.6677e-05, -1.1555e-05, -1.0582e-05, -4.6875e-04]])\n",
      "fc3.bias grad: tensor([-0.0055,  0.0027, -0.0005])\n",
      "fc3.weight grad: tensor([[-1.8912e-04,  1.1074e-04,  3.7319e-04,  1.4732e-03],\n",
      "        [ 5.3539e-05, -5.6821e-05,  2.6508e-04,  5.1112e-04],\n",
      "        [-9.8970e-06, -4.6971e-06, -7.7553e-05, -3.6837e-04]])\n",
      "fc3.bias grad: tensor([ 0.0025, -0.0002, -0.0003])\n",
      "fc3.weight grad: tensor([[ 3.0921e-04,  1.4003e-03,  2.1796e-04, -1.1906e-02],\n",
      "        [ 5.7388e-05,  9.0498e-05, -1.4679e-04,  1.1091e-03],\n",
      "        [ 3.0490e-05,  2.6233e-04,  1.3244e-04, -2.8276e-03]])\n",
      "fc3.bias grad: tensor([-0.0123, -0.0002, -0.0027])\n",
      "fc3.weight grad: tensor([[-8.4480e-06, -1.3350e-04, -6.9574e-05,  1.3551e-03],\n",
      "        [ 4.0098e-05,  2.4054e-04,  5.7896e-05, -1.7232e-03],\n",
      "        [ 1.0838e-05,  1.2903e-04,  7.8102e-05, -1.5582e-03]])\n",
      "fc3.bias grad: tensor([ 0.0016, -0.0024, -0.0015])\n",
      "fc3.weight grad: tensor([[ 2.2668e-05,  2.9167e-05, -3.9374e-05, -6.2093e-04],\n",
      "        [-1.3992e-04, -5.5521e-04, -1.1206e-04,  6.3198e-03],\n",
      "        [ 1.6235e-04,  5.9289e-04,  7.6943e-05, -6.9342e-03]])\n",
      "fc3.bias grad: tensor([-0.0004,  0.0062, -0.0068])\n",
      "fc3.weight grad: tensor([[-6.8303e-05, -4.6793e-04, -1.2465e-04,  7.2163e-03],\n",
      "        [-2.2905e-04, -1.8003e-03, -4.8324e-04,  3.0161e-02],\n",
      "        [ 1.7723e-06, -4.1521e-06, -1.2149e-05, -1.7320e-04]])\n",
      "fc3.bias grad: tensor([ 5.1203e-03,  2.1344e-02, -1.6598e-05])\n",
      "fc3.weight grad: tensor([[-1.9936e-04, -1.4978e-03, -1.6817e-04,  1.0467e-02],\n",
      "        [ 4.7831e-05,  2.5898e-04, -5.9452e-05, -1.0615e-03],\n",
      "        [ 9.9673e-05,  7.0905e-04, -3.3575e-05, -4.9460e-03]])\n",
      "fc3.bias grad: tensor([ 0.0127, -0.0016, -0.0058])\n",
      "fc3.weight grad: tensor([[-1.8561e-01, -1.3453e-01,  7.4372e-04,  3.7265e+01],\n",
      "        [-6.5855e-01, -4.9984e-01, -1.0198e-01,  1.1713e+02],\n",
      "        [-1.0887e-01, -8.0814e-02, -7.7917e-03,  2.0631e+01]])\n",
      "fc3.bias grad: tensor([0.4325, 1.4922, 0.2532])\n",
      "fc3.weight grad: tensor([[-3.1243e-03, -6.8086e-03, -6.8648e-03,  5.4887e+00],\n",
      "        [-1.6382e-02, -2.6171e-02, -2.5712e-02,  1.6678e+01],\n",
      "        [-3.6206e-03, -5.0705e-03, -5.0428e-03,  2.8350e+00]])\n",
      "fc3.bias grad: tensor([0.1292, 0.3940, 0.0682])\n",
      "fc3.weight grad: tensor([[-3.6140e-03, -4.6614e-03, -3.9174e-03,  1.9769e+00],\n",
      "        [-9.0504e-03, -1.3443e-02, -1.0255e-02,  6.8570e+00],\n",
      "        [-1.7113e-03, -2.4700e-03, -1.9398e-03,  1.2348e+00]])\n",
      "fc3.bias grad: tensor([0.0541, 0.1847, 0.0339])\n",
      "Episode 1: Total Return: 0.001, Epsilon: 0.10\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_rate=0.2):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 4)\n",
    "        # self.dropout1 = nn.Dropout(p=dropout_rate) \n",
    "        self.fc2 = nn.Linear(4, 4)\n",
    "        # self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.fc3 = nn.Linear(4, output_size)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        # x = self.dropout1(x)\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        # x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, input_size, output_size, learning_rate, discount_factor, epsilon, batch_size=9000, replay_buffer_size=10000):\n",
    "        self.q_network = QNetwork(input_size, output_size)\n",
    "        self.target_network = QNetwork(input_size, output_size)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate) \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.learn_count = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "        \n",
    "        # Action to index mapping\n",
    "        self.action_to_index = {-1: 0, 0: 1, 1: 2}\n",
    "        self.index_to_action = {0: -1, 1: 0, 2: 1}\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice([-1, 0, 1])  # Explore\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "                action_index = torch.argmax(q_values, dim=1).item()  # Choose best action\n",
    "            action = self.index_to_action[action_index]  # Map index to action\n",
    "        return action\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32)\n",
    "        actions = torch.tensor([self.action_to_index[action] for action in actions], dtype=torch.long).view(-1, 1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).view(-1, 1)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Get max Q-value for the next states from target network\n",
    "        q_values = self.q_network(states).gather(1, actions)\n",
    "        next_q_values = self.target_network(next_states).max(1, keepdim=True)[0].detach().view(-1, 1)\n",
    "        target_q_values = rewards + self.discount_factor * next_q_values * (1 - dones).view(-1,1)\n",
    "\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if self.learn_count%1000==0:\n",
    "            for name, param in agent.q_network.named_parameters():\n",
    "                if 'fc3' in name and param.requires_grad:\n",
    "                    print(f\"{name} grad: {param.grad}\")\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.learn_count += 1\n",
    "        # Update target network every few episodes\n",
    "        if self.learn_count % 1 == 0:\n",
    "            self.update_target_network()\n",
    "        if self.learn_count % 100 == 0:\n",
    "            self.epsilon = max(0.1, self.epsilon * 0.9885)\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "input_size = 6\n",
    "output_size = 3 \n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99\n",
    "epsilon = 1.0\n",
    "\n",
    "agent = QLearningAgent(input_size, output_size, learning_rate, discount_factor, epsilon)\n",
    "\n",
    "## Training constants\n",
    "total_episodes = 1\n",
    "number_of_pairs = len(workingPairOutcome)\n",
    "ls_total_reward = []\n",
    "\n",
    "# Simulating agent learning (in practice, use a loop with environment interaction)\n",
    "for episode in range(total_episodes):\n",
    "    arr_pair_reward = np.zeros(number_of_pairs)\n",
    "\n",
    "    for pair_idx in range(number_of_pairs):\n",
    "        state = env.reset(pair_idx)\n",
    "        pair_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action, pair_idx)\n",
    "            pair_reward += reward\n",
    "            if reward > 0:\n",
    "                reward += 1\n",
    "            elif reward<0:\n",
    "                reward += -1\n",
    "\n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "            agent.learn()\n",
    "            \n",
    "            state = next_state\n",
    "\n",
    "        arr_pair_reward[pair_idx] = pair_reward\n",
    "    \n",
    "    total_reward = arr_pair_reward.mean()\n",
    "    print(f\"Episode {episode+1}: Total Return: {total_reward:.3f}, Epsilon: {agent.epsilon:.2f}\")\n",
    "    # Capture total_reward for further analysis\n",
    "    ls_total_reward.append(total_reward)\n",
    "\n",
    "# evaluate random and final trained performance\n",
    "plt.plot(ls_total_reward)\n",
    "\n",
    "# After training, save the entire Q-network\n",
    "torch.save(agent.q_network, 'q_network.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11 Dec\n",
    "- add back spread as one continuous variable for DQN\n",
    "    - initial results is biased and resorts to shorting\n",
    "- since its biased, try removing the dropout to get more fitting ==> still biased\n",
    "- try reducing the epsilon to get more training\n",
    "- Underfitting\n",
    "  - remove dropout layers\n",
    "  - increase learning rate\n",
    "  - increase update to once every 2 learning\n",
    "  - increase batch size to 8000\n",
    "- reached 28 mins and ran out of memory\n",
    "- run no. 2 gradient vanished. try again. == make sure to quit other apps ==\n",
    "  - ***v20241211_1***\n",
    "  - removed dropout layers\n",
    "  - learning rate 0.9\n",
    "  - update target every 2 learns\n",
    "  - batch size 8000\n",
    "  - 2 hidden layers 4x4 nn\n",
    "  - changed relu to leaky relu ==> mean reverting!!!! but overfit.  (6mins~)\n",
    "- try reducing neural net. v20241211_1 but changed learning rate to 0.1. less overfitting but negative performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_q_network = torch.load('q_network_replay1batch_32x4.pth')\n",
    "\n",
    "# loaded_q_network = QNetwork(6,3)  # Replace QNetwork with your model class\n",
    "# loaded_q_network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: Average Total Train Return: -0.11745\n"
     ]
    }
   ],
   "source": [
    "def evaluate_agent(agent, env, number_of_pairs):\n",
    "    \"\"\"\n",
    "    Evaluates the agent's performance over one episode without epsilon exploration.\n",
    "    \n",
    "    Parameters:\n",
    "        agent (QLearningAgent): The trained agent.\n",
    "        env: The environment with reset() and step() methods.\n",
    "        number_of_pairs (int): Number of pairs to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        float: The average total return across all pairs.\n",
    "    \"\"\"\n",
    "    total_rewards = np.zeros(number_of_pairs)\n",
    "\n",
    "    for pair_idx in range(number_of_pairs):\n",
    "        state = env.reset(pair_idx)\n",
    "        pair_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                # Select action based purely on Q-network (greedy action)\n",
    "                q_values = agent.q_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "                action_index = torch.argmax(q_values, dim=1).item()\n",
    "                action = agent.index_to_action[action_index]\n",
    "            \n",
    "            # Take the selected action\n",
    "            next_state, reward, done, _ = env.step(action, pair_idx)\n",
    "            pair_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_rewards[pair_idx] = pair_reward\n",
    "\n",
    "    # Return the average reward across all pairs\n",
    "    return total_rewards.mean()\n",
    "\n",
    "agent.q_network.eval()\n",
    "agent.target_network.eval()\n",
    "# Example usage\n",
    "average_return = evaluate_agent(agent, env, number_of_pairs)\n",
    "print(f\"Evaluation: Average Total Train Return: {average_return:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values: tensor([[13.2044, 13.2907, 13.2454],\n",
      "        [12.6350, 12.5167, 12.4519],\n",
      "        [ 9.9294,  9.7160,  9.7976],\n",
      "        [13.3873, 13.7309, 13.6511],\n",
      "        [12.7761, 12.6675, 12.5929],\n",
      "        [11.5692, 11.4117, 11.4017]])\n",
      "Greedy actions: [0, -1, -1, 0, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "# Assuming states is a list of 6 states, each a list or NumPy array\n",
    "states = [[0, 0, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 0],\n",
    "        [0, 0, 0, 1, 1, 1],\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 1, 1, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 0, 0]]\n",
    "\n",
    "# Convert to PyTorch tensor (ensure float32 for compatibility)\n",
    "states_tensor = torch.tensor(states, dtype=torch.float32)\n",
    "\n",
    "# Evaluate the Q-values for all states\n",
    "agent.q_network.eval()  # Set the network to evaluation mode\n",
    "with torch.no_grad():\n",
    "    q_values = agent.q_network(states_tensor)  # Output will be a tensor of shape (6, output_size)\n",
    "\n",
    "# Example: Get the greedy actions for each state\n",
    "action_indices = torch.argmax(q_values, dim=1).tolist()\n",
    "actions = [agent.index_to_action[index] for index in action_indices]\n",
    "\n",
    "print(\"Q-values:\", q_values)\n",
    "print(\"Greedy actions:\", actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0, 0, 1, 0, 0): 0,\n",
       " (0, 0, 0, 1, 1, 0): -1,\n",
       " (0, 0, 0, 1, 1, 1): -1,\n",
       " (0, 0, 1, 0, 0, 0): 0,\n",
       " (0, 1, 1, 0, 0, 0): -1,\n",
       " (1, 1, 1, 0, 0, 0): -1,\n",
       " (0, 0, 0, 0, 0, 0): 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_policy_actions = dict(zip([tuple(state) for state in states], actions))\n",
    "dict_policy_actions[(0, 0, 0, 0, 0, 0)] = 0\n",
    "dict_policy_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "workingPairQresults = {}\n",
    "\n",
    "for pair_idx in range(number_of_pairs):\n",
    "    df = pd.DataFrame(workingPairOutcome[top_keys[pair_idx]], columns=dummy_df.columns)\n",
    "\n",
    "    # Assign policy values using the dictionary\n",
    "    df['position'] = df.apply(lambda row: dict_policy_actions.get(tuple(row), np.nan), axis=1)\n",
    "    df[df.isna().any(axis=1)]\n",
    "    pair = validPairsList[pair_idx]\n",
    "    #Calculate PnL\n",
    "    df[f'{pair[0]} position'] = df['position']\n",
    "    df[f'{pair[1]} position'] = df['position'] * -1\n",
    "    df['dailypnl'] = df[f'{pair[1]} position'].values*return_df[f'{pair[1]}'].shift(-1).values \\\n",
    "                    + df[f'{pair[0]} position'].values*return_df[f'{pair[0]}'].shift(-1).values\n",
    "    df['cumpnl'] = df['dailypnl'].cumsum()\n",
    "    workingPairQresults[top_keys[pair_idx]] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total return \t\t-0.03785\n",
      "total train return \t-0.11745\n",
      "total test return \t0.07960\n"
     ]
    }
   ],
   "source": [
    "total_q_return = 0\n",
    "total_train_q_return = 0\n",
    "total_test_q_return = 0\n",
    "\n",
    "for pair in top_keys:\n",
    "    total_q_return += workingPairQresults[pair]['cumpnl'].iloc[-2]\n",
    "    total_train_q_return += workingPairQresults[pair]['cumpnl'].iloc[2868-2]\n",
    "\n",
    "\n",
    "print(f\"total return \\t\\t{total_q_return/len(top_keys):.5f}\")\n",
    "print(f\"total train return \\t{total_train_q_return/len(top_keys):.5f}\")\n",
    "print(f\"total test return \\t{(total_q_return - total_train_q_return)/len(top_keys):.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

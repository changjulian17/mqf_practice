{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/r7qn2m9n1zb6y_0q191gdqth0000gn/T/ipykernel_56118/1916853760.py:27: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  tpxData = pd.read_csv('TPX_prices.csv', index_col=0, parse_dates=True, date_parser=custom_date_parser)\n"
     ]
    }
   ],
   "source": [
    "## anaconda3 (Python 3.12.0) Kernel\n",
    "# pair trade packages\n",
    "import csv\n",
    "import gym\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "# nn packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "def custom_date_parser(date_str):\n",
    "    return datetime.strptime(date_str, '%d/%m/%Y')\n",
    "# Load the dictionary from the pickle file\n",
    "with open('pairsOutcome.pkl', 'rb') as file:\n",
    "    pairsOutcome = pickle.load(file)\n",
    "# Load stock data and get return \n",
    "tpxData = pd.read_csv('TPX_prices.csv', index_col=0, parse_dates=True, date_parser=custom_date_parser)\n",
    "tpxData = tpxData.dropna(axis='columns')\n",
    "return_df = (tpxData / tpxData.shift(1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pairs Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Pair Trade Portfolio\n",
    "`pairsOutcome` already have TOPIX stocks with highest liquidity and are tested for stationarity over a 1 year window\n",
    "\n",
    "Choose top 10 known pair trades by returns in the total dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 clustered trades:\n",
      "1. Key: 6503 JP Equity 7269 JP Equity, Return: 1.33\n",
      "2. Key: 6326 JP Equity 6954 JP Equity, Return: 1.19\n",
      "3. Key: 8053 JP Equity 8058 JP Equity, Return: 0.52\n",
      "4. Key: 4901 JP Equity 9613 JP Equity, Return: 1.10\n",
      "5. Key: 6988 JP Equity 7267 JP Equity, Return: 0.65\n",
      "6. Key: 4901 JP Equity 6702 JP Equity, Return: -0.34\n",
      "7. Key: 4684 JP Equity 7832 JP Equity, Return: 0.89\n",
      "8. Key: 7267 JP Equity 8306 JP Equity, Return: 1.16\n",
      "9. Key: 7267 JP Equity 8801 JP Equity, Return: 0.64\n",
      "10. Key: 4519 JP Equity 7532 JP Equity, Return: 1.14\n"
     ]
    }
   ],
   "source": [
    "with open(\"output_clustering.csv\", 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    # Skip the header row\n",
    "    next(reader)\n",
    "    working_pairs = [tuple(row) for row in reader]\n",
    "\n",
    "top_keys = [f\"{pair[0]} {pair[1]}\" for pair in working_pairs]\n",
    "print(\"Top 10 clustered trades:\")\n",
    "for i, key in enumerate(top_keys, 1):\n",
    "    print(f\"{i}. Key: {key}, Return: {pairsOutcome[key].cumpnl.iloc[-2]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/r7qn2m9n1zb6y_0q191gdqth0000gn/T/ipykernel_56118/2914914838.py:1: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  valid = pd.read_csv('validPairs5.csv',\n"
     ]
    }
   ],
   "source": [
    "valid = pd.read_csv('validPairs5.csv', \n",
    "                    index_col=0, \n",
    "                    parse_dates=True, \n",
    "                    date_parser=custom_date_parser)\n",
    "## get list of pair stocks\n",
    "validPairsList = [\n",
    "    [item.strip() + ' Equity' for item in pair.split('Equity') if item.strip()]\n",
    "    for pair in top_keys\n",
    "]\n",
    "\n",
    "# due to spikey-ness of reward around 0, scale from -1 to 1 and give more rewards above 1, 2, 3\n",
    "dailypnl_sd = 0.018390515013803736 \n",
    "# see /Users/ju/Projects/00_SMU/mqf_practice/QF634_Applied_Quantitative_Research_Methods/QF634 Project/Pair Trading/03_project_state_space_analysis.ipynb for SD derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "table is taking a long time to generalise. using one pair is not good enough to get any poistioning as everything is flat. only after adding all 10 pairs then performance churns out shorting on 1 SD high cross.\n",
    "\n",
    "Experiment: try making gamma 0.1, Q table should closely mimic the state space analysis table with strong mean reversion tendency. This is not what we see. and it even has Q values opposite to state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.00028887, 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [2.6859092 , 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [2.52197973, 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [2.40723813, 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [2.46852476, 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollingWindow = 262\n",
    "cutLossSd = 2\n",
    "\n",
    "for pair in validPairsList:\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    #Calculate Standard Deviations\n",
    "    df['spread'] = valid[f'spread_{pair[0]}_{pair[1]}']\n",
    "    df['mid'] =  df['spread'].rolling(rollingWindow).mean()\n",
    "    df['1sd high'] = df['spread'].rolling(rollingWindow).mean() + df['spread'].rolling(rollingWindow).std()\n",
    "    df['1sd low'] = df['spread'].rolling(rollingWindow).mean() - df['spread'].rolling(rollingWindow).std()\n",
    "    df['2sd high'] = df['spread'].rolling(rollingWindow).mean() + df['spread'].rolling(rollingWindow).std() * cutLossSd\n",
    "    df['2sd low'] = df['spread'].rolling(rollingWindow).mean() - df['spread'].rolling(rollingWindow).std() * cutLossSd\n",
    "    df['position'] = 0\n",
    "\n",
    "    df.loc[(df['spread'] > df['1sd high']) & (df['spread'] < df['2sd high']), 'position'] = -1\n",
    "    df.loc[(df['spread']< df['1sd low']) & (df['spread'] > df['2sd low']), 'position'] = 1\n",
    "\n",
    "    #Calculate PnL\n",
    "    df[f'{pair[0]} position'] = df['position']\n",
    "    df[f'{pair[1]} position'] = df['position'] * -1\n",
    "    df['dailypnl'] = df[f'{pair[1]} position']*return_df[f'{pair[1]}'].shift(-1) + df[f'{pair[0]} position']*return_df[f'{pair[0]}'].shift(-1)\n",
    "    df['cumpnl'] = df['dailypnl'].cumsum()\n",
    "\n",
    "    pairsOutcome[f'{pair[0]} {pair[1]}'] = df\n",
    "\n",
    "workingPairOutcome = {}\n",
    "\n",
    "for pair in top_keys:\n",
    "    dummy_df = pairsOutcome[pair].iloc[::,:6]\n",
    "    dummy_df = dummy_df.subtract(dummy_df['mid'], axis=0).drop(columns=['mid']) # centre spread and SD\n",
    "    dummy_df = dummy_df.div(dummy_df['2sd high']-dummy_df['1sd high'],axis=0)   # express SD as integers, give spread as propotionate\n",
    "    dummy_df['2sd_high_boolean'] = (dummy_df['spread']>dummy_df['2sd high']).astype(int)\n",
    "    dummy_df['1sd_high_boolean'] = (dummy_df['spread']>dummy_df['1sd high']).astype(int)\n",
    "    dummy_df['0sd_high_boolean'] = (dummy_df['spread']>0).astype(int)\n",
    "    dummy_df['0sd_low_boolean']  = (dummy_df['spread']<0).astype(int)\n",
    "    dummy_df['1sd_low_boolean']  = (dummy_df['spread']<dummy_df['1sd low'] ).astype(int)\n",
    "    dummy_df['2sd_low_boolean']  = (dummy_df['spread']<dummy_df['2sd low'] ).astype(int)\n",
    "    dummy_df = dummy_df.drop(columns=['1sd high', '1sd low', '2sd high', '2sd low'])\n",
    "    workingPairOutcome[pair] = dummy_df.to_numpy()\n",
    "\n",
    "workingPairOutcome[top_keys[5]][-5:]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test one timestep at a time (even though we can test all at the same time)\n",
    "- give state\n",
    "- Trading should be path dependent due to stop loss. in this case I can only give last position as one of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairTradeEnv(gym.Env):\n",
    "    # ... (define your environment's state space, action space, etc.)\n",
    "    def __init__(self, workingPairOutcome, top_keys, validPairsList, return_df):\n",
    "        # ... (initialize other parameters)\n",
    "        self.earliest_step = 261\n",
    "        self.last_step = 2868\n",
    "        self.current_step = self.earliest_step\n",
    "\n",
    "\n",
    "    def step(self, action, pair_idx):\n",
    "        \"\"\"\n",
    "        Input\n",
    "            action: single value e.g. -1 (short)\n",
    "            pair_idx: index of pair trade\n",
    "        Output:\n",
    "            next_state: next state \n",
    "            reward: reward for last timestep\n",
    "            done: boolean for if end of dataset\n",
    "            info: optional\n",
    "        \"\"\"\n",
    "        self.current_step += 1\n",
    "        next_state = workingPairOutcome[top_keys[pair_idx]][self.current_step]\n",
    "        reward = self.calculate_reward(action, self.current_step, validPairsList[pair_idx])\n",
    "        done = self.current_step >= self.last_step\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def reset(self, pair_idx):\n",
    "        self.current_step = self.earliest_step\n",
    "        initial_state = workingPairOutcome[top_keys[pair_idx]][self.current_step]\n",
    "        return initial_state\n",
    "    \n",
    "    def calculate_reward(self, position, idx, pair):\n",
    "        \"\"\"\n",
    "        Give one _previous_ day's return\n",
    "        Input:\n",
    "            position: position for idx (current step)\n",
    "            idx: usually current timestp \n",
    "            pair: tuple of tpx stock\n",
    "        Output:\n",
    "            dailypnl\n",
    "        \"\"\"\n",
    "        position_0 = position\n",
    "        position_1 = position * -1\n",
    "        dailypnl = position_0*return_df[f'{pair[0]}'].iloc[idx] + position_1*return_df[f'{pair[1]}'].iloc[idx] \n",
    "\n",
    "        return dailypnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_priorities(replay_buffer, alpha=0.6):\n",
    "    \"\"\"\n",
    "    Compute priorities for sampling based on temporal difference (TD) error or recency.\n",
    "    \"\"\"\n",
    "    priorities = []\n",
    "    for experience in replay_buffer:\n",
    "        _, _, reward, _, _ = experience\n",
    "        td_error = abs(reward)  # Simplified proxy for TD error\n",
    "        priority = (td_error + 1e-5) ** alpha\n",
    "        priorities.append(priority)\n",
    "    return priorities\n",
    "\n",
    "def evaluate_agent_train(agent, env, number_of_pairs):\n",
    "    \"\"\"\n",
    "    Evaluates the agent's train performance over one episode without epsilon exploration.\n",
    "    \"\"\"\n",
    "    total_rewards = np.zeros(number_of_pairs)\n",
    "\n",
    "    for pair_idx in range(number_of_pairs):\n",
    "        state = env.reset(pair_idx)\n",
    "        env.current_step = 261\n",
    "        env.last_step = 2978\n",
    "        pair_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                q_values = agent.q_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "                action_index = torch.argmax(q_values, dim=1).item()\n",
    "                action = agent.index_to_action[action_index]\n",
    "            \n",
    "            # Take the selected action\n",
    "            next_state, reward, done, _ = env.step(action, pair_idx)\n",
    "            pair_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_rewards[pair_idx] = pair_reward\n",
    "\n",
    "    # Return the average reward across all pairs\n",
    "    return total_rewards.mean()\n",
    "\n",
    "def evaluate_agent_test(agent, env, number_of_pairs):\n",
    "    \"\"\"\n",
    "    Evaluates the agent's test performance over one episode without epsilon exploration.\n",
    "    \"\"\"\n",
    "    total_rewards = np.zeros(number_of_pairs)\n",
    "\n",
    "    for pair_idx in range(number_of_pairs):\n",
    "        state = env.reset(pair_idx)\n",
    "        env.current_step = 2868\n",
    "        env.last_step = 2978\n",
    "        pair_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                # Select action based purely on Q-network (greedy action)\n",
    "                q_values = agent.q_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "                action_index = torch.argmax(q_values, dim=1).item()\n",
    "                action = agent.index_to_action[action_index]\n",
    "            \n",
    "            # Take the selected action\n",
    "            next_state, reward, done, _ = env.step(action, pair_idx)\n",
    "            pair_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_rewards[pair_idx] = pair_reward\n",
    "\n",
    "    # Return the average reward across all pairs\n",
    "    return total_rewards.mean()\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_rate=0.2):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 8)\n",
    "        # self.dropout1 = nn.Dropout(p=dropout_rate) \n",
    "        self.fc2 = nn.Linear(8, 8)\n",
    "        # self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.fc3 = nn.Linear(8, output_size)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        # x = self.dropout1(x)\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        # x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, input_size, output_size, learning_rate, discount_factor, epsilon, epsilon_decay, batch_size=1000, replay_buffer_size=10000):\n",
    "        self.q_network = QNetwork(input_size, output_size)\n",
    "        self.target_network = QNetwork(input_size, output_size)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate) \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learn_count = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        \n",
    "        # Action to index mapping\n",
    "        self.action_to_index = {-1: 0, 0: 1, 1: 2}\n",
    "        self.index_to_action = {0: -1, 1: 0, 2: 1}\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice([-1, 0, 1])  # Explore\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "                action_index = torch.argmax(q_values, dim=1).item()  # Choose best action\n",
    "            action = self.index_to_action[action_index] \n",
    "        return action\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < self.replay_buffer_size:\n",
    "            return\n",
    "\n",
    "        # Compute priorities for sampling and sampler\n",
    "        priorities = compute_priorities(self.replay_buffer)\n",
    "        weights = np.array(priorities) / sum(priorities)\n",
    "        sampler = WeightedRandomSampler(weights, self.batch_size, replacement=True)\n",
    "        # dataloader = DataLoader(list(self.replay_buffer), batch_size=self.batch_size, sampler=sampler)\n",
    "        dataloader = DataLoader(list(self.replay_buffer), batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        # batch_count = 0\n",
    "        # if batch_count >=1:\n",
    "        #     break\n",
    "        states, actions, rewards, next_states, dones = next(iter(dataloader))\n",
    "        states = states.clone().detach().float()\n",
    "        next_states = next_states.clone().detach().float()\n",
    "        actions = torch.tensor([self.action_to_index[action.item()] for action in actions]).view(-1, 1)\n",
    "        rewards = rewards.clone().detach().float().view(-1, 1)\n",
    "        q_values = self.q_network(states).gather(1, actions)\n",
    "        dones = dones.float()\n",
    "\n",
    "        next_q_values = self.target_network(next_states).max(1, keepdim=True)[0].detach().view(-1, 1)\n",
    "        target_q_values = rewards + self.discount_factor * next_q_values * (1 - dones).view(-1,1)\n",
    "\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if self.learn_count%200000==0:\n",
    "            for name, param in agent.q_network.named_parameters():\n",
    "                if 'fc2.weight' in name and param.requires_grad:\n",
    "                    print(f\"{name} grad: {param.grad}\")\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.learn_count += 1\n",
    "        # Update target network every few episodes\n",
    "        if self.learn_count % 5 == 0:\n",
    "            self.update_target_network()\n",
    "\n",
    "        # batch_count += 1\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc2.weight grad: tensor([[-3.8743e-03, -7.1230e-04, -4.3414e-03, -3.0667e-03, -4.0062e-03,\n",
      "         -2.4171e-03, -1.8466e-03, -6.3957e-04],\n",
      "        [ 1.5062e-02,  4.0870e-03,  1.3433e-02,  1.5573e-02,  1.1989e-02,\n",
      "          1.1619e-02,  1.1425e-02,  1.7977e-03],\n",
      "        [-7.3162e-05, -1.1791e-05, -7.6810e-05, -1.0158e-04, -7.7617e-05,\n",
      "         -7.7995e-05, -5.0889e-05, -1.3163e-05],\n",
      "        [-1.2299e-03, -4.0544e-04, -1.1680e-03, -1.5809e-04, -8.5322e-04,\n",
      "         -1.1425e-04, -5.8313e-04, -1.0340e-04],\n",
      "        [ 4.4418e-03,  2.1376e-03,  1.1573e-03,  3.0176e-05,  3.0563e-05,\n",
      "         -3.5802e-06,  4.6630e-03, -1.7732e-05],\n",
      "        [ 2.6494e-02,  6.4775e-03,  2.5051e-02,  2.7646e-02,  2.2908e-02,\n",
      "          2.0858e-02,  1.8788e-02,  3.5442e-03],\n",
      "        [ 2.8913e-03,  1.8819e-03,  1.4283e-02,  2.3712e-02,  1.2522e-02,\n",
      "          1.7575e-02,  2.8715e-03,  1.4086e-03],\n",
      "        [ 2.6971e-03,  8.7068e-04,  2.6786e-03, -5.5832e-05,  1.9397e-03,\n",
      "         -3.6468e-05,  1.0639e-03,  2.3477e-04]])\n",
      "Episode 1: Total Return: -0.313, Epsilon: 1.00\n",
      "Episode 2: Total Return: 0.034, Epsilon: 0.88\n",
      "Episode 3: Total Return: 0.322, Epsilon: 0.77\n",
      "Episode 4: Total Return: 0.329, Epsilon: 0.68\n",
      "Episode 5: Total Return: 0.156, Epsilon: 0.60\n",
      "Episode 6: Total Return: 0.435, Epsilon: 0.53\n",
      "Episode 7: Total Return: 0.824, Epsilon: 0.46\n",
      "Episode 8: Total Return: 0.622, Epsilon: 0.41\n",
      "fc2.weight grad: tensor([[-9.2244e-05,  9.8998e-06, -6.0105e-05, -6.4084e-05, -3.1566e-05,\n",
      "         -1.4224e-04, -2.5016e-05, -5.0515e-05],\n",
      "        [-2.0685e-04, -2.1450e-05, -1.3759e-04, -1.4927e-04, -2.2103e-05,\n",
      "         -3.0644e-04, -7.9549e-05, -1.1801e-04],\n",
      "        [-4.0530e-05,  3.3276e-05, -2.4783e-05, -2.4603e-05, -4.6340e-05,\n",
      "         -7.0716e-05,  4.4725e-06, -1.9307e-05],\n",
      "        [-2.4791e-04, -7.5008e-05, -1.6796e-04, -1.8512e-04,  2.8593e-05,\n",
      "         -3.5315e-04, -1.2179e-04, -1.4666e-04],\n",
      "        [-6.3116e-06,  6.4673e-05, -2.0059e-07,  3.6554e-06, -7.3718e-05,\n",
      "         -2.8032e-05,  3.2604e-05,  3.2653e-06],\n",
      "        [ 8.3003e-06,  1.4303e-04,  1.4076e-05,  2.3775e-05, -1.5821e-04,\n",
      "         -2.8306e-05,  7.9383e-05,  1.9525e-05],\n",
      "        [-4.1652e-04, -1.2556e-04, -2.8200e-04, -3.1087e-04,  4.7679e-05,\n",
      "         -5.9354e-04, -2.0432e-04, -2.4618e-04],\n",
      "        [ 5.4426e-04,  2.7937e-05,  3.6035e-04,  3.8921e-04,  9.0081e-05,\n",
      "          8.1442e-04,  1.9404e-04,  3.0758e-04]])\n",
      "Episode 9: Total Return: 0.531, Epsilon: 0.36\n",
      "Episode 10: Total Return: 0.589, Epsilon: 0.32\n",
      "Episode 11: Total Return: 0.408, Epsilon: 0.30\n",
      "Episode 12: Total Return: 0.791, Epsilon: 0.30\n",
      "Episode 13: Total Return: 0.871, Epsilon: 0.30\n",
      "Episode 14: Total Return: 0.580, Epsilon: 0.30\n",
      "Episode 15: Total Return: 0.629, Epsilon: 0.30\n",
      "fc2.weight grad: tensor([[-1.3400e-05, -8.2365e-06,  1.0067e-05, -3.2211e-06, -2.0651e-05,\n",
      "         -5.6362e-06,  1.0841e-05, -8.7901e-06],\n",
      "        [ 7.6519e-05,  3.1184e-05,  1.0942e-04,  5.1172e-05,  7.9490e-05,\n",
      "          5.4170e-05,  6.3490e-05,  3.2860e-05],\n",
      "        [ 1.0486e-04,  4.9039e-05,  8.3296e-05,  5.7219e-05,  1.2430e-04,\n",
      "          6.5563e-05,  3.7010e-05,  5.2109e-05],\n",
      "        [ 1.2642e-04,  5.4956e-05,  1.4399e-04,  7.7708e-05,  1.3982e-04,\n",
      "          8.4880e-05,  7.7426e-05,  5.8439e-05],\n",
      "        [-6.5862e-05, -2.2464e-05, -1.4005e-04, -5.3208e-05, -5.7867e-05,\n",
      "         -5.2759e-05, -8.9177e-05, -2.3650e-05],\n",
      "        [-3.1236e-04, -1.3336e-04, -3.8184e-04, -1.9682e-04, -3.3946e-04,\n",
      "         -2.1298e-04, -2.1076e-04, -1.4143e-04],\n",
      "        [ 1.4684e-04,  6.0228e-05,  2.0551e-04,  9.7600e-05,  1.5359e-04,\n",
      "          1.0353e-04,  1.1861e-04,  6.3757e-05],\n",
      "        [-3.3772e-05, -1.4713e-05, -3.8204e-05, -2.0661e-05, -3.7410e-05,\n",
      "         -2.2613e-05, -2.0467e-05, -1.5599e-05]])\n",
      "Episode 16: Total Return: 0.853, Epsilon: 0.30\n",
      "Episode 17: Total Return: 0.636, Epsilon: 0.30\n",
      "Episode 18: Total Return: 0.849, Epsilon: 0.30\n",
      "Episode 19: Total Return: 0.766, Epsilon: 0.30\n",
      "Episode 20: Total Return: 1.081, Epsilon: 0.30\n",
      "fc2.weight grad: tensor([[ 1.4188e-02,  7.1563e-03,  2.3324e-03,  4.6829e-03,  7.1123e-03,\n",
      "          1.1452e-02,  9.2465e-03, -1.3647e-04],\n",
      "        [-3.1984e-05,  1.0766e-04,  1.5088e-05, -2.8387e-05,  9.9039e-05,\n",
      "         -1.1429e-04, -1.2910e-05, -2.4361e-07],\n",
      "        [ 5.5554e-02,  2.2712e-02,  3.2975e-03,  9.1373e-03,  2.7628e-02,\n",
      "          4.2102e-02,  1.8921e-02, -3.8419e-04],\n",
      "        [ 1.0999e-02,  1.7749e-02,  2.7242e-03, -3.9852e-04,  1.9547e-02,\n",
      "         -3.4920e-03,  5.5216e-03, -1.3995e-04],\n",
      "        [ 3.5577e-02,  2.7625e-02,  3.4684e-03,  2.1833e-03,  3.1364e-02,\n",
      "          1.5632e-02,  1.0160e-02, -2.8844e-04],\n",
      "        [-3.8298e-02, -3.1941e-02, -3.8402e-03, -2.3341e-03, -3.5676e-02,\n",
      "         -1.8625e-02, -7.6479e-03,  3.2232e-04],\n",
      "        [ 4.4757e-02,  2.2586e-02,  3.0435e-03,  6.1220e-03,  2.7203e-02,\n",
      "          2.9911e-02,  1.4658e-02, -3.2599e-04],\n",
      "        [ 2.7193e-02,  2.0656e-02,  2.8593e-03,  2.7386e-03,  2.1561e-02,\n",
      "          1.4734e-02,  8.8068e-03, -2.1171e-04]])\n",
      "Episode 1: Total Return: -0.090, Epsilon: 1.00\n",
      "Episode 2: Total Return: -0.109, Epsilon: 0.88\n",
      "Episode 3: Total Return: -0.071, Epsilon: 0.77\n",
      "Episode 4: Total Return: 0.144, Epsilon: 0.68\n",
      "Episode 5: Total Return: 0.583, Epsilon: 0.60\n",
      "Episode 6: Total Return: 0.286, Epsilon: 0.53\n",
      "Episode 7: Total Return: 0.638, Epsilon: 0.46\n",
      "Episode 8: Total Return: 0.438, Epsilon: 0.41\n",
      "fc2.weight grad: tensor([[ 1.8638e-05,  7.4528e-05,  2.1411e-05,  2.7363e-04,  4.0825e-05,\n",
      "         -2.3617e-05,  6.4968e-05,  1.1819e-04],\n",
      "        [ 2.7553e-04, -4.7557e-05,  2.0090e-04, -1.5396e-03,  2.3513e-06,\n",
      "          6.1599e-05, -6.2558e-05, -4.2508e-05],\n",
      "        [ 3.7193e-05, -3.6777e-06,  9.3913e-06,  6.8725e-05,  2.8249e-05,\n",
      "         -8.8147e-05,  1.3922e-05,  1.2403e-04],\n",
      "        [-1.9508e-05, -2.3971e-04, -3.9280e-05, -1.0683e-03, -1.2723e-04,\n",
      "          8.3131e-05, -2.1171e-04, -3.7587e-04],\n",
      "        [-6.7801e-05, -1.1674e-04, -6.2521e-05, -2.4424e-04, -6.7742e-05,\n",
      "          3.0911e-05, -9.8872e-05, -1.8965e-04],\n",
      "        [ 1.8213e-04, -1.2778e-04,  1.2248e-04, -1.4818e-03, -4.8756e-05,\n",
      "          7.5810e-05, -1.2687e-04, -1.7850e-04],\n",
      "        [ 7.8628e-05, -5.0732e-05,  5.4073e-05, -6.2301e-04, -1.8831e-05,\n",
      "          3.0360e-05, -5.1103e-05, -6.9722e-05],\n",
      "        [ 2.5959e-04, -1.1155e-04,  1.8223e-04, -1.7727e-03, -3.2652e-05,\n",
      "          8.2249e-05, -1.1823e-04, -1.4419e-04]])\n",
      "Episode 9: Total Return: 0.514, Epsilon: 0.36\n",
      "Episode 10: Total Return: 0.319, Epsilon: 0.32\n",
      "Episode 11: Total Return: 0.540, Epsilon: 0.30\n",
      "Episode 12: Total Return: 0.511, Epsilon: 0.30\n"
     ]
    }
   ],
   "source": [
    "# learning_rate = 0.1\n",
    "# ls_lr = [.02, .03, .05]\n",
    "learning_rate = 0.02\n",
    "discount_factor = 0.1\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.88\n",
    "\n",
    "## Training constants\n",
    "total_episodes = 20\n",
    "total_epoch = 2\n",
    "number_of_pairs = len(workingPairOutcome)\n",
    "ls_epo_train_reward = []\n",
    "ls_epo_test_reward = []\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    env = PairTradeEnv(workingPairOutcome, top_keys, validPairsList, return_df)\n",
    "    agent = QLearningAgent(input_size=7, \n",
    "                           output_size=3, \n",
    "                           learning_rate=learning_rate, \n",
    "                           discount_factor=discount_factor, \n",
    "                           epsilon=1.0, \n",
    "                           epsilon_decay=0.88, \n",
    "                           batch_size=len(top_keys)*100, \n",
    "                           replay_buffer_size=10000)\n",
    "\n",
    "    ls_epi_reward = []\n",
    "    for episode in range(total_episodes):\n",
    "        arr_pair_reward = np.zeros(number_of_pairs)\n",
    "\n",
    "        for pair_idx in range(number_of_pairs):\n",
    "            state = env.reset(pair_idx)\n",
    "            pair_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = agent.choose_action(state)\n",
    "                next_state, reward, done, _ = env.step(action, pair_idx)\n",
    "                pair_reward += reward\n",
    "                reward /= dailypnl_sd\n",
    "\n",
    "                agent.store_experience(state, action, reward, next_state, done)\n",
    "                agent.learn()\n",
    "                \n",
    "                state = next_state\n",
    "\n",
    "            arr_pair_reward[pair_idx] = pair_reward\n",
    "        \n",
    "        total_reward = arr_pair_reward.mean()\n",
    "        print(f\"Episode {episode+1}: Total Return: {total_reward:.3f}, Epsilon: {agent.epsilon:.2f}\")\n",
    "        ls_epi_reward.append(total_reward)\n",
    "\n",
    "        agent.epsilon = max(0.3, agent.epsilon * agent.epsilon_decay)\n",
    "\n",
    "        plt.plot(ls_epi_reward)\n",
    "\n",
    "    agent.q_network.eval()\n",
    "    agent.target_network.eval()\n",
    "\n",
    "    ls_epo_train_reward.append(evaluate_agent_train(agent, env, number_of_pairs))\n",
    "    ls_epo_test_reward.append(evaluate_agent_test(agent, env, number_of_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "1.0\n",
      "0.88\n",
      "3\n",
      "10\n",
      "Evaluation: Average Total Train Return:[1.5686587630554472, 1.334381824485434, 0.3910475469273056]\n",
      "Evaluation: Average Total Train Return: mean 1.0980, SD 0.5090\n",
      "Evaluation: Average Total Test Return: [0.1360401584920572, 0.06058529518087073, 0.02115436661176917]\n",
      "Evaluation: Average Total Test Return: mean 0.0726, SD 0.0477\n"
     ]
    }
   ],
   "source": [
    "ls_print = [\n",
    "    learning_rate,\n",
    "    discount_factor,\n",
    "    epsilon,\n",
    "    epsilon_decay,\n",
    "    total_epoch,\n",
    "    total_episodes\n",
    "    ]\n",
    "for i, p in enumerate(ls_print):\n",
    "    print(f\"{p}\")\n",
    "print(f\"Evaluation: Average Total Train Return:{ls_epo_train_reward}\")\n",
    "print(f\"Evaluation: Average Total Train Return: mean {np.array(ls_epo_train_reward).mean():.4f}, SD {np.array(ls_epo_train_reward).std():.4f}\")\n",
    "print(f\"Evaluation: Average Total Test Return: {ls_epo_test_reward}\")\n",
    "print(f\"Evaluation: Average Total Test Return: mean {np.array(ls_epo_test_reward).mean():.4f}, SD {np.array(ls_epo_test_reward).std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values: tensor([[3.1623, 3.2272, 3.2066],\n",
      "        [3.6044, 3.7550, 3.7611],\n",
      "        [4.2959, 4.2977, 4.4672],\n",
      "        [2.7056, 2.7194, 2.7323],\n",
      "        [2.4055, 2.6625, 2.4283],\n",
      "        [3.2370, 3.1392, 3.0915]])\n",
      "Greedy actions: [0, 1, 1, 1, 0, -1]\n"
     ]
    }
   ],
   "source": [
    "# Assuming states is a list of 6 states, each a list or NumPy array\n",
    "states = [[-0.5, 0, 0, 0, 1, 0, 0],\n",
    "        [-1.5, 0, 0, 0, 1, 1, 0],\n",
    "        [-2.5, 0, 0, 0, 1, 1, 1],\n",
    "        [0.5, 0, 0, 1, 0, 0, 0],\n",
    "        [1.5, 1, 1, 1, 0, 0, 0],\n",
    "        [2.5, 1, 1, 1, 0, 0, 0]]\n",
    "\n",
    "# Convert to PyTorch tensor (ensure float32 for compatibility)\n",
    "states_tensor = torch.tensor(states, dtype=torch.float32)\n",
    "\n",
    "# Evaluate the Q-values for all states\n",
    "agent.q_network.eval()  # Set the network to evaluation mode\n",
    "with torch.no_grad():\n",
    "    q_values = agent.q_network(states_tensor)  # Output will be a tensor of shape (, output_size)\n",
    "\n",
    "# Example: Get the greedy actions for each state\n",
    "action_indices = torch.argmax(q_values, dim=1).tolist()\n",
    "actions = [agent.index_to_action[index] for index in action_indices]\n",
    "\n",
    "print(\"Q-values:\", q_values)\n",
    "print(\"Greedy actions:\", actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, save the entire Q-network\n",
    "torch.save(agent.q_network, f\"q_network_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/r7qn2m9n1zb6y_0q191gdqth0000gn/T/ipykernel_47880/3756087512.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_q_network = torch.load('q_network_20241215_131027.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (fc1): Linear(in_features=7, out_features=8, bias=True)\n",
       "  (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
       "  (fc3): Linear(in_features=8, out_features=3, bias=True)\n",
       "  (leaky_relu): LeakyReLU(negative_slope=0.01)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(-0.5, 0, 0, 0, 1, 0, 0): 0,\n",
       " (-1.5, 0, 0, 0, 1, 1, 0): 1,\n",
       " (-2.5, 0, 0, 0, 1, 1, 1): 1,\n",
       " (0.5, 0, 0, 1, 0, 0, 0): 1,\n",
       " (1.5, 1, 1, 1, 0, 0, 0): 0,\n",
       " (2.5, 1, 1, 1, 0, 0, 0): -1,\n",
       " (0, 0, 0, 0, 0, 0, 0): 0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_policy_actions = dict(zip([tuple(state) for state in states], actions))\n",
    "dict_policy_actions[(0, 0, 0, 0, 0, 0, 0)] = 0\n",
    "dict_policy_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saved Results\n",
    "## DQN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_q_network = torch.load(\"/Users/ju/Projects/00_SMU/mqf_practice/QF634_Applied_Quantitative_Research_Methods/QF634 Project/Pair Trading/25_q_network_20241215_140555.pth\")\n",
    "loaded_q_network.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_q_network = torch.load(\"/Users/ju/Projects/00_SMU/mqf_practice/QF634_Applied_Quantitative_Research_Methods/QF634 Project/Pair Trading/25_q_network_20241215_140555.pth\")\n",
    "loaded_q_network.eval()\n",
    "\n",
    "result_dict = {}\n",
    "\n",
    "for key, pair in zip(top_keys, validPairsList):\n",
    "    result_dict[key] = {}\n",
    "    result_dict[key]['state'] = workingPairOutcome[key]\n",
    "\n",
    "    t_array = torch.tensor(workingPairOutcome[key], dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        q_values = loaded_q_network(t_array)\n",
    "    index_to_action = {0: -1, 1: 0, 2: 1}\n",
    "    action_indices = torch.argmax(q_values, dim=1).tolist()\n",
    "    actions = [index_to_action[index] for index in action_indices]\n",
    "    actions[:261] = [0] * 261\n",
    "    result_dict[key]['action'] = actions\n",
    "\n",
    "    dummy_df = pairsOutcome[key]\n",
    "    dummy_df['position'] = result_dict[key]['action']\n",
    "    dummy_df[f\"{pair[0]} position\"] = dummy_df['position']\n",
    "    dummy_df[f\"{pair[1]} position\"] = dummy_df['position'] * -1\n",
    "\n",
    "    dummy_df['dailypnl'] = dummy_df[f'{pair[0]} position'].values*return_df[f'{pair[0]}'].shift(-1).values  + dummy_df[f'{pair[1]} position'].values*return_df[f'{pair[1]}'].shift(-1).values\n",
    "    dummy_df['cumpnl'] = dummy_df['dailypnl'].cumsum()\n",
    "    pairsOutcome[key] = dummy_df\n",
    "\n",
    "# combine daily returns from all pair trades\n",
    "dailypnl_data = [pairsOutcome[key]['dailypnl'] for key in top_keys]\n",
    "df_daily_returns =  pd.DataFrame(dailypnl_data).T # Transpose to align rows\n",
    "# obtain train and test returns\n",
    "baseline_train_mean_return = df_daily_returns.iloc[:2868].cumsum().mean(axis=1)[-1]\n",
    "baseline_test_mean_return = df_daily_returns.iloc[2869:].cumsum().iloc[-2].mean()\n",
    "print(\"=\"*20, \"Baseline Return Train\",\"=\"*20)\n",
    "print(f\"Mean: \\t\\t\\t{baseline_train_mean_return:.5f}\")\n",
    "print(f\"Mean (ave daily return): {baseline_train_mean_return/(2868-261):.5f}\")\n",
    "print(\"=\"*20, \"Baseline Return Test\",\"=\"*20)\n",
    "print(f\"Mean: \\t\\t\\t{baseline_test_mean_return:.5f}\")\n",
    "print(f\"Mean (ave daily return): {baseline_test_mean_return/(2979-2868):.5f}\")\n",
    "# print baseline restults to csv\n",
    "df_export = df_daily_returns.mean(axis=1).cumsum()[261:]\n",
    "df_export.to_csv(\"26_DQN_return.csv\")\n",
    "df_export.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15 Dec\n",
    "- lr = 0.01, discount factor 0.99. 10 episodes X 3 had good performance but slow. there was upward slope up to 0.6 total rewards.\n",
    "- rerun with learning rate at 0.1. also decreased discount factor to match Q table params. might find out that results are bad when we increase lr too fast. can try something in the middel like .05. Some learning but not as clean as 0.01. going to do grid search for .02 to .05, 10 episodes each\n",
    "\n",
    "    `0.1\n",
    "    1.0\n",
    "    0.88\n",
    "    3\n",
    "    10\n",
    "    Evaluation: Average Total Train Return:[1.5686587630554472, 1.334381824485434, 0.3910475469273056]\n",
    "    Evaluation: Average Total Train Return: mean 1.0980, SD 0.5090\n",
    "    Evaluation: Average Total Test Return: [0.1360401584920572, 0.06058529518087073, 0.02115436661176917]\n",
    "    Evaluation: Average Total Test Return: mean 0.0726, SD 0.0477`\n",
    "    - .02 seems to be the most consistent. but the growth in performance looks the same"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

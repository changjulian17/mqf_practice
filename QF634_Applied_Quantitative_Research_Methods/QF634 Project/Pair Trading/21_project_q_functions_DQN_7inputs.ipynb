{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/r7qn2m9n1zb6y_0q191gdqth0000gn/T/ipykernel_47880/1916853760.py:27: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  tpxData = pd.read_csv('TPX_prices.csv', index_col=0, parse_dates=True, date_parser=custom_date_parser)\n"
     ]
    }
   ],
   "source": [
    "## anaconda3 (Python 3.12.0) Kernel\n",
    "# pair trade packages\n",
    "import csv\n",
    "import gym\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "# nn packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "def custom_date_parser(date_str):\n",
    "    return datetime.strptime(date_str, '%d/%m/%Y')\n",
    "# Load the dictionary from the pickle file\n",
    "with open('pairsOutcome.pkl', 'rb') as file:\n",
    "    pairsOutcome = pickle.load(file)\n",
    "# Load stock data and get return \n",
    "tpxData = pd.read_csv('TPX_prices.csv', index_col=0, parse_dates=True, date_parser=custom_date_parser)\n",
    "tpxData = tpxData.dropna(axis='columns')\n",
    "return_df = (tpxData / tpxData.shift(1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pairs Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Pair Trade Portfolio\n",
    "`pairsOutcome` already have TOPIX stocks with highest liquidity and are tested for stationarity over a 1 year window\n",
    "\n",
    "Choose top 10 known pair trades by returns in the total dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 clustered trades:\n",
      "1. Key: 6503 JP Equity 7269 JP Equity, Return: 1.33\n",
      "2. Key: 6326 JP Equity 6954 JP Equity, Return: 1.19\n",
      "3. Key: 8053 JP Equity 8058 JP Equity, Return: 0.52\n",
      "4. Key: 4901 JP Equity 9613 JP Equity, Return: 1.10\n",
      "5. Key: 6988 JP Equity 7267 JP Equity, Return: 0.65\n",
      "6. Key: 4901 JP Equity 6702 JP Equity, Return: -0.34\n",
      "7. Key: 4684 JP Equity 7832 JP Equity, Return: 0.89\n",
      "8. Key: 7267 JP Equity 8306 JP Equity, Return: 1.16\n",
      "9. Key: 7267 JP Equity 8801 JP Equity, Return: 0.64\n",
      "10. Key: 4519 JP Equity 7532 JP Equity, Return: 1.14\n"
     ]
    }
   ],
   "source": [
    "with open(\"output_clustering.csv\", 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    # Skip the header row\n",
    "    next(reader)\n",
    "    working_pairs = [tuple(row) for row in reader]\n",
    "\n",
    "top_keys = [f\"{pair[0]} {pair[1]}\" for pair in working_pairs]\n",
    "print(\"Top 10 clustered trades:\")\n",
    "for i, key in enumerate(top_keys, 1):\n",
    "    print(f\"{i}. Key: {key}, Return: {pairsOutcome[key].cumpnl.iloc[-2]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/r7qn2m9n1zb6y_0q191gdqth0000gn/T/ipykernel_47880/2914914838.py:1: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  valid = pd.read_csv('validPairs5.csv',\n"
     ]
    }
   ],
   "source": [
    "valid = pd.read_csv('validPairs5.csv', \n",
    "                    index_col=0, \n",
    "                    parse_dates=True, \n",
    "                    date_parser=custom_date_parser)\n",
    "## get list of pair stocks\n",
    "validPairsList = [\n",
    "    [item.strip() + ' Equity' for item in pair.split('Equity') if item.strip()]\n",
    "    for pair in top_keys\n",
    "]\n",
    "\n",
    "# due to spikey-ness of reward around 0, scale from -1 to 1 and give more rewards above 1, 2, 3\n",
    "dailypnl_sd = 0.018390515013803736 \n",
    "# see /Users/ju/Projects/00_SMU/mqf_practice/QF634_Applied_Quantitative_Research_Methods/QF634 Project/Pair Trading/03_project_state_space_analysis.ipynb for SD derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "table is taking a long time to generalise. using one pair is not good enough to get any poistioning as everything is flat. only after adding all 10 pairs then performance churns out shorting on 1 SD high cross.\n",
    "\n",
    "Experiment: try making gamma 0.1, Q table should closely mimic the state space analysis table with strong mean reversion tendency. This is not what we see. and it even has Q values opposite to state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.00028887, 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [2.6859092 , 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [2.52197973, 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [2.40723813, 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [2.46852476, 1.        , 1.        , 1.        , 0.        ,\n",
       "        0.        , 0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollingWindow = 262\n",
    "cutLossSd = 2\n",
    "\n",
    "for pair in validPairsList:\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    #Calculate Standard Deviations\n",
    "    df['spread'] = valid[f'spread_{pair[0]}_{pair[1]}']\n",
    "    df['mid'] =  df['spread'].rolling(rollingWindow).mean()\n",
    "    df['1sd high'] = df['spread'].rolling(rollingWindow).mean() + df['spread'].rolling(rollingWindow).std()\n",
    "    df['1sd low'] = df['spread'].rolling(rollingWindow).mean() - df['spread'].rolling(rollingWindow).std()\n",
    "    df['2sd high'] = df['spread'].rolling(rollingWindow).mean() + df['spread'].rolling(rollingWindow).std() * cutLossSd\n",
    "    df['2sd low'] = df['spread'].rolling(rollingWindow).mean() - df['spread'].rolling(rollingWindow).std() * cutLossSd\n",
    "    df['position'] = 0\n",
    "\n",
    "    df.loc[(df['spread'] > df['1sd high']) & (df['spread'] < df['2sd high']), 'position'] = -1\n",
    "    df.loc[(df['spread']< df['1sd low']) & (df['spread'] > df['2sd low']), 'position'] = 1\n",
    "\n",
    "    #Calculate PnL\n",
    "    df[f'{pair[0]} position'] = df['position']\n",
    "    df[f'{pair[1]} position'] = df['position'] * -1\n",
    "    df['dailypnl'] = df[f'{pair[1]} position']*return_df[f'{pair[1]}'].shift(-1) + df[f'{pair[0]} position']*return_df[f'{pair[0]}'].shift(-1)\n",
    "    df['cumpnl'] = df['dailypnl'].cumsum()\n",
    "\n",
    "    pairsOutcome[f'{pair[0]} {pair[1]}'] = df\n",
    "\n",
    "workingPairOutcome = {}\n",
    "\n",
    "for pair in top_keys:\n",
    "    dummy_df = pairsOutcome[pair].iloc[::,:6]\n",
    "    dummy_df = dummy_df.subtract(dummy_df['mid'], axis=0).drop(columns=['mid']) # centre spread and SD\n",
    "    dummy_df = dummy_df.div(dummy_df['2sd high']-dummy_df['1sd high'],axis=0)   # express SD as integers, give spread as propotionate\n",
    "    dummy_df['2sd_high_boolean'] = (dummy_df['spread']>dummy_df['2sd high']).astype(int)\n",
    "    dummy_df['1sd_high_boolean'] = (dummy_df['spread']>dummy_df['1sd high']).astype(int)\n",
    "    dummy_df['0sd_high_boolean'] = (dummy_df['spread']>0).astype(int)\n",
    "    dummy_df['0sd_low_boolean']  = (dummy_df['spread']<0).astype(int)\n",
    "    dummy_df['1sd_low_boolean']  = (dummy_df['spread']<dummy_df['1sd low'] ).astype(int)\n",
    "    dummy_df['2sd_low_boolean']  = (dummy_df['spread']<dummy_df['2sd low'] ).astype(int)\n",
    "    dummy_df = dummy_df.drop(columns=['1sd high', '1sd low', '2sd high', '2sd low'])\n",
    "    workingPairOutcome[pair] = dummy_df.to_numpy()\n",
    "\n",
    "workingPairOutcome[top_keys[5]][-5:]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test one timestep at a time (even though we can test all at the same time)\n",
    "- give state\n",
    "- Trading should be path dependent due to stop loss. in this case I can only give last position as one of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairTradeEnv(gym.Env):\n",
    "    # ... (define your environment's state space, action space, etc.)\n",
    "    def __init__(self, workingPairOutcome, top_keys, validPairsList, return_df):\n",
    "        # ... (initialize other parameters)\n",
    "        self.earliest_step = 261\n",
    "        self.last_step = 2868\n",
    "        self.current_step = self.earliest_step\n",
    "\n",
    "\n",
    "    def step(self, action, pair_idx):\n",
    "        \"\"\"\n",
    "        Input\n",
    "            action: single value e.g. -1 (short)\n",
    "            pair_idx: index of pair trade\n",
    "        Output:\n",
    "            next_state: next state \n",
    "            reward: reward for last timestep\n",
    "            done: boolean for if end of dataset\n",
    "            info: optional\n",
    "        \"\"\"\n",
    "        self.current_step += 1\n",
    "        next_state = workingPairOutcome[top_keys[pair_idx]][self.current_step]\n",
    "        reward = self.calculate_reward(action, self.current_step, validPairsList[pair_idx])\n",
    "        done = self.current_step >= self.last_step\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def reset(self, pair_idx):\n",
    "        self.current_step = self.earliest_step\n",
    "        initial_state = workingPairOutcome[top_keys[pair_idx]][self.current_step]\n",
    "        return initial_state\n",
    "    \n",
    "    def calculate_reward(self, position, idx, pair):\n",
    "        \"\"\"\n",
    "        Give one _previous_ day's return\n",
    "        Input:\n",
    "            position: position for idx (current step)\n",
    "            idx: usually current timestp \n",
    "            pair: tuple of tpx stock\n",
    "        Output:\n",
    "            dailypnl\n",
    "        \"\"\"\n",
    "        position_0 = position\n",
    "        position_1 = position * -1\n",
    "        dailypnl = position_0*return_df[f'{pair[0]}'].iloc[idx] + position_1*return_df[f'{pair[1]}'].iloc[idx] \n",
    "\n",
    "        return dailypnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_priorities(replay_buffer, alpha=0.6):\n",
    "    \"\"\"\n",
    "    Compute priorities for sampling based on temporal difference (TD) error or recency.\n",
    "    \"\"\"\n",
    "    priorities = []\n",
    "    for experience in replay_buffer:\n",
    "        _, _, reward, _, _ = experience\n",
    "        td_error = abs(reward)  # Simplified proxy for TD error\n",
    "        priority = (td_error + 1e-5) ** alpha\n",
    "        priorities.append(priority)\n",
    "    return priorities\n",
    "\n",
    "def evaluate_agent_train(agent, env, number_of_pairs):\n",
    "    \"\"\"\n",
    "    Evaluates the agent's train performance over one episode without epsilon exploration.\n",
    "    \"\"\"\n",
    "    total_rewards = np.zeros(number_of_pairs)\n",
    "\n",
    "    for pair_idx in range(number_of_pairs):\n",
    "        state = env.reset(pair_idx)\n",
    "        env.current_step = 261\n",
    "        env.last_step = 2978\n",
    "        pair_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                q_values = agent.q_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "                action_index = torch.argmax(q_values, dim=1).item()\n",
    "                action = agent.index_to_action[action_index]\n",
    "            \n",
    "            # Take the selected action\n",
    "            next_state, reward, done, _ = env.step(action, pair_idx)\n",
    "            pair_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_rewards[pair_idx] = pair_reward\n",
    "\n",
    "    # Return the average reward across all pairs\n",
    "    return total_rewards.mean()\n",
    "\n",
    "def evaluate_agent_test(agent, env, number_of_pairs):\n",
    "    \"\"\"\n",
    "    Evaluates the agent's test performance over one episode without epsilon exploration.\n",
    "    \"\"\"\n",
    "    total_rewards = np.zeros(number_of_pairs)\n",
    "\n",
    "    for pair_idx in range(number_of_pairs):\n",
    "        state = env.reset(pair_idx)\n",
    "        env.current_step = 2868\n",
    "        env.last_step = 2978\n",
    "        pair_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                # Select action based purely on Q-network (greedy action)\n",
    "                q_values = agent.q_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "                action_index = torch.argmax(q_values, dim=1).item()\n",
    "                action = agent.index_to_action[action_index]\n",
    "            \n",
    "            # Take the selected action\n",
    "            next_state, reward, done, _ = env.step(action, pair_idx)\n",
    "            pair_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_rewards[pair_idx] = pair_reward\n",
    "\n",
    "    # Return the average reward across all pairs\n",
    "    return total_rewards.mean()\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_rate=0.2):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 8)\n",
    "        # self.dropout1 = nn.Dropout(p=dropout_rate) \n",
    "        self.fc2 = nn.Linear(8, 8)\n",
    "        # self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.fc3 = nn.Linear(8, output_size)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        # x = self.dropout1(x)\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        # x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, input_size, output_size, learning_rate, discount_factor, epsilon, epsilon_decay, batch_size=1000, replay_buffer_size=10000):\n",
    "        self.q_network = QNetwork(input_size, output_size)\n",
    "        self.target_network = QNetwork(input_size, output_size)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate) \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learn_count = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        \n",
    "        # Action to index mapping\n",
    "        self.action_to_index = {-1: 0, 0: 1, 1: 2}\n",
    "        self.index_to_action = {0: -1, 1: 0, 2: 1}\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice([-1, 0, 1])  # Explore\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "                action_index = torch.argmax(q_values, dim=1).item()  # Choose best action\n",
    "            action = self.index_to_action[action_index] \n",
    "        return action\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < self.replay_buffer_size:\n",
    "            return\n",
    "\n",
    "        # Compute priorities for sampling and sampler\n",
    "        priorities = compute_priorities(self.replay_buffer)\n",
    "        weights = np.array(priorities) / sum(priorities)\n",
    "        sampler = WeightedRandomSampler(weights, self.batch_size, replacement=True)\n",
    "        # dataloader = DataLoader(list(self.replay_buffer), batch_size=self.batch_size, sampler=sampler)\n",
    "        dataloader = DataLoader(list(self.replay_buffer), batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        # batch_count = 0\n",
    "        # if batch_count >=1:\n",
    "        #     break\n",
    "        states, actions, rewards, next_states, dones = next(iter(dataloader))\n",
    "        states = states.clone().detach().float()\n",
    "        next_states = next_states.clone().detach().float()\n",
    "        actions = torch.tensor([self.action_to_index[action.item()] for action in actions]).view(-1, 1)\n",
    "        rewards = rewards.clone().detach().float().view(-1, 1)\n",
    "        q_values = self.q_network(states).gather(1, actions)\n",
    "        dones = dones.float()\n",
    "\n",
    "        next_q_values = self.target_network(next_states).max(1, keepdim=True)[0].detach().view(-1, 1)\n",
    "        target_q_values = rewards + self.discount_factor * next_q_values * (1 - dones).view(-1,1)\n",
    "\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if self.learn_count%100000==0:\n",
    "            for name, param in agent.q_network.named_parameters():\n",
    "                if 'fc2.weight' in name and param.requires_grad:\n",
    "                    print(f\"{name} grad: {param.grad}\")\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.learn_count += 1\n",
    "        # Update target network every few episodes\n",
    "        if self.learn_count % 5 == 0:\n",
    "            self.update_target_network()\n",
    "        if self.learn_count % 100 == 0:\n",
    "            self.epsilon = max(0.3, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        # batch_count += 1\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc2.weight grad: tensor([[ 1.9679e-04,  2.4254e-02, -2.4425e-04,  3.2069e-03,  5.1245e-03,\n",
      "          8.3180e-03,  1.4518e-02, -2.8427e-04],\n",
      "        [-2.4881e-04, -1.0457e-02,  7.5941e-05,  2.9268e-03,  4.4463e-03,\n",
      "         -3.6420e-03, -3.2898e-03,  6.8644e-05],\n",
      "        [-1.8879e-05,  2.0666e-03, -2.8682e-05,  9.3808e-05,  9.3552e-05,\n",
      "          2.0437e-03,  1.6492e-03, -2.3585e-05],\n",
      "        [-2.8246e-06, -1.9596e-04,  2.1801e-06, -5.7032e-05, -2.5067e-05,\n",
      "         -8.1425e-05, -9.3927e-05,  2.2156e-06],\n",
      "        [-1.0183e-04, -2.9874e-02,  4.7019e-04, -2.9704e-02, -2.2796e-02,\n",
      "         -1.4612e-02, -1.9320e-02,  4.9390e-04],\n",
      "        [ 2.1499e-04,  2.8741e-02, -4.0422e-04,  2.0791e-02,  1.5729e-02,\n",
      "          1.3473e-02,  1.7372e-02, -4.2878e-04],\n",
      "        [ 3.0619e-05, -9.8875e-05,  1.3738e-04, -1.9964e-02, -1.1363e-02,\n",
      "         -4.6871e-03, -1.7942e-04,  1.0588e-04],\n",
      "        [ 3.9144e-06, -4.4767e-04,  1.1959e-05, -1.0810e-03, -8.5398e-04,\n",
      "         -3.9414e-04, -3.7445e-04,  1.3017e-05]])\n",
      "Episode 1: Total Return: 0.020, Epsilon: 0.92\n",
      "Episode 2: Total Return: 0.240, Epsilon: 0.81\n",
      "Episode 3: Total Return: 0.282, Epsilon: 0.71\n",
      "Episode 4: Total Return: 0.255, Epsilon: 0.62\n",
      "fc2.weight grad: tensor([[-9.3917e-05, -4.7022e-05,  1.7288e-04,  3.3815e-04, -9.6263e-03,\n",
      "          2.2185e-04,  2.9956e-04,  3.3988e-05],\n",
      "        [ 8.7836e-04, -2.4375e-04, -2.4608e-04,  5.8030e-04, -1.5318e-02,\n",
      "          4.5675e-04,  6.3016e-04,  5.5520e-05],\n",
      "        [-8.3664e-04,  5.3841e-05,  8.1377e-04,  3.4145e-04, -1.1166e-02,\n",
      "          1.3693e-04,  1.6969e-04,  4.5542e-05],\n",
      "        [ 1.5088e-03, -6.0932e-04,  5.0053e-04,  1.8398e-03, -5.1475e-02,\n",
      "          1.2747e-03,  1.7335e-03,  2.0196e-04],\n",
      "        [-4.9483e-02,  2.4357e-02, -3.3917e-02, -8.1051e-02,  2.2943e+00,\n",
      "         -5.4522e-02, -7.3876e-02, -9.0527e-03],\n",
      "        [ 9.9081e-04,  2.3345e-05, -1.2733e-03, -8.3132e-04,  2.5599e-02,\n",
      "         -4.3098e-04, -5.6204e-04, -1.0213e-04],\n",
      "        [-1.4860e-03,  6.4806e-04, -6.5410e-04, -2.0505e-03,  5.7580e-02,\n",
      "         -1.4071e-03, -1.9113e-03, -2.2540e-04],\n",
      "        [-1.4261e-03,  5.8351e-04, -4.8814e-04, -1.7805e-03,  4.9823e-02,\n",
      "         -1.2330e-03, -1.6767e-03, -1.9503e-04]])\n",
      "Episode 5: Total Return: 0.063, Epsilon: 0.55\n",
      "Episode 6: Total Return: 0.116, Epsilon: 0.48\n",
      "Episode 7: Total Return: 0.424, Epsilon: 0.42\n",
      "Episode 8: Total Return: 0.534, Epsilon: 0.37\n",
      "fc2.weight grad: tensor([[-1.5580e-03,  1.3173e-04, -1.7185e-04,  4.4592e-04, -5.2366e-03,\n",
      "          2.7802e-04,  3.5901e-04,  1.0460e-04],\n",
      "        [-8.4472e-04,  7.2226e-05, -2.6975e-04,  2.7430e-04, -3.3334e-03,\n",
      "          1.5743e-04,  1.7823e-04,  7.4757e-05],\n",
      "        [-2.7702e-03,  2.9282e-04,  4.6551e-05,  8.8053e-04, -1.0296e-02,\n",
      "          6.2193e-04,  8.2437e-04,  2.1084e-04],\n",
      "        [-4.4165e-03,  5.0560e-04, -6.6071e-04,  1.6290e-03, -1.9631e-02,\n",
      "          1.1023e-03,  1.3362e-03,  4.4704e-04],\n",
      "        [ 3.5095e-02, -6.5212e-03,  3.0691e-03, -1.9580e-02,  2.1503e-01,\n",
      "         -1.3174e-02, -1.4598e-02, -5.2589e-03],\n",
      "        [ 2.4425e-03, -2.3607e-04, -3.7394e-04, -6.6285e-04,  7.4721e-03,\n",
      "         -4.8752e-04, -7.0534e-04, -1.3107e-04],\n",
      "        [ 1.5407e-03, -1.4987e-04,  3.2457e-04, -5.1733e-04,  6.2297e-03,\n",
      "         -3.2503e-04, -3.8876e-04, -1.3834e-04],\n",
      "        [ 9.0623e-04, -1.1448e-04, -3.1536e-06, -3.3750e-04,  4.0120e-03,\n",
      "         -2.4754e-04, -3.1601e-04, -8.9039e-05]])\n",
      "Episode 9: Total Return: 0.470, Epsilon: 0.33\n",
      "Episode 10: Total Return: 0.603, Epsilon: 0.30\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_size = 7\n",
    "output_size = 3 \n",
    "learning_rate = 0.01\n",
    "discount_factor = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.9995\n",
    "\n",
    "## Training constants\n",
    "total_episodes = 10\n",
    "total_epoch = 1\n",
    "number_of_pairs = len(workingPairOutcome)\n",
    "ls_epo_train_reward = []\n",
    "ls_epo_test_reward = []\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    env = PairTradeEnv(workingPairOutcome, top_keys, validPairsList, return_df)\n",
    "    agent = QLearningAgent(input_size, output_size, learning_rate, discount_factor, epsilon, epsilon_decay)\n",
    "\n",
    "    ls_epi_reward = []\n",
    "    for episode in range(total_episodes):\n",
    "        arr_pair_reward = np.zeros(number_of_pairs)\n",
    "\n",
    "        for pair_idx in range(number_of_pairs):\n",
    "            state = env.reset(pair_idx)\n",
    "            pair_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = agent.choose_action(state)\n",
    "                next_state, reward, done, _ = env.step(action, pair_idx)\n",
    "                pair_reward += reward\n",
    "                reward /= dailypnl_sd\n",
    "\n",
    "                agent.store_experience(state, action, reward, next_state, done)\n",
    "                agent.learn()\n",
    "                \n",
    "                state = next_state\n",
    "\n",
    "            arr_pair_reward[pair_idx] = pair_reward\n",
    "        \n",
    "        total_reward = arr_pair_reward.mean()\n",
    "        print(f\"Episode {episode+1}: Total Return: {total_reward:.3f}, Epsilon: {agent.epsilon:.2f}\")\n",
    "        ls_epi_reward.append(total_reward)\n",
    "\n",
    "        plt.plot(ls_epi_reward)\n",
    "\n",
    "    agent.q_network.eval()\n",
    "    agent.target_network.eval()\n",
    "\n",
    "    ls_epo_train_reward.append(evaluate_agent_train(agent, env, number_of_pairs))\n",
    "    ls_epo_test_reward.append(evaluate_agent_test(agent, env, number_of_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "3\n",
      "0.01\n",
      "0.99\n",
      "1.0\n",
      "0.9995\n",
      "1\n",
      "10\n",
      "Evaluation: Average Total Train Return:[1.1838475472078316]\n",
      "Evaluation: Average Total Train Return: mean 1.1838, SD 0.0000\n",
      "Evaluation: Average Total Test Return: [0.18319940459773282]\n",
      "Evaluation: Average Total Test Return: mean 0.1832, SD 0.0000\n"
     ]
    }
   ],
   "source": [
    "ls_print = [\n",
    "    input_size,\n",
    "    output_size,\n",
    "    learning_rate,\n",
    "    discount_factor,\n",
    "    epsilon,\n",
    "    epsilon_decay,\n",
    "    total_epoch,\n",
    "    total_episodes\n",
    "    ]\n",
    "for i, p in enumerate(ls_print):\n",
    "    print(f\"{p}\")\n",
    "print(f\"Evaluation: Average Total Train Return:{ls_epo_train_reward}\")\n",
    "print(f\"Evaluation: Average Total Train Return: mean {np.array(ls_epo_train_reward).mean():.4f}, SD {np.array(ls_epo_train_reward).std():.4f}\")\n",
    "print(f\"Evaluation: Average Total Test Return: {ls_epo_test_reward}\")\n",
    "print(f\"Evaluation: Average Total Test Return: mean {np.array(ls_epo_test_reward).mean():.4f}, SD {np.array(ls_epo_test_reward).std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# After training, save the entire Q-network\n",
    "torch.save(agent.q_network, f\"q_network_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/r7qn2m9n1zb6y_0q191gdqth0000gn/T/ipykernel_47880/3756087512.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_q_network = torch.load('q_network_20241215_131027.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (fc1): Linear(in_features=7, out_features=8, bias=True)\n",
       "  (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
       "  (fc3): Linear(in_features=8, out_features=3, bias=True)\n",
       "  (leaky_relu): LeakyReLU(negative_slope=0.01)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_q_network = torch.load('q_network_20241215_131027.pth')\n",
    "\n",
    "loaded_q_network = QNetwork(7,3)  # Replace QNetwork with your model class\n",
    "loaded_q_network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values: tensor([[3.1623, 3.2272, 3.2066],\n",
      "        [3.6044, 3.7550, 3.7611],\n",
      "        [4.2959, 4.2977, 4.4672],\n",
      "        [2.7056, 2.7194, 2.7323],\n",
      "        [2.4055, 2.6625, 2.4283],\n",
      "        [3.2370, 3.1392, 3.0915]])\n",
      "Greedy actions: [0, 1, 1, 1, 0, -1]\n"
     ]
    }
   ],
   "source": [
    "# Assuming states is a list of 6 states, each a list or NumPy array\n",
    "states = [[-0.5, 0, 0, 0, 1, 0, 0],\n",
    "        [-1.5, 0, 0, 0, 1, 1, 0],\n",
    "        [-2.5, 0, 0, 0, 1, 1, 1],\n",
    "        [0.5, 0, 0, 1, 0, 0, 0],\n",
    "        [1.5, 1, 1, 1, 0, 0, 0],\n",
    "        [2.5, 1, 1, 1, 0, 0, 0]]\n",
    "\n",
    "# Convert to PyTorch tensor (ensure float32 for compatibility)\n",
    "states_tensor = torch.tensor(states, dtype=torch.float32)\n",
    "\n",
    "# Evaluate the Q-values for all states\n",
    "agent.q_network.eval()  # Set the network to evaluation mode\n",
    "with torch.no_grad():\n",
    "    q_values = agent.q_network(states_tensor)  # Output will be a tensor of shape (6, output_size)\n",
    "\n",
    "# Example: Get the greedy actions for each state\n",
    "action_indices = torch.argmax(q_values, dim=1).tolist()\n",
    "actions = [agent.index_to_action[index] for index in action_indices]\n",
    "\n",
    "print(\"Q-values:\", q_values)\n",
    "print(\"Greedy actions:\", actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(-0.5, 0, 0, 0, 1, 0, 0): 0,\n",
       " (-1.5, 0, 0, 0, 1, 1, 0): 1,\n",
       " (-2.5, 0, 0, 0, 1, 1, 1): 1,\n",
       " (0.5, 0, 0, 1, 0, 0, 0): 1,\n",
       " (1.5, 1, 1, 1, 0, 0, 0): 0,\n",
       " (2.5, 1, 1, 1, 0, 0, 0): -1,\n",
       " (0, 0, 0, 0, 0, 0, 0): 0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_policy_actions = dict(zip([tuple(state) for state in states], actions))\n",
    "dict_policy_actions[(0, 0, 0, 0, 0, 0, 0)] = 0\n",
    "dict_policy_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30 Nov\n",
    "- first few tries, network is very large\n",
    "- added epsilon search in \"choose_action\" functionso that there will be some chance to explore\n",
    "- changed reward function to multiply losses and give exponential returns to incentivise risk taking\n",
    "\n",
    "### 1 dec 2105: \n",
    "- might have performance is always oscillating negative and positive. This might be because of too large a learning rate. also start from start of training periods max steps to be 3000 so that total results are comparable\n",
    "    - this helped quite abit. \n",
    "    \n",
    "`\n",
    "input_size = 7  # Adjust to your specific input size\n",
    "output_size = 3  # Adjust to your desired number of discrete actions\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.8\n",
    "epsilon = 1 # down to .3\n",
    "epsilon_decay = 0.9999\n",
    "num_episodes = 500\n",
    "max_steps_per_episode = 3000\n",
    "`\n",
    "\n",
    "- want to try changing epsilon to only update after the entire episode instead of after each step. its decaying too quickly\n",
    "- I want to try with changing reward by changing \"learn\" to use total_reward instead of \"reward\"\n",
    "- Scale the states. need to explore scaling the state since it is still in terms of absolute differences. NN is not able to do proportions\n",
    "- training epochs should be smaller at up to 30 days because mean reversion pattern is 1 to 33 days\n",
    "    - very bad performance with 40 day epochs\n",
    "\n",
    "### 1 dec 2217:\n",
    "- changed target q value fxn to remove exponential reward and scaled negative reward. now both positive and negative are the same. added portion of total reward in episode to incentivise more long term rewards.\n",
    "    - `        if reward > 0:\n",
    "            target_q_value = reward + self.discount_factor * next_q_value * (1 - done) + total_reward * .1\n",
    "        else:\n",
    "            target_q_value = reward + self.discount_factor * next_q_value * (1 - done) + total_reward * .1`\n",
    "    -       `  if episode%1==0:\n",
    "            agent.epsilon *= agent.epsilon_decay`\n",
    "\n",
    "### 2 Dec 2101:\n",
    "- managed to scale but results are not any better\n",
    "- thinking of reducing learning rate to reduce the oscillations\n",
    "    - will try to run with learning rate at 0.01\n",
    "- right now total reward is taking all of the target q function. maybe can make it a 50/50 split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 dec\n",
    "- training taking \n",
    "    - a full length training dataset.\n",
    "    - 1000 per learning step\n",
    "    - learning rate test (1.5 mins per episode)\n",
    "        - episodes: 5\n",
    "        - learning rate: 0.05 ==> total reward: .037\n",
    "        - learning rate: 0.5 ==> total reward: -.6\n",
    "        - learning rate: 0.3 ==> total reward: 0.88, .044\n",
    "        - learning rate: 0.15 ==> total reward: -.023\n",
    "        - learning rate: 0.25 ==> total reward: -.6\n",
    "        - learning rate: 0.35 ==> total reward: .028\n",
    "    - with drop out layer test (1.75 mins per episode)\n",
    "        - learning rate: 0.3 ==> total reward: -0.488\n",
    "        - learning rate: 0.4 ==> total reward: -0.422\n",
    "        - learning rate: 0.5 ==> total reward: .26, .13, .096\n",
    "        - learning rate: 0.6 ==> total reward: .03\n",
    "        - learning rate: 0.7 ==> total reward: -.08\n",
    "- performance still bad. should include dropout layer? --> performance a bit worse but more consistent\n",
    "- try removing spread so that the input is only boolean of SD and last position\n",
    "    - drop out layer test (1.75 mins per episode)\n",
    "        - learning rate: 0.1 ==> total reward: -.36\n",
    "        - learning rate: 0.3 ==> total reward: -.01\n",
    "        - learning rate: 0.5 ==> total reward: -.26\n",
    "### 5 dec\n",
    "- previously target and online network updated at the same time, but it should be used to regularise. So will try with updating more periodically instead of every learn step. reduced NN to 16 and 8 hidden layers with dropout (1.67 min per episode)\n",
    "    - learning rate: 0.3, update target every 10 learn occurences ==> total reward: .03\n",
    "    - learning rate: 0.3, update target every 100 learn occurences ==> total reward: .26\n",
    "    - learning rate: 0.3, update target every 500 learn occurences ==> total reward: -.383\n",
    "    - learning rate: 0.3, update target every 600 learn occurences ==> total reward: .22\n",
    "    - learning rate: 0.3, update target every 675 learn occurences ==> total reward: .41\n",
    "    - learning rate: 0.3, update target every 700 learn occurences ==> total reward: .10, .03\n",
    "    - learning rate: 0.3, update target every 750 learn occurences ==> total reward: .40, .15, -0.353\n",
    "    - learning rate: 0.3, update target every 1000 learn occurences ==> total reward: .05\n",
    "    - learning rate: 0.3, update target every 10 learn occurences, remove dropout layers ==> total reward: -.203\n",
    "    - learning rate: 0.5, update target every 10 learn occurences, remove dropout layers ==> total reward: -.334\n",
    "- changed ADAM optimiser to SGD (1.6 min per episode)\n",
    "    - learning rate: 0.15, update target every 750 learn occurences ==> total reward: .084\n",
    "    - learning rate: 0.3, update target every 750 learn occurences ==> total reward: -.20\n",
    "    - learning rate: 0.5, update target every 750 learn occurences ==> total reward: .303\n",
    "    - learning rate: 0.6, update target every 750 learn occurences ==> total reward: -.110\n",
    "    - learning rate: 0.7, update target every 750 learn occurences ==> total reward: -.556, -0.449\n",
    "- long term run with 50 episodes\n",
    "    - SGD, learning rate: 0.5, update target every 750 learn occurences ==> total reward: -.04 (79 min)\n",
    "    - ADAM, learning rate: 0.3, update target every 750 learn occurences ==> total reward: -.01 (84.5 min)\n",
    "    - ADAM, 32 hidden layer, learning rate: 0.3, update target every 750 learn occurences ==> total reward: 0.54 (88 min)\n",
    "- long term run with 300 episodes\n",
    "    - ADAM, 32X16 hidden layer, learning rate: 0.3, update target every 750 learn occurences ==> total reward:  -0.28(540 min)\n",
    "### 6 Dec\n",
    "1. reduce to only 4 SD flags - Done\n",
    "2. discount factor up to .99 - Done\n",
    "3. try increasing punishment with X10 negative reward if less than 0 - Done\n",
    "\n",
    "- there is a positive gradient\n",
    "    - 5 episodes, X10 negative reward ADAM, 32X16 hidden layer, learning rate: 0.3, update target every 750 learn occurences ==> total reward: .05\n",
    "    - 5 episodes, X10 negative reward ADAM, 32X8 hidden layer, learning rate: 0.3, update target every 750 learn occurences ==> total reward: -0.01\n",
    "    - 5 episodes, X100 negative reward ADAM, 32X16 hidden layer, learning rate: 0.3, update target every 750 learn occurences ==> total reward: -0.01 (8 mins)\n",
    "    - 5 episodes, X100 negative reward ADAM, 32X16 hidden layer, learning rate: 0.3, update target every 750 learn occurences ==> total reward: 0.014\n",
    "    - 300 episodes, X100 negative reward ADAM, 32X16 hidden layer, learning rate: 0.3, update target every 750 learn occurences ==> total reward: 0.017\n",
    "    - 50 episodes, X100 negative reward ADAM, 32X16 hidden layer, learning rate: 0.3, update target every 750 learn occurences, discount_factor = 0.99 ==> total reward: \n",
    "- try reducing net to only 4 neurons\n",
    "    - 5 episodes, X100 negative reward ADAM, 4X4 hidden layer, learning rate: 0.3, update target every 750 learn occurences ==> total reward: 0.0\n",
    "    - 10 episodes, X100 negative reward ADAM, 4X4 hidden layer, learning rate: 0.3, update target every 750 learn occurences ==> total reward: 0.0\n",
    "### 8 Dec \n",
    "- 50 episodes, X100 negative reward ADAM, 4X4 hidden layer, learning rate: 0.3, update target every 750 learn occurences, discount_factor = 0.99 ==> total reward: 0.118\n",
    "- 300 episodes, X100 negative reward ADAM, 4X4 hidden layer, learning rate: 0.3, update target every 750 learn occurences, discount_factor = 0.99 ==> total reward: -0.02 (epsilon 10), 0.29 (epsilon .46)\n",
    "- try removing previous action in state space to move to 4 state spaces only . 4X4\n",
    "    - 4X4, 10 episodes, X100 negative reward ADAM, 4X4 hidden layer, learning rate: 0.3, update target every 750 learn occurences, discount_factor = 0.99 ==> total reward: 0.282\n",
    "    - 4X4, epsilon_decay = 0.955, 20 episodes, X100 negative reward ADAM, learning rate: 0.3, update target every 750 learn occurences, discount_factor = 0.99 ==> total reward: -.20\n",
    "    - 4X4, epsilon_decay = 0.9885, 20 episodes, X100 negative reward ADAM, learning rate: 0.3, update target every 750 learn occurences, discount_factor = 0.99 ==> total reward: 0.08\n",
    "    - 32X4, epsilon_decay = 0.9885, 20 episodes, X100 negative reward ADAM, learning rate: 0.3, update target every 750 learn occurences, discount_factor = 0.99 ==> total reward: 0.07\n",
    "    - add back dropout layers, 32X4, epsilon_decay = 0.9885, 20 episodes, X100 negative reward ADAM, learning rate: 0.3, update target every 750 learn occurences, discount_factor = 0.99 ==> total reward: .21\n",
    "    - add back dropout layers, 32X4, epsilon_decay = 0.9885, 10 episodes, X100 negative reward ADAM, learning rate: 0.3, update target every 750 learn occurences, discount_factor = 0.5 ==> total reward: .002\n",
    "    - add back dropout layers, 32X4, epsilon_decay = 0.9885, 10 episodes, X100 negative reward ADAM, learning rate: 0.3, update target every 750 learn occurences, discount_factor = 0.5 ==> total reward: .002\n",
    "    - add back dropout layers, 32X4, epsilon_decay = 0.9885, 300 episodes, X100 negative reward ADAM, learning rate: 0.3, update target every 750 learn occurences, discount_factor = 0.99 ==> total reward: 0.115\n",
    "### 9 Dec\n",
    "- state cases should be imbalanced\n",
    "    - add back dropout layers, 32X4, epsilon_decay = 0.9885, 20 episodes,  ADAM, learning rate: 0.3, update target every 1000 learn occurences, discount_factor = 0.99 ==> total reward: -0.29\n",
    "    - add back dropout layers, 32X4, epsilon_decay = 0.9885, 5 episodes,  ADAM, learning rate: 0.3, update target every 1000 learn occurences, discount_factor = 0.99 ==> total reward: 0.223\n",
    "    - add back dropout layers, 32X4, epsilon_decay = 0.9885, 5 episodes, ADAM, learning rate: 0.3, update target every 550 learn occurences, discount_factor = 0.99 ==> total reward: 0.126\n",
    "    - add back dropout layers, 32X4, epsilon_decay = 0.9885, 10 episodes,  ADAM, learning rate: 0.3, update target every 550 learn occurences, discount_factor = 0.99 ==> total reward: \n",
    "- changed data set to 17 pairs\n",
    "    - add back dropout layers, 32X4, epsilon_decay = 0.9885, 10 episodes,  ADAM, learning rate: 0.3, update target every 550 learn occurences, discount_factor = 0.99 ==> total reward: -0.16\n",
    "    - 17 pairs, add back dropout layers, 32X4, epsilon_decay = 0.9885, 5 episodes,  ADAM, learning rate: 0.3, update target every 550 learn occurences, discount_factor = 0.99, changed reward *5 for positive, -1 for negative ==> total reward:0.06 (at least got positive gradient), try run again to get more data on consistency. also need to observe if exploiting might be necessary\n",
    "    - 17 pairs, add back dropout layers, 32X4, epsilon_decay = 0.9885, 5 episodes,  ADAM, learning rate: 0.3, update target every 550 learn occurences, discount_factor = 0.99, changed reward *5 for positive, -1 for negative ==> total reward:0.087 \n",
    "\n",
    "### 10 Dec\n",
    "- changed data set to 17 pairs\n",
    "    - 17 pairs, add back dropout layers, 32X4, epsilon_decay = 0.9885, 10 episodes,  ADAM, learning rate: 0.3, update target every 550 learn occurences, discount_factor = 0.99, changed reward *5 for positive, -1 for negative ==> total reward:-0.10 \n",
    "- no need for complicated network, reduce to 8 (5 min per ep)\n",
    "    - 17 pairs, add back dropout layers, 8X4, epsilon_decay = 0.9885, 5 episodes,  ADAM, learning rate: 0.3, update target every 550 learn occurences, discount_factor = 0.99, changed reward *5 for positive, -1 for negative ==> total reward: -0.113\n",
    "- try removing dropout layers\n",
    "    - 17 pairs, add back dropout layers, 8X4, epsilon_decay = 0.9885, 5 episodes,  ADAM, learning rate: 0.3, update target every 550 learn occurences, discount_factor = 0.99, changed reward *5 for positive, -1 for negative ==> total reward: 0.04\n",
    "- replay above 20241209 good result\n",
    "    - **v20241210_0** add back dropout layers, 32X4, epsilon_decay = 0.9885, 5 episodes,  ADAM, learning rate: 0.3, update target every 1000 learn occurences, discount_factor = 0.99 ==> total reward: 0.33 (20 min per episode)\n",
    "    - add back dropout layers, 32X4, epsilon_decay = 0.9885, 10 episodes,  ADAM, learning rate: 0.3, update target every 1000 learn occurences, discount_factor = 0.99 ==> total reward: 0.082 at episode 5, 0.195 at episode 10 (18.5 mins per episode)\n",
    "- Think need to update the target network more frequently. might be missing the learnings from the policy network.\n",
    "    - add back dropout layers, 32X4, epsilon_decay = 0.9885, 30 episodes,  ADAM, learning rate: 0.3, update target every 50 learn occurences, discount_factor = 0.99 ==> total reward: -.11 after 28 episodes (18.5 mins per sepisode)-0.04 after 5 episodes\n",
    "\n",
    "### 11 Dec\n",
    "- revert to **v20241210_0**, verify result is with reward modification\n",
    "    - **v20241211_0** epsilon update after 2000, modified reward *5 positive / -1 else, update target every 2000 learn occurences ==> ***total reward: 0.44***\n",
    "- revert to **v20241210_0**, verify result \n",
    "    - without reward modification, ensure update network 1000 learn occ. 3 episodes only, epsilon_decay = 0.93 ==> still doing short only\n",
    "- experimenting with one episode and get the q-values:\n",
    "    - 1 episode has very even Q value across all actions ==> Q values all 0 (18 mins)\n",
    "    - 1 episode try with reward shaping max(1,reward*5) positive else -1 ==> all shorting emphasise Short term rewards too much(18 mins)\n",
    "    - 1 episode try with reward shaping reward*5 if positive else -1 ==> all shorting still\n",
    "    - 1 episode try with reward shaping reward if positive else -1 to reward ==> all shorting still\n",
    "    - 1 episode try with reward shaping reward if positive elif <0 -1 to reward ==> all flat\n",
    "    - 1 episode try with reward shaping 2*reward if positive elif <0 -1 to reward ==> all flat\n",
    "    - 1 episode try with reward shaping 5*reward if positive elif <0 -1 to reward ==> all flat\n",
    "    - 1 episode try with reward shaping +1 reward if positive elif <0 -1 to reward ==> all short\n",
    "- add back spread as one continuous variable for DQN\n",
    "    - use ***v20241211_1*** architecture except. learning rate: .3, eps decay: 0.9885, episode: 1, learn every 100 occ, 4x4, ADAM, disc factor: .99 ==> Q-values are different! but seems it is too underfit.. mark this as first 7 input version with some fitting. mark this as ***v20241211_2***\n",
    "    - v20241211_2 but with learning rate: .9 ==> learning rate too high, oscillating for one gradient from 0 to non-zero through training period\n",
    "    - v20241211_2 but with learning rate: .5 ==> still underfit as both test and train are negative but small returns\n",
    "    - v20241211_2 to overcome the underfitting in both previous scenarios plan to increase neurons and decrease learning rate.\n",
    "        - v20241211_2: 8x8 nn ==> overfit\n",
    "- seems that it is underfit rather than overfit because, it can get the general trend of mean reversion or choose a general model. First try to train more episodes before changing the params. can afford to reduce the eps decay\n",
    "\n",
    "### 14 Dec\n",
    "- update workflow to run multiple epoch to auto check consistency. did 10 episodes 2 epochs over 37 mins. not too long. \n",
    "- running large batch of 50 episodes 3 epochs to see if SD of the returns can reduce.\n",
    "- reward also observed to be -.2 to +.2 range. with very high 0 values. must find a way to reduce the peakness. for now *5 the reward\n",
    "\n",
    "- trying reward / reward SD\n",
    "    - i think this is the right track \n",
    "    \n",
    "- special reward function with more rewards. (30 mins)\n",
    "                if reward > 2:\n",
    "                    reward = 2\n",
    "                elif reward > .5:\n",
    "                    reward += 1\n",
    "                elif reward < -2:\n",
    "                    reward = -2\n",
    "                elif reward < -.5:\n",
    "                    reward += -1\n",
    "\n",
    "`7\n",
    "3\n",
    "0.9\n",
    "0.99\n",
    "1.0\n",
    "0.9995\n",
    "3\n",
    "3\n",
    "Evaluation: Average Total Train Return:[-0.020302455715636845, -0.026456559360757727, -0.015957870073517066]\n",
    "Evaluation: Average Total Train Return: mean -0.0209, SD 0.0043\n",
    "Evaluation: Average Total Test Return: [0.0, 0.04100139645240182, 0.010911636716948203]\n",
    "Evaluation: Average Total Test Return: mean 0.0173, SD 0.0173`\n",
    "\n",
    "- try without scaled rewards\n",
    "\n",
    "`7\n",
    "3\n",
    "0.9\n",
    "0.99\n",
    "1.0\n",
    "0.9995\n",
    "3\n",
    "3\n",
    "Evaluation: Average Total Train Return:[-0.21204523123119973, 0.0, -0.08536008671074533]\n",
    "Evaluation: Average Total Train Return: mean -0.0991, SD 0.0871\n",
    "Evaluation: Average Total Test Return: [-0.07280725995434345, 0.0, -0.0437009527948386]\n",
    "Evaluation: Average Total Test Return: mean -0.0388, SD 0.0299`\n",
    "\n",
    "- use normalised reward. but just make the episode longer. we can reduce the learning rate now that the reward is on track\n",
    "10 epidodes with very good SD. \n",
    "\n",
    "`7\n",
    "3\n",
    "0.1\n",
    "0.99\n",
    "1.0\n",
    "0.9995\n",
    "3\n",
    "10\n",
    "Evaluation: Average Total Train Return:[0.01290827870161031, 0.19459444744142035, 0.05879513532063172]\n",
    "Evaluation: Average Total Train Return: mean 0.0888, SD 0.0771\n",
    "Evaluation: Average Total Test Return: [0.007286558280539401, 0.01949411710155653, 0.06346864693540508]\n",
    "Evaluation: Average Total Test Return: mean 0.0301, SD 0.0241`/\n",
    "reducing learning rate is important. very long run with much smaller learning rate.\n",
    "\n",
    "- used normalised reward\n",
    "\n",
    "`7\n",
    "3\n",
    "0.01\n",
    "0.99\n",
    "1.0\n",
    "0.9995\n",
    "3\n",
    "10\n",
    "Evaluation: Average Total Train Return:[-0.0893182425866827, -0.07026478040190354, -0.006342248897760472]\n",
    "Evaluation: Average Total Train Return: mean -0.0553, SD 0.0355\n",
    "Evaluation: Average Total Test Return: [0.03880864466354409, -0.05088046311132578, 0.010911636716948203]\n",
    "Evaluation: Average Total Test Return: mean -0.0004, SD 0.0375\n",
    "`\n",
    "\n",
    "- try removing resampling and see performance. but want to do above for longer up to 100 episodes\n",
    "\n",
    "`7\n",
    "3\n",
    "0.01\n",
    "0.99\n",
    "1.0\n",
    "0.9995\n",
    "1\n",
    "10\n",
    "Evaluation: Average Total Train Return:[0.3405281364631186]\n",
    "Evaluation: Average Total Train Return: mean 0.3405, SD 0.0000\n",
    "Evaluation: Average Total Test Return: [-0.029588736150981697]\n",
    "Evaluation: Average Total Test Return: mean -0.0296, SD 0.0000`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Challenge\n",
    "\n",
    "## Background\n",
    "Initial evaluation of the baseline portfolio shows that draw downs are small. Originally team had the idea of using Machine Learning to optimise for sizing of these pair trades. However since there was no significant drawdowns the returns are linearly increasing with investment sizing i.e. greater nominal investment in the the pair trade the proportionate increase in returns without realising significant drawdown risk.\n",
    "\n",
    "Instead of optimising for sizing, we can explore Machine Learning in terms of strategy on this stationary dataset. Whereas our prescribed strategy is to enter at +/- 1 std dev, exit at 0 with +/- 2 std dev stop loss. These are only suggestions and arbitrary levels.\n",
    "\n",
    "With Machine Learning, we can discover if it will uncover the mean reverting nature and recommend another threshhold. We use Q Learner to understand state space with the same spread, mid, std dev parameters as the baseline.\n",
    "\n",
    "### Steps\n",
    "#### Environment:\n",
    "- State Space: A set of all possible states the agent can be in. \n",
    "  - [spread, mid, 2 sd low, 1 sd low, 1 sd high, 2 sd high]\n",
    "- Action Space: A set of all possible actions the agent can take in each state.  \n",
    "  - [-1, # short\\\n",
    "      0, # uninvested\\\n",
    "      1  # long]   \n",
    "- Reward Function: A function that assigns a numerical reward to each state-action pair, indicating the immediate consequence of taking a particular action in a specific state.\n",
    "  - dailypnl\n",
    "- Transition Function: A function that determines the probability of transitioning from one state to another when a particular action is taken.\n",
    "  - deterministic based on historical performance\n",
    "#### Agent:\n",
    "\n",
    "- Q-Table: A matrix that stores the estimated Q-values for each state-action pair. Q-values represent the expected future reward for taking a specific action in a given state.  \n",
    "  - continuous Q table?\n",
    "- Learning Rate (): A parameter that controls how much the Q-values are updated with each new experience.  \n",
    "- Discount Factor (): A parameter that determines the importance of future rewards. A higher discount factor gives more weight to future rewards.  \n",
    "- Exploration Rate (): A parameter that controls the balance between exploration (trying new actions) and exploitation (choosing the action with the highest Q-value).  \n",
    "- Q-Learning Algorithm:\n",
    "\n",
    "  - Initialization: Initialize the Q-table with random values or zeros.  \n",
    "  - Exploration and Exploitation: Use an exploration strategy (e.g., -greedy) to choose an action:\n",
    "    - With probability , choose a random action.  \n",
    "    - With probability 1-, choose the action with the highest Q-value for the current state.  \n",
    " \n",
    "  - Take Action: Execute the chosen action in the environment.  \n",
    "  - Observe Reward and Next State: Observe the immediate reward and the next state resulting from the action.\n",
    "- Update Q-Value: Update the Q-value of the current state-action pair using the following formula:\n",
    "\n",
    "## Make indicators and spread stationary around 0\n",
    "Deduct the mean from all values to translate to 0 axis\n",
    "\n",
    "#### Training and Test set\n",
    "\n",
    "2013 is used for warm start\\\n",
    "2014 - 2023 train data since NN need a lot of training data {end 2023 idx == 2868}\\\n",
    "2024 onwards (5 months) test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

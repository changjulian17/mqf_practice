{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## anaconda3 (Python 3.12.0) Kernel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# pair trade packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pairs Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary loaded from pairsOutcome.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/r7qn2m9n1zb6y_0q191gdqth0000gn/T/ipykernel_76659/3199335484.py:12: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  tpxData = pd.read_csv('TPX_prices.csv', index_col=0, parse_dates=True, date_parser=custom_date_parser)\n"
     ]
    }
   ],
   "source": [
    "def custom_date_parser(date_str):\n",
    "    return datetime.strptime(date_str, '%d/%m/%Y')\n",
    "\n",
    "# Load the dictionary from the pickle file\n",
    "with open('pairsOutcome.pkl', 'rb') as file:\n",
    "    pairsOutcome = pickle.load(file)\n",
    "\n",
    "print(\"Dictionary loaded from pairsOutcome.pkl\")\n",
    "\n",
    "\n",
    "# Load stock data and get return \n",
    "tpxData = pd.read_csv('TPX_prices.csv', index_col=0, parse_dates=True, date_parser=custom_date_parser)\n",
    "tpxData = tpxData.dropna(axis='columns')\n",
    "return_df = (tpxData / tpxData.shift(1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Pair Trade Portfolio\n",
    "`pairsOutcome` already have TOPIX stocks with highest liquidity and are tested for stationarity over a 1 year window\n",
    "\n",
    "Choose top 10 known pair trades by returns in the total dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 performing trades:\n",
      "1. Key: 1801 JP Equity 2670 JP Equity, Value: 2.5797887367591246\n",
      "2. Key: 3778 JP Equity 6701 JP Equity, Value: 2.537242032391529\n",
      "3. Key: 2760 JP Equity 6254 JP Equity, Value: 2.3688208386917404\n",
      "4. Key: 5706 JP Equity 6954 JP Equity, Value: 2.2676474298290237\n",
      "5. Key: 7951 JP Equity 9684 JP Equity, Value: 2.0657325467200596\n",
      "6. Key: 1808 JP Equity 6481 JP Equity, Value: 1.9929348941248262\n",
      "7. Key: 3099 JP Equity 5831 JP Equity, Value: 1.939742664925484\n",
      "8. Key: 1808 JP Equity 6971 JP Equity, Value: 1.9132602773493155\n",
      "9. Key: 4021 JP Equity 9843 JP Equity, Value: 1.8675031161000868\n",
      "10. Key: 5929 JP Equity 6504 JP Equity, Value: 1.811533049967201\n"
     ]
    }
   ],
   "source": [
    "# Sort the keys by their cumpnl[-2] values in descending order\n",
    "top_keys = sorted(\n",
    "    pairsOutcome,\n",
    "    key=lambda k: pairsOutcome[k].cumpnl.iloc[-2],  # Access cumpnl[-2] safely\n",
    "    reverse=True\n",
    ")[:10]  # Get the top 10 keys\n",
    "\n",
    "# Print the top 10 performing trades\n",
    "print(\"Top 10 performing trades:\")\n",
    "for i, key in enumerate(top_keys, 1):\n",
    "    print(f\"{i}. Key: {key}, Value: {pairsOutcome[key].cumpnl.iloc[-2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Challenge\n",
    "\n",
    "## Background\n",
    "Initial evaluation of the baseline portfolio shows that draw downs are small. Originally team had the idea of using Machine Learning to optimise for sizing of these pair trades. However since there was no significant drawdowns the returns are linearly increasing with investment sizing i.e. greater nominal investment in the the pair trade the proportionate increase in returns without realising significant drawdown risk.\n",
    "\n",
    "Instead of optimising for sizing, we can explore Machine Learning in terms of strategy on this stationary dataset. Whereas our prescribed strategy is to enter at +/- 1 std dev, exit at 0 with +/- 2 std dev stop loss. These are only suggestions and arbitrary levels.\n",
    "\n",
    "With Machine Learning, we can discover if it will uncover the mean reverting nature and recommend another threshhold. We use Q Learner to understand state space with the same spread, mid, std dev parameters as the baseline.\n",
    "\n",
    "### Steps\n",
    "#### Environment:\n",
    "- State Space: A set of all possible states the agent can be in.  \n",
    "  - [spread, mid, 2 sd low, 1 sd low, 1 sd high, 2 sd high]\n",
    "- Action Space: A set of all possible actions the agent can take in each state.   \n",
    "  - [-1, # short\\\n",
    "      0, # uninvested\\\n",
    "      1  # long]   \n",
    "- Reward Function: A function that assigns a numerical reward to each state-action pair, indicating the immediate consequence of taking a particular action in a specific state.\n",
    "  - dailypnl\n",
    "- Transition Function: A function that determines the probability of transitioning from one state to another when a particular action is taken.\n",
    "  - deterministic based on historical performance\n",
    "#### Agent:\n",
    "\n",
    "- Q-Table: A matrix that stores the estimated Q-values for each state-action pair. Q-values represent the expected future reward for taking a specific action in a given state.   \n",
    "  - continuous Q table?\n",
    "- Learning Rate (α): A parameter that controls how much the Q-values are updated with each new experience.   \n",
    "- Discount Factor (γ): A parameter that determines the importance of future rewards. A higher discount factor gives more weight to future rewards.   \n",
    "- Exploration Rate (ε): A parameter that controls the balance between exploration (trying new actions) and exploitation (choosing the action with the highest Q-value).   \n",
    "- Q-Learning Algorithm:\n",
    "\n",
    "  - Initialization: Initialize the Q-table with random values or zeros.   \n",
    "  - Exploration and Exploitation: Use an exploration strategy (e.g., ε-greedy) to choose an action:\n",
    "    - With probability ε, choose a random action.   \n",
    "    - With probability 1-ε, choose the action with the highest Q-value for the current state.   \n",
    "  \n",
    "  - Take Action: Execute the chosen action in the environment.   \n",
    "  - Observe Reward and Next State: Observe the immediate reward and the next state resulting from the action.\n",
    "- Update Q-Value: Update the Q-value of the current state-action pair using the following formula:\n",
    "\n",
    "#### Training and Test set\n",
    "\n",
    "2013 is used for warm start\\\n",
    "2014 - 2023 train data since NN need a lot of training data {end 2023 idx == 2868}\\\n",
    "2024 onwards (5 months) test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/r7qn2m9n1zb6y_0q191gdqth0000gn/T/ipykernel_76659/199844939.py:4: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  valid = pd.read_csv('validPairs4.csv',\n"
     ]
    }
   ],
   "source": [
    "## Get pair stock data\n",
    "def custom_date_parser(date_str):\n",
    "    return datetime.strptime(date_str, '%d/%m/%Y')\n",
    "valid = pd.read_csv('validPairs4.csv', \n",
    "                    index_col=0, \n",
    "                    parse_dates=True, \n",
    "                    date_parser=custom_date_parser)\n",
    "## get list of pair stocks\n",
    "validPairsList = [\n",
    "    [item.strip() + ' Equity' for item in pair.split('Equity') if item.strip()]\n",
    "    for pair in top_keys\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollingWindow = 262\n",
    "cutLossSd = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in validPairsList:\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    #Calculate Standard Deviations\n",
    "    df['spread'] = valid[f'spread_{pair[0]}_{pair[1]}']\n",
    "    df['mid'] =  df['spread'].rolling(rollingWindow).mean()\n",
    "    df['1sd high'] = df['spread'].rolling(rollingWindow).mean() + df['spread'].rolling(rollingWindow).std()\n",
    "    df['1sd low'] = df['spread'].rolling(rollingWindow).mean() - df['spread'].rolling(rollingWindow).std()\n",
    "    df['2sd high'] = df['spread'].rolling(rollingWindow).mean() + df['spread'].rolling(rollingWindow).std() * cutLossSd\n",
    "    df['2sd low'] = df['spread'].rolling(rollingWindow).mean() - df['spread'].rolling(rollingWindow).std() * cutLossSd\n",
    "    df['position'] = 0\n",
    "\n",
    "    df.loc[(df['spread'] > df['1sd high']) & (df['spread'] < df['2sd high']), 'position'] = -1\n",
    "    df.loc[(df['spread']< df['1sd low']) & (df['spread'] > df['2sd low']), 'position'] = 1\n",
    "\n",
    "    #Calculate PnL\n",
    "    df[f'{pair[0]} position'] = df['position']\n",
    "    df[f'{pair[1]} position'] = df['position'] * -1\n",
    "    df['dailypnl'] = df[f'{pair[1]} position']*return_df[f'{pair[1]}'].shift(-1) + df[f'{pair[0]} position']*return_df[f'{pair[0]}'].shift(-1)\n",
    "    df['cumpnl'] = df['dailypnl'].cumsum()\n",
    "\n",
    "    pairsOutcome[f'{pair[0]} {pair[1]}'] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make indicators and spread stationary around 0\n",
    "Deduct the mean from all values to translate to 0 axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "workingPairOutcome = {}\n",
    "\n",
    "for pair in top_keys:\n",
    "    dummy_df = pairsOutcome[top_keys[0]].iloc[::,:6]\n",
    "    dummy_df = dummy_df.subtract(dummy_df['mid'], axis=0).drop(columns=['mid']) # centre spread and SD\n",
    "    dummy_df = dummy_df.div(dummy_df['2sd high']-dummy_df['1sd high'],axis=0)   # express SD as integers, give spread as propotionate\n",
    "    dummy_df['1sd_high_boolean'] = (dummy_df['spread']>dummy_df['1sd high']).astype(int)\n",
    "    dummy_df['2sd_high_boolean'] = (dummy_df['spread']>dummy_df['2sd high']).astype(int)\n",
    "    dummy_df['1sd_low_boolean'] =  (dummy_df['spread']<dummy_df['1sd low'] ).astype(int)\n",
    "    dummy_df['2sd_low_boolean'] =  (dummy_df['spread']<dummy_df['2sd low'] ).astype(int)\n",
    "    dummy_df = dummy_df.drop(columns=['spread','1sd high', '1sd low', '2sd high', '2sd low'])\n",
    "    workingPairOutcome[pair] = dummy_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workingPairOutcome[top_keys[5]][-5:]     # spread is not a proportion and direction of SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workingPairOutcome = {top_keys[0]:workingPairOutcome[top_keys[0]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test one timestep at a time (even though we can test all at the same time)\n",
    "- give state\n",
    "- Trading should be path dependent due to stop loss. in this case I can only give last position as one of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "class PairTradeEnv(gym.Env):\n",
    "    # ... (define your environment's state space, action space, etc.)\n",
    "    def __init__(self, workingPairOutcome, top_keys, validPairsList, return_df):\n",
    "        # ... (initialize other parameters)\n",
    "        self.earliest_step = 261  # hot start\n",
    "        self.last_step = 2868\n",
    "        # self.current_step = random.randint(self.earliest_step, self.last_step - 1)\n",
    "        self.current_step = self.earliest_step\n",
    "\n",
    "\n",
    "    def step(self, action, pair_idx):\n",
    "        \"\"\"\n",
    "        Input\n",
    "            action: single value e.g. -1 (short)\n",
    "            pair_idx: index of pair trade\n",
    "        Output:\n",
    "            next_state: next state \n",
    "            reward: reward for last timestep\n",
    "            done: boolean for if end of dataset\n",
    "            info: optional\n",
    "        \"\"\"\n",
    "        # Advance the time step\n",
    "        self.current_step += 1\n",
    "        # Get the next state\n",
    "        next_state = np.append(\n",
    "                            workingPairOutcome[top_keys[pair_idx]][self.current_step],\n",
    "                            action)\n",
    "        # Calculate reward (implement your reward function here)\n",
    "        reward = self.calculate_reward(action, self.current_step, validPairsList[pair_idx]) # TODO change pair selected\n",
    "        # Check for termination (implement your termination condition here)\n",
    "        done = self.current_step >= self.last_step\n",
    "\n",
    "        # Provide additional information (optional)\n",
    "        info = {}\n",
    "\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def reset(self, pair_idx):\n",
    "        # ... (implement the reset function to initialize the environment)\n",
    "        # reset to start of 2014 every time\n",
    "        # self.current_step = random.randint(self.earliest_step, self.last_step - 1)\n",
    "        self.current_step = self.earliest_step\n",
    "        initial_state = np.append(\n",
    "                            workingPairOutcome[top_keys[pair_idx]][self.current_step],\n",
    "                            0)\n",
    "        return initial_state\n",
    "    \n",
    "    def calculate_reward(self, position, idx, pair):\n",
    "        \"\"\"\n",
    "        Give one _previous_ day's return\n",
    "        Input:\n",
    "            position: position for idx (current step)\n",
    "            idx: usually current timestp \n",
    "            pair: tuple of tpx stock\n",
    "        Output:\n",
    "            dailypnl\n",
    "        \"\"\"\n",
    "        # position = position_vector @ np.array([-1,0,1])\n",
    "        position_0 = position\n",
    "        position_1 = position * -1\n",
    "        ## return_df gives the return for the previous day for the given idx\n",
    "        dailypnl = position_0*return_df[f'{pair[0]}'].iloc[idx] + position_1*return_df[f'{pair[1]}'].iloc[idx] \n",
    "\n",
    "        return dailypnl\n",
    "\n",
    "# Instantiate the custom environment\n",
    "env = PairTradeEnv(workingPairOutcome, top_keys, validPairsList, return_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward: 0.038, Epsilon: 1.00\n",
      "Episode 2: Total Reward: -0.038, Epsilon: 1.00\n",
      "Episode 3: Total Reward: -0.385, Epsilon: 1.00\n",
      "Episode 4: Total Reward: -0.077, Epsilon: 1.00\n",
      "Episode 5: Total Reward: 0.618, Epsilon: 1.00\n",
      "Episode 6: Total Reward: 0.207, Epsilon: 1.00\n",
      "Episode 7: Total Reward: -0.299, Epsilon: 1.00\n",
      "Episode 8: Total Reward: -0.244, Epsilon: 1.00\n",
      "Episode 9: Total Reward: 0.282, Epsilon: 1.00\n",
      "Episode 10: Total Reward: 0.269, Epsilon: 1.00\n",
      "Episode 11: Total Reward: -0.238, Epsilon: 1.00\n",
      "Episode 12: Total Reward: -0.090, Epsilon: 1.00\n",
      "Episode 13: Total Reward: 0.195, Epsilon: 1.00\n",
      "Episode 14: Total Reward: 0.032, Epsilon: 1.00\n",
      "Episode 15: Total Reward: 0.060, Epsilon: 1.00\n",
      "Episode 16: Total Reward: 0.004, Epsilon: 1.00\n",
      "Episode 17: Total Reward: -0.147, Epsilon: 1.00\n",
      "Episode 18: Total Reward: 0.085, Epsilon: 1.00\n",
      "Episode 19: Total Reward: 0.342, Epsilon: 1.00\n",
      "Episode 20: Total Reward: 0.310, Epsilon: 1.00\n",
      "Episode 21: Total Reward: 0.051, Epsilon: 1.00\n",
      "Episode 22: Total Reward: -0.011, Epsilon: 0.99\n",
      "Episode 23: Total Reward: 0.431, Epsilon: 0.99\n",
      "Episode 24: Total Reward: 0.090, Epsilon: 0.99\n",
      "Episode 25: Total Reward: -0.029, Epsilon: 0.99\n",
      "Episode 26: Total Reward: -0.121, Epsilon: 0.99\n",
      "Episode 27: Total Reward: -0.132, Epsilon: 0.99\n",
      "Episode 28: Total Reward: 0.420, Epsilon: 0.99\n",
      "Episode 29: Total Reward: 1.055, Epsilon: 0.99\n",
      "Episode 30: Total Reward: 0.060, Epsilon: 0.99\n",
      "Episode 31: Total Reward: 0.360, Epsilon: 0.99\n",
      "Episode 32: Total Reward: 0.684, Epsilon: 0.99\n",
      "Episode 33: Total Reward: 0.185, Epsilon: 0.99\n",
      "Episode 34: Total Reward: -0.074, Epsilon: 0.99\n",
      "Episode 35: Total Reward: 0.771, Epsilon: 0.99\n",
      "Episode 36: Total Reward: 0.319, Epsilon: 0.99\n",
      "Episode 37: Total Reward: -0.392, Epsilon: 0.98\n",
      "Episode 38: Total Reward: 0.204, Epsilon: 0.98\n",
      "Episode 39: Total Reward: -0.238, Epsilon: 0.98\n",
      "Episode 40: Total Reward: -0.256, Epsilon: 0.98\n",
      "Episode 41: Total Reward: -0.528, Epsilon: 0.98\n",
      "Episode 42: Total Reward: 0.287, Epsilon: 0.98\n",
      "Episode 43: Total Reward: 0.033, Epsilon: 0.98\n",
      "Episode 44: Total Reward: -0.002, Epsilon: 0.98\n",
      "Episode 45: Total Reward: -0.393, Epsilon: 0.98\n",
      "Episode 46: Total Reward: 0.169, Epsilon: 0.98\n",
      "Episode 47: Total Reward: -0.042, Epsilon: 0.98\n",
      "Episode 48: Total Reward: -0.118, Epsilon: 0.97\n",
      "Episode 49: Total Reward: -0.434, Epsilon: 0.97\n",
      "Episode 50: Total Reward: 0.159, Epsilon: 0.97\n",
      "Episode 51: Total Reward: -0.259, Epsilon: 0.97\n",
      "Episode 52: Total Reward: -0.171, Epsilon: 0.97\n",
      "Episode 53: Total Reward: -0.120, Epsilon: 0.97\n",
      "Episode 54: Total Reward: 0.526, Epsilon: 0.97\n",
      "Episode 55: Total Reward: -0.185, Epsilon: 0.97\n",
      "Episode 56: Total Reward: -0.044, Epsilon: 0.97\n",
      "Episode 57: Total Reward: 0.200, Epsilon: 0.96\n",
      "Episode 58: Total Reward: 0.063, Epsilon: 0.96\n",
      "Episode 59: Total Reward: -0.381, Epsilon: 0.96\n",
      "Episode 60: Total Reward: 0.157, Epsilon: 0.96\n",
      "Episode 61: Total Reward: 0.861, Epsilon: 0.96\n",
      "Episode 62: Total Reward: -0.056, Epsilon: 0.96\n",
      "Episode 63: Total Reward: -0.592, Epsilon: 0.96\n",
      "Episode 64: Total Reward: -0.364, Epsilon: 0.95\n",
      "Episode 65: Total Reward: -0.178, Epsilon: 0.95\n",
      "Episode 66: Total Reward: 0.115, Epsilon: 0.95\n",
      "Episode 67: Total Reward: 0.010, Epsilon: 0.95\n",
      "Episode 68: Total Reward: -0.304, Epsilon: 0.95\n",
      "Episode 69: Total Reward: 0.079, Epsilon: 0.95\n",
      "Episode 70: Total Reward: 0.412, Epsilon: 0.95\n",
      "Episode 71: Total Reward: 0.072, Epsilon: 0.94\n",
      "Episode 72: Total Reward: -0.100, Epsilon: 0.94\n",
      "Episode 73: Total Reward: -0.378, Epsilon: 0.94\n",
      "Episode 74: Total Reward: 0.036, Epsilon: 0.94\n",
      "Episode 75: Total Reward: 0.056, Epsilon: 0.94\n",
      "Episode 76: Total Reward: -0.165, Epsilon: 0.94\n",
      "Episode 77: Total Reward: -0.480, Epsilon: 0.93\n",
      "Episode 78: Total Reward: -0.056, Epsilon: 0.93\n",
      "Episode 79: Total Reward: 0.020, Epsilon: 0.93\n",
      "Episode 80: Total Reward: -0.123, Epsilon: 0.93\n",
      "Episode 81: Total Reward: -0.541, Epsilon: 0.93\n",
      "Episode 82: Total Reward: -0.201, Epsilon: 0.93\n",
      "Episode 83: Total Reward: 0.282, Epsilon: 0.92\n",
      "Episode 84: Total Reward: 0.030, Epsilon: 0.92\n",
      "Episode 85: Total Reward: 0.080, Epsilon: 0.92\n",
      "Episode 86: Total Reward: -0.436, Epsilon: 0.92\n",
      "Episode 87: Total Reward: 0.122, Epsilon: 0.92\n",
      "Episode 88: Total Reward: -0.260, Epsilon: 0.91\n",
      "Episode 89: Total Reward: -0.259, Epsilon: 0.91\n",
      "Episode 90: Total Reward: 0.496, Epsilon: 0.91\n",
      "Episode 91: Total Reward: 0.282, Epsilon: 0.91\n",
      "Episode 92: Total Reward: 0.252, Epsilon: 0.91\n",
      "Episode 93: Total Reward: 0.137, Epsilon: 0.90\n",
      "Episode 94: Total Reward: -0.450, Epsilon: 0.90\n",
      "Episode 95: Total Reward: -0.375, Epsilon: 0.90\n",
      "Episode 96: Total Reward: -0.030, Epsilon: 0.90\n",
      "Episode 97: Total Reward: 0.683, Epsilon: 0.90\n",
      "Episode 98: Total Reward: -0.096, Epsilon: 0.89\n",
      "Episode 99: Total Reward: 0.174, Epsilon: 0.89\n",
      "Episode 100: Total Reward: -0.357, Epsilon: 0.89\n",
      "Episode 101: Total Reward: 0.181, Epsilon: 0.89\n",
      "Episode 102: Total Reward: 0.608, Epsilon: 0.88\n",
      "Episode 103: Total Reward: -0.089, Epsilon: 0.88\n",
      "Episode 104: Total Reward: 0.350, Epsilon: 0.88\n",
      "Episode 105: Total Reward: -0.253, Epsilon: 0.88\n",
      "Episode 106: Total Reward: 0.467, Epsilon: 0.88\n",
      "Episode 107: Total Reward: -0.040, Epsilon: 0.87\n",
      "Episode 108: Total Reward: -0.061, Epsilon: 0.87\n",
      "Episode 109: Total Reward: -0.321, Epsilon: 0.87\n",
      "Episode 110: Total Reward: -0.336, Epsilon: 0.87\n",
      "Episode 111: Total Reward: 0.052, Epsilon: 0.86\n",
      "Episode 112: Total Reward: -0.472, Epsilon: 0.86\n",
      "Episode 113: Total Reward: 0.256, Epsilon: 0.86\n",
      "Episode 114: Total Reward: -0.342, Epsilon: 0.86\n",
      "Episode 115: Total Reward: -0.070, Epsilon: 0.85\n",
      "Episode 116: Total Reward: 0.229, Epsilon: 0.85\n",
      "Episode 117: Total Reward: 0.107, Epsilon: 0.85\n",
      "Episode 118: Total Reward: 0.245, Epsilon: 0.85\n",
      "Episode 119: Total Reward: 0.404, Epsilon: 0.84\n",
      "Episode 120: Total Reward: -0.379, Epsilon: 0.84\n",
      "Episode 121: Total Reward: -0.104, Epsilon: 0.84\n",
      "Episode 122: Total Reward: -0.332, Epsilon: 0.83\n",
      "Episode 123: Total Reward: -0.066, Epsilon: 0.83\n",
      "Episode 124: Total Reward: 0.402, Epsilon: 0.83\n",
      "Episode 125: Total Reward: 0.083, Epsilon: 0.83\n",
      "Episode 126: Total Reward: 0.294, Epsilon: 0.82\n",
      "Episode 127: Total Reward: 0.305, Epsilon: 0.82\n",
      "Episode 128: Total Reward: -0.535, Epsilon: 0.82\n",
      "Episode 129: Total Reward: 0.302, Epsilon: 0.82\n",
      "Episode 130: Total Reward: -0.257, Epsilon: 0.81\n",
      "Episode 131: Total Reward: 0.279, Epsilon: 0.81\n",
      "Episode 132: Total Reward: 0.194, Epsilon: 0.81\n",
      "Episode 133: Total Reward: -0.007, Epsilon: 0.80\n",
      "Episode 134: Total Reward: 0.081, Epsilon: 0.80\n",
      "Episode 135: Total Reward: 0.296, Epsilon: 0.80\n",
      "Episode 136: Total Reward: -0.199, Epsilon: 0.79\n",
      "Episode 137: Total Reward: -0.475, Epsilon: 0.79\n",
      "Episode 138: Total Reward: -0.078, Epsilon: 0.79\n",
      "Episode 139: Total Reward: 0.023, Epsilon: 0.79\n",
      "Episode 140: Total Reward: 0.161, Epsilon: 0.78\n",
      "Episode 141: Total Reward: 0.003, Epsilon: 0.78\n",
      "Episode 142: Total Reward: -0.089, Epsilon: 0.78\n",
      "Episode 143: Total Reward: 0.446, Epsilon: 0.77\n",
      "Episode 144: Total Reward: -0.007, Epsilon: 0.77\n",
      "Episode 145: Total Reward: 0.067, Epsilon: 0.77\n",
      "Episode 146: Total Reward: 0.054, Epsilon: 0.76\n",
      "Episode 147: Total Reward: 0.528, Epsilon: 0.76\n",
      "Episode 148: Total Reward: 0.126, Epsilon: 0.76\n",
      "Episode 149: Total Reward: -0.771, Epsilon: 0.75\n",
      "Episode 150: Total Reward: -0.376, Epsilon: 0.75\n",
      "Episode 151: Total Reward: -0.568, Epsilon: 0.75\n",
      "Episode 152: Total Reward: 0.231, Epsilon: 0.74\n",
      "Episode 153: Total Reward: -0.349, Epsilon: 0.74\n",
      "Episode 154: Total Reward: -0.167, Epsilon: 0.74\n",
      "Episode 155: Total Reward: -0.340, Epsilon: 0.73\n",
      "Episode 156: Total Reward: 0.391, Epsilon: 0.73\n",
      "Episode 157: Total Reward: -0.186, Epsilon: 0.73\n",
      "Episode 158: Total Reward: 0.341, Epsilon: 0.72\n",
      "Episode 159: Total Reward: -0.396, Epsilon: 0.72\n",
      "Episode 160: Total Reward: 0.295, Epsilon: 0.72\n",
      "Episode 161: Total Reward: -0.130, Epsilon: 0.71\n",
      "Episode 162: Total Reward: -0.168, Epsilon: 0.71\n",
      "Episode 163: Total Reward: -0.146, Epsilon: 0.70\n",
      "Episode 164: Total Reward: -0.008, Epsilon: 0.70\n",
      "Episode 165: Total Reward: 0.283, Epsilon: 0.70\n",
      "Episode 166: Total Reward: 0.307, Epsilon: 0.69\n",
      "Episode 167: Total Reward: 0.229, Epsilon: 0.69\n",
      "Episode 168: Total Reward: 0.498, Epsilon: 0.69\n",
      "Episode 169: Total Reward: 0.222, Epsilon: 0.68\n",
      "Episode 170: Total Reward: 0.133, Epsilon: 0.68\n",
      "Episode 171: Total Reward: -0.004, Epsilon: 0.68\n",
      "Episode 172: Total Reward: -0.467, Epsilon: 0.67\n",
      "Episode 173: Total Reward: 0.437, Epsilon: 0.67\n",
      "Episode 174: Total Reward: 0.225, Epsilon: 0.66\n",
      "Episode 175: Total Reward: 0.015, Epsilon: 0.66\n",
      "Episode 176: Total Reward: 0.180, Epsilon: 0.66\n",
      "Episode 177: Total Reward: 0.241, Epsilon: 0.65\n",
      "Episode 178: Total Reward: -0.515, Epsilon: 0.65\n",
      "Episode 179: Total Reward: 0.009, Epsilon: 0.64\n",
      "Episode 180: Total Reward: 0.324, Epsilon: 0.64\n",
      "Episode 181: Total Reward: -0.227, Epsilon: 0.64\n",
      "Episode 182: Total Reward: -0.194, Epsilon: 0.63\n",
      "Episode 183: Total Reward: -0.098, Epsilon: 0.63\n",
      "Episode 184: Total Reward: -0.065, Epsilon: 0.62\n",
      "Episode 185: Total Reward: 0.273, Epsilon: 0.62\n",
      "Episode 186: Total Reward: -0.148, Epsilon: 0.62\n",
      "Episode 187: Total Reward: -0.023, Epsilon: 0.61\n",
      "Episode 188: Total Reward: 0.513, Epsilon: 0.61\n",
      "Episode 189: Total Reward: -0.099, Epsilon: 0.60\n",
      "Episode 190: Total Reward: -0.073, Epsilon: 0.60\n",
      "Episode 191: Total Reward: -0.215, Epsilon: 0.59\n",
      "Episode 192: Total Reward: -0.129, Epsilon: 0.59\n",
      "Episode 193: Total Reward: -0.521, Epsilon: 0.59\n",
      "Episode 194: Total Reward: -0.124, Epsilon: 0.58\n",
      "Episode 195: Total Reward: -0.605, Epsilon: 0.58\n",
      "Episode 196: Total Reward: -0.125, Epsilon: 0.57\n",
      "Episode 197: Total Reward: -0.137, Epsilon: 0.57\n",
      "Episode 198: Total Reward: 0.152, Epsilon: 0.56\n",
      "Episode 199: Total Reward: 0.070, Epsilon: 0.56\n",
      "Episode 200: Total Reward: 0.096, Epsilon: 0.56\n",
      "Episode 201: Total Reward: -0.180, Epsilon: 0.55\n",
      "Episode 202: Total Reward: 0.144, Epsilon: 0.55\n",
      "Episode 203: Total Reward: -0.100, Epsilon: 0.54\n",
      "Episode 204: Total Reward: -0.359, Epsilon: 0.54\n",
      "Episode 205: Total Reward: -0.120, Epsilon: 0.53\n",
      "Episode 206: Total Reward: 0.491, Epsilon: 0.53\n",
      "Episode 207: Total Reward: -0.169, Epsilon: 0.52\n",
      "Episode 208: Total Reward: -0.510, Epsilon: 0.52\n",
      "Episode 209: Total Reward: 0.037, Epsilon: 0.51\n",
      "Episode 210: Total Reward: 0.244, Epsilon: 0.51\n",
      "Episode 211: Total Reward: 0.082, Epsilon: 0.51\n",
      "Episode 212: Total Reward: -0.827, Epsilon: 0.50\n",
      "Episode 213: Total Reward: -0.078, Epsilon: 0.50\n",
      "Episode 214: Total Reward: -0.032, Epsilon: 0.49\n",
      "Episode 215: Total Reward: 0.133, Epsilon: 0.49\n",
      "Episode 216: Total Reward: -0.283, Epsilon: 0.48\n",
      "Episode 217: Total Reward: -0.110, Epsilon: 0.48\n",
      "Episode 218: Total Reward: -0.451, Epsilon: 0.47\n",
      "Episode 219: Total Reward: -0.154, Epsilon: 0.47\n",
      "Episode 220: Total Reward: -0.571, Epsilon: 0.46\n",
      "Episode 221: Total Reward: 0.207, Epsilon: 0.46\n",
      "Episode 222: Total Reward: 0.184, Epsilon: 0.45\n",
      "Episode 223: Total Reward: -0.100, Epsilon: 0.45\n",
      "Episode 224: Total Reward: 0.057, Epsilon: 0.44\n",
      "Episode 225: Total Reward: -0.000, Epsilon: 0.44\n",
      "Episode 226: Total Reward: -0.602, Epsilon: 0.43\n",
      "Episode 227: Total Reward: -0.205, Epsilon: 0.43\n",
      "Episode 228: Total Reward: -0.101, Epsilon: 0.42\n",
      "Episode 229: Total Reward: -0.075, Epsilon: 0.42\n",
      "Episode 230: Total Reward: 0.208, Epsilon: 0.41\n",
      "Episode 231: Total Reward: 0.476, Epsilon: 0.41\n",
      "Episode 232: Total Reward: -0.004, Epsilon: 0.40\n",
      "Episode 233: Total Reward: 0.117, Epsilon: 0.40\n",
      "Episode 234: Total Reward: 0.015, Epsilon: 0.39\n",
      "Episode 235: Total Reward: -0.157, Epsilon: 0.39\n",
      "Episode 236: Total Reward: 0.039, Epsilon: 0.38\n",
      "Episode 237: Total Reward: -0.146, Epsilon: 0.38\n",
      "Episode 238: Total Reward: 0.092, Epsilon: 0.37\n",
      "Episode 239: Total Reward: -0.130, Epsilon: 0.37\n",
      "Episode 240: Total Reward: -0.055, Epsilon: 0.36\n",
      "Episode 241: Total Reward: 0.161, Epsilon: 0.35\n",
      "Episode 242: Total Reward: 0.091, Epsilon: 0.35\n",
      "Episode 243: Total Reward: -0.023, Epsilon: 0.34\n",
      "Episode 244: Total Reward: -0.213, Epsilon: 0.34\n",
      "Episode 245: Total Reward: 0.137, Epsilon: 0.33\n",
      "Episode 246: Total Reward: -0.294, Epsilon: 0.33\n",
      "Episode 247: Total Reward: -0.471, Epsilon: 0.32\n",
      "Episode 248: Total Reward: 0.300, Epsilon: 0.32\n",
      "Episode 249: Total Reward: -0.465, Epsilon: 0.31\n",
      "Episode 250: Total Reward: 0.227, Epsilon: 0.31\n",
      "Episode 251: Total Reward: 0.346, Epsilon: 0.30\n",
      "Episode 252: Total Reward: -0.186, Epsilon: 0.29\n",
      "Episode 253: Total Reward: 0.171, Epsilon: 0.29\n",
      "Episode 254: Total Reward: -0.013, Epsilon: 0.28\n",
      "Episode 255: Total Reward: 0.139, Epsilon: 0.28\n",
      "Episode 256: Total Reward: -0.096, Epsilon: 0.27\n",
      "Episode 257: Total Reward: 0.540, Epsilon: 0.27\n",
      "Episode 258: Total Reward: -0.180, Epsilon: 0.26\n",
      "Episode 259: Total Reward: -0.117, Epsilon: 0.25\n",
      "Episode 260: Total Reward: -0.167, Epsilon: 0.25\n",
      "Episode 261: Total Reward: -0.098, Epsilon: 0.24\n",
      "Episode 262: Total Reward: -0.168, Epsilon: 0.24\n",
      "Episode 263: Total Reward: 0.014, Epsilon: 0.23\n",
      "Episode 264: Total Reward: 0.243, Epsilon: 0.23\n",
      "Episode 265: Total Reward: 0.155, Epsilon: 0.22\n",
      "Episode 266: Total Reward: -0.004, Epsilon: 0.21\n",
      "Episode 267: Total Reward: -0.356, Epsilon: 0.21\n",
      "Episode 268: Total Reward: -0.573, Epsilon: 0.20\n",
      "Episode 269: Total Reward: -0.128, Epsilon: 0.20\n",
      "Episode 270: Total Reward: 0.351, Epsilon: 0.19\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_rate=0.2):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate) \n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.fc3 = nn.Linear(16, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, input_size, output_size, learning_rate, discount_factor, epsilon, batch_size=1000, replay_buffer_size=10000):\n",
    "        self.q_network = QNetwork(input_size, output_size)\n",
    "        self.target_network = QNetwork(input_size, output_size)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate) \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.learn_count = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "        \n",
    "        # Action to index mapping\n",
    "        self.action_to_index = {-1: 0, 0: 1, 1: 2}\n",
    "        self.index_to_action = {0: -1, 1: 0, 2: 1}\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice([-1, 0, 1])  # Explore\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "                action_index = torch.argmax(q_values, dim=1).item()  # Choose best action\n",
    "            action = self.index_to_action[action_index]  # Map index to action\n",
    "        return action\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        self.learn_count += 1\n",
    "\n",
    "        # Get samples randomly\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        actions_np = np.array([np.array(self.action_to_index[action]) for action in actions])\n",
    "        actions = torch.tensor(actions_np, dtype=torch.long).view(-1, 1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).view(-1, 1)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        # Get Q-values for each action\n",
    "        q_values = self.q_network(states).gather(1, actions)\n",
    "\n",
    "        # Get max Q-value for the next states from target network\n",
    "        next_q_values = self.target_network(next_states).max(1, keepdim=True)[0].detach()\n",
    "        target_q_values = rewards + self.discount_factor * next_q_values * (1 - dones)\n",
    "\n",
    "        # Compute loss and update the Q-network\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network every few episodes\n",
    "        if self.learn_count % 750 == 0:\n",
    "            self.update_target_network()   \n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "input_size = 5 \n",
    "output_size = 3  # Number of discrete actions\n",
    "learning_rate = 0.3\n",
    "discount_factor = 0.7\n",
    "epsilon = 1.0\n",
    "\n",
    "agent = QLearningAgent(input_size, output_size, learning_rate, discount_factor, epsilon)\n",
    "\n",
    "\n",
    "## Training constants\n",
    "total_episodes = 300\n",
    "number_of_pairs = len(workingPairOutcome)\n",
    "ls_total_reward = []\n",
    "\n",
    "# Simulating agent learning (in practice, use a loop with environment interaction)\n",
    "for episode in range(total_episodes):\n",
    "    agent.epsilon = 1 - ((episode+1) / total_episodes) ** 2\n",
    "    arr_pair_reward = np.zeros(number_of_pairs)\n",
    "\n",
    "    for pair_idx in range(number_of_pairs):\n",
    "        state = env.reset(pair_idx)\n",
    "        pair_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action, pair_idx)\n",
    "\n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "            agent.learn()\n",
    "            \n",
    "            state = next_state\n",
    "            pair_reward += reward\n",
    "\n",
    "        arr_pair_reward[pair_idx] = pair_reward\n",
    "    \n",
    "    total_reward = arr_pair_reward.mean()\n",
    "    print(f\"Episode {episode+1}: Total Reward: {total_reward:.3f}, Epsilon: {agent.epsilon:.2f}\")\n",
    "    # Capture total_reward for further analysis\n",
    "    ls_total_reward.append(total_reward)\n",
    "\n",
    "# evaluate random and final trained performance\n",
    "plt.plot(ls_total_reward)\n",
    "\n",
    "# After training, save the Q-network\n",
    "torch.save(agent.q_network.state_dict(), 'q_network.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 dec\n",
    "- training taking \n",
    "    - a full length training dataset.\n",
    "    - 1000 per learning step\n",
    "    - learning rate test (1.5 mins per episode)\n",
    "        - episodes: 5\n",
    "        - learning rate: 0.05 ==> total reward: .037\n",
    "        - learning rate: 0.5 ==> total reward: -.6\n",
    "        - learning rate: 0.3 ==> total reward: 0.88, .044\n",
    "        - learning rate: 0.15 ==> total reward: -.023\n",
    "        - learning rate: 0.25 ==> total reward: -.6\n",
    "        - learning rate: 0.35 ==> total reward: .028\n",
    "    - with drop out layer test (1.75 mins per episode)\n",
    "        - learning rate: 0.3 ==> total reward: -0.488\n",
    "        - learning rate: 0.4 ==> total reward: -0.422\n",
    "        - learning rate: 0.5 ==> total reward: .26, .13, .096\n",
    "        - learning rate: 0.6 ==> total reward: .03\n",
    "        - learning rate: 0.7 ==> total reward: -.08\n",
    "- performance still bad. should include dropout layer? --> performance a bit worse but more consistent\n",
    "- try removing spread so that the input is only boolean of SD and last position\n",
    "    - drop out layer test (1.75 mins per episode)\n",
    "        - learning rate: 0.1 ==> total reward: -.36\n",
    "        - learning rate: 0.3 ==> total reward: -.01\n",
    "        - learning rate: 0.5 ==> total reward: -.26\n",
    "### 5 dec\n",
    "- previously target and online network updated at the same time, but it should be used to regularise. So will try with updating more periodically instead of every learn step. reduced NN to 16 and 8 hidden layers with dropout (1.67 min per episode)\n",
    "    - learning rate: 0.3, update target every 10 learn occurences ==> total reward: .03\n",
    "    - learning rate: 0.3, update target every 100 learn occurences ==> total reward: .26\n",
    "    - learning rate: 0.3, update target every 500 learn occurences ==> total reward: -.383\n",
    "    - learning rate: 0.3, update target every 600 learn occurences ==> total reward: .22\n",
    "    - learning rate: 0.3, update target every 675 learn occurences ==> total reward: .41\n",
    "    - learning rate: 0.3, update target every 700 learn occurences ==> total reward: .10, .03\n",
    "    - learning rate: 0.3, update target every 750 learn occurences ==> total reward: .40, .15, -0.353\n",
    "    - learning rate: 0.3, update target every 1000 learn occurences ==> total reward: .05\n",
    "    - learning rate: 0.3, update target every 10 learn occurences, remove dropout layers ==> total reward: -.203\n",
    "    - learning rate: 0.5, update target every 10 learn occurences, remove dropout layers ==> total reward: -.334\n",
    "- changed ADAM optimiser to SGD (1.6 min per episode)\n",
    "    - learning rate: 0.15, update target every 750 learn occurences ==> total reward: .084\n",
    "    - learning rate: 0.3, update target every 750 learn occurences ==> total reward: -.20\n",
    "    - learning rate: 0.5, update target every 750 learn occurences ==> total reward: .303\n",
    "    - learning rate: 0.6, update target every 750 learn occurences ==> total reward: -.110\n",
    "    - learning rate: 0.7, update target every 750 learn occurences ==> total reward: -.556, -0.449\n",
    "- long term run with 50 episodes\n",
    "    - SGD, learning rate: 0.5, update target every 750 learn occurences ==> total reward: -.04 (79 min)\n",
    "    - ADAM, learning rate: 0.3, update target every 750 learn occurences ==> total reward: -.01 (84.5 min)\n",
    "    - ADAM, 32 hidden layer, learning rate: 0.3, update target every 750 learn occurences ==> total reward: 0.54 (88 min)\n",
    "0.544\n",
    "- long term run with 300 episodes\n",
    "    - ADAM, 32X16 hidden layer, learning rate: 0.3, update target every 750 learn occurences ==> total reward:  ( min)\n",
    "0.544\n",
    "\n",
    "### 6 Dec\n",
    "1. reduce to only 4 flags\n",
    "2. discount factor up to .99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get baseline results\n",
    "# t_pair = validPairsList[0]\n",
    "# max_steps_per_episode = 3000\n",
    "\n",
    "# def get_baseline(env, max_steps_per_episode, t_pair):\n",
    "#     env.reset()\n",
    "#     total_reward = 0\n",
    "#     current_step = 261\n",
    "#     env.current_step = current_step\n",
    "#     env.last_step = 2868\n",
    "\n",
    "#     for step in range(max_steps_per_episode):\n",
    "#         action = pairsOutcome[t_pair].iloc[env.current_step]['position']\n",
    "#         _, reward, done, _ = env.step(action)\n",
    "#         total_reward += reward\n",
    "\n",
    "#         if done:\n",
    "#             break\n",
    "\n",
    "#     print(f\"Baseline {t_pair}, Total Reward: {total_reward}, step {step}\")\n",
    "\n",
    "# get_baseline(env, 3000, top_keys[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- first few tries, network is very large\n",
    "- added epsilon search in \"choose_action\" functionso that there will be some chance to explore\n",
    "- changed reward function to multiply losses and give exponential returns to incentivise risk taking\n",
    "\n",
    "### 1 dec 2105: \n",
    "- might have performance is always oscillating negative and positive. This might be because of too large a learning rate. also start from start of training periods max steps to be 3000 so that total results are comparable\n",
    "    - this helped quite abit. \n",
    "`\n",
    "input_size = 7  # Adjust to your specific input size\n",
    "output_size = 3  # Adjust to your desired number of discrete actions\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.8\n",
    "epsilon = 1 # down to .3\n",
    "epsilon_decay = 0.9999\n",
    "num_episodes = 500\n",
    "max_steps_per_episode = 3000\n",
    "`\n",
    "- want to try changing epsilon to only update after the entire episode instead of after each step. its decaying too quickly\n",
    "    - \n",
    "- I want to try with changing reward by changing \"learn\" to use total_reward instead of \"reward\"\n",
    "- Scale the states. need to explore scaling the state since it is still in terms of absolute differences. NN is not able to do proportions\n",
    "- training epochs should be smaller at up to 30 days because mean reversion pattern is 1 to 33 days\n",
    "    - very bad performance with 40 day epochs\n",
    "\n",
    "### 1 dec 2217:\n",
    "- changed target q value fxn to remove exponential reward and scaled negative reward. now both positive and negative are the same. added portion of total reward in episode to incentivise more long term rewards.\n",
    "    - `        if reward > 0:\n",
    "            target_q_value = reward + self.discount_factor * next_q_value * (1 - done) + total_reward * .1\n",
    "        else:\n",
    "            target_q_value = reward + self.discount_factor * next_q_value * (1 - done) + total_reward * .1`\n",
    "    -       `  if episode%1==0:\n",
    "            agent.epsilon *= agent.epsilon_decay`\n",
    "\n",
    "### 2 Dec 2101:\n",
    "- managed to scale but results are not any better\n",
    "- thinking of reducing learning rate to reduce the oscillations\n",
    "    - will try to run with learning rate at 0.01\n",
    "- right now total reward is taking all of the target q function. maybe can make it a 50/50 split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
